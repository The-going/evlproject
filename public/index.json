[
{
	"uri": "https://xenomai.org/pipeline/usage/stage_push/",
	"title": "Installing the head stage",
	"tags": [],
	"description": "",
	"content": " irq_push_stage()\n irq_pop_stage()\n  TBD.\n"
},
{
	"uri": "https://xenomai.org/pipeline/porting/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": " Generic requirements The interrupt pipeline requires the following features to be available from the target Linux kernel:\n Generic IRQ handling (CONFIG_GENERIC_IRQ), which most architectures should support these days.\n IRQ domains (CONFIG_IRQ_DOMAIN).\n Generic clock event abstraction (CONFIG_GENERIC_CLOCKEVENTS).\n  Other assumptions ARM  a target ARM machine port must be allowed to specify its own IRQ handler at run time (CONFIG_MULTI_IRQ_HANDLER).\n only armv6 CPUs and later are supported, excluding older generations of ARM CPUs. Support for ASID (CONFIG_CPU_HAS_ASID) is required.\n  armv5 is not supported due to the use of VIVT caches on these CPUs, which don\u0026rsquo;t cope well - at all - with low latency requirements. A work aimed at leveraging the legacy FCSE PID register for reducing the cost of cache invalidation in context switches has been maintained until 2013 by Gilles Chanteperdrix, as part of the legacy I-pipe project, Dovetail\u0026rsquo;s ancestor. This work can still be cloned from this GIT repository.\n "
},
{
	"uri": "https://xenomai.org/pipeline/optimistic/",
	"title": "Optimistic Interrupt Protection",
	"tags": [],
	"description": "",
	"content": " Predictable response time of out-of-band handlers to IRQ receipts requires the in-band kernel work not to be allowed to delay them by masking interrupts in the CPU.\nHowever, critical sections delimited this way by the in-band code must still be enforced for the root stage, so that system integrity is not at risk. This means that although out-of-band IRQ handlers may run at any time while the head stage is accepting interrupts, in-band IRQ handlers should be allowed to run only when the root stage is accepting interrupts too. So we need to decouple the interrupt masking and delivery logic which applies to the head stage from the one in effect on the root stage, by implementing a dual interrupt control.\nVirtual interrupt disabling To this end, a software logic managing a virtual interrupt disable flag is introduced by the interrupt pipeline between the hardware and the generic IRQ management layer. This logic can mask IRQs from the perspective of the in-band kernel work when local_irq_save(), local_irq_disable() or any lock-controlled masking operations like spin_lock_irqsave() is called, while still accepting IRQs from the CPU for immediate delivery to out-of-band handlers.\nWhen a real IRQ arrives while interrupts are virtually masked, the event is logged for the receiving CPU, kept there until the virtual interrupt disable flag is cleared at which point it is dispatched as if it just happened. The principle of deferring interrupt delivery based on a software flag coupled to an event log has been originally described as Optimistic interrupt protection in this paper. It was originally intended as a low-overhead technique for manipulating the processor interrupt state, reducing the cost of interrupt masking for the common case of absence of interrupts.\nIn Dovetail\u0026rsquo;s two-stage pipeline, the head stage protects from interrupts by disabling them in the CPU\u0026rsquo;s status register as usual, while the root stage disables interrupts only virtually. A stage for which interrupts are disabled is said to be stalled. Conversely, unstalling a stage means re-enabling interrupts for it.\nObviously, stalling the head stage implicitly means disabling further IRQ receipts for the root stage too.\n Interrupt deferral for the root stage When the root stage is stalled because the virtual interrupt disable flag is set, any IRQ event which was not immediately delivered to the head stage is recorded into a per-CPU log, postponing delivery to the in-band kernel handler.\nSuch delivery is deferred until the in-band kernel code clears the virtual interrupt disable flag by calling local_irq_enable() or any of its variants, which unstalls the root stage. When this happens, the interrupt state is resynchronized by playing the log, firing the in-band handlers for which an IRQ event is pending.\n/* Both stages unstalled on entry */ local_irq_save(flags); \u0026lt;IRQx received: no out-of-band handler\u0026gt; (pipeline logs IRQx event) ... local_irq_restore(flags); (pipeline plays IRQx event) handle_IRQx_interrupt();  If the root stage is unstalled at the time of the IRQ receipt, the in-band handler is immediately invoked, just like with the non-pipelined IRQ model.\nAll interrupts are (seemingly) NMIs From the standpoint of the in-band kernel code (i.e. the one running over the root interrupt stage) , the interrupt pipelining logic virtually turns all device IRQs into NMIs, for running out-of-band handlers.\nFor this reason, out-of-band code may generally NOT re-enter in-band code, for preventing creepy situations like this one:\n/* in-band context */ spin_lock_irqsave(\u0026amp;lock, flags); \u0026lt;IRQx received: out-of-band handler installed\u0026gt; handle_oob_event(); /* attempted re-entry to in-band from out-of-band. */ in_band_routine(); spin_lock_irqsave(\u0026amp;lock, flags); \u0026lt;DEADLOCK\u0026gt; ... ... ... ... spin_unlock irqrestore(\u0026amp;lock, flags);  Even in absence of an attempt to get a spinlock recursively, the outer in-band code in the example above is entitled to assume that no access race can occur on the current CPU while interrupts are masked. Re-entering in-band code from an out-of-band handler would invalidate this assumption.\nIn rare cases, we may need to fix up the in-band kernel routines in order to allow out-of-band handlers to call them. Typically, atomic helpers are such routines, which serialize in-band and out-of-band callers.\nFor all other cases, the IRQ work API is available for scheduling the execution of a routine from the head stage, which will be invoked later from the root stage as soon as it gets back in control on the current CPU.\n"
},
{
	"uri": "https://xenomai.org/pipeline/",
	"title": "Interrupt pipeline",
	"tags": [],
	"description": "",
	"content": " The real-time core has to act upon device interrupts with no delay, regardless of the regular kernel operations which may be ongoing when the interrupt is received by the CPU. Therefore, there is a basic requirement for prioritizing interrupt masking and delivery between the real-time core and GPOS operations, while maintaining consistent internal serialization for the kernel.\nHowever, to protect from deadlocks and maintain data integrity, Linux hard disables interrupts around any critical section of code which must not be preempted by interrupt handlers on the same CPU, enforcing a strictly serialized execution among those contexts. The unpredictable delay this may cause before external events can be handled is a major roadblock for kernel components requiring predictable and very short response times to external events, in the range of a few microseconds.\nTo address this issue, Dovetail introduces a mechanism called interrupt pipelining which turns all device IRQs into pseudo-NMIs, only to run NMI-safe interrupt handlers from the perspective of the regular kernel activities.\nTwo-stage IRQ pipeline Interrupt pipelining is a lightweight approach based on the introduction of a separate, high-priority execution stage for running out-of-band interrupt handlers immediately upon IRQ receipt, which cannot be delayed by the in-band, regular kernel work. By immediately, we mean unconditionally, regardless of whether the in-band kernel code had disabled interrupts when the event arrived, using the common local_irq_save(), local_irq_disable() helpers or any of their derivatives. IRQs which have no handlers in the high priority stage may be deferred on the receiving CPU until the out-of-band activity has quiesced on that CPU. Eventually, the preempted in-band code can resume normally, which may involve handling the deferred interrupts.\nIn other words, interrupts are flowing down from the out-of-band to the in-band interrupt stages, which form a two-stage pipeline for prioritizing interrupt delivery. The runtime context of the out-of-band interrupt handlers is known as the head stage of the pipeline, as opposed to the in-band kernel activities sitting on the root stage:\n Flow of interrupts through the pipeline\n Out-of-band In-band IRQ handlers() IRQ handlers() __________ _______________________ ______ . / / . . / / . . / / . . / / . . / / . . / / . ___/ /______________________/ / . [IRQ] -----\u0026gt; _______________________________/ . . . . . . Head . . Root . . Stage . . Stage . _____________________________________________  A real-time core can base its own activities on the head stage, interposing on specific IRQ events, for delivering real-time capabilities to a particular set of applications. Meanwhile, the regular kernel operations keep going over the root stage unaffected, only delayed by short preemption times for running the out-of-band work.\n"
},
{
	"uri": "https://xenomai.org/pipeline/usage/irq_handling/",
	"title": "IRQ handling",
	"tags": [],
	"description": "",
	"content": "The driver API to the IRQ subsystem exposes the new interrupt type flag IRQF_OOB, denoting an out-of-band handler with the following routines:\n setup_irq() for early registration of special interrupts request_irq() for device interrupts __request_percpu_irq() for per-CPU interrupts  An IRQ action handler bearing this flag will run from out-of-band context over the head stage, regardless of the current interrupt state of the root stage. If no head stage is present, the flag will be ignored, with the interrupt handler running in-band over the root stage as usual.\nConversely, out-of-band handlers can be dismissed using the regular API, such as:\n free_irq() for device interrupts free_percpu_irq() for per-CPU interrupts  Out-of-band IRQ handling has the following constraints:\n If the IRQ is shared, with multiple action handlers registered for the same event, all other handlers on the same interrupt channel must bear the IRQF_OOB flag too, or the request will fail.  If meeting real-time requirements is your goal, sharing an IRQ line among multiple devices can only be a bad idea. You may want to do that in desparate hardware situations only.\n  Obviously, out-of-band handlers cannot be threaded (IRQF_NO_THREAD is implicit, IRQF_ONESHOT is ignored).   Installing an out-of-band handler for a device interrupt\n #include \u0026lt;linux/interrupt.h\u0026gt; static irqreturn_t oob_interrupt_handler(int irq, void *dev_id) { ... return IRQ_HANDLED; } init __init driver_init_routine(void) { int ret; ... ret = request_irq(DEVICE_IRQ, oob_interrupt_handler, IRQF_OOB, \u0026quot;Out-of-band device IRQ\u0026quot;, device_data); if (ret) goto fail; return 0; fail: /* Unwind upon error. */ ... }  "
},
{
	"uri": "https://xenomai.org/pipeline/usage/",
	"title": "Usage",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://xenomai.org/altsched/",
	"title": "Shared Task Control",
	"tags": [],
	"description": "",
	"content": "Dovetail promotes the idea that a dual kernel system should keep the functional overlap between the kernel and the real-time core minimal. To this end, a real-time thread should be merely seen as a regular task with additional scheduling capabilities guaranteeing very low response times. To support such idea, Dovetail enables kthreads and regular user tasks to run alternatively in the out-of-band execution context introduced by the interrupt pipeline (aka head stage), or the common in-band kernel context for GPOS operations (aka root stage).\nAs a result, real-time core applications in user-space benefit from the common Linux programming model - including virtual memory protection -, and still have access to the regular Linux services when carrying out non time-critical work.\n"
},
{
	"uri": "https://xenomai.org/pipeline/porting/irqflow/",
	"title": "Interrupt flow",
	"tags": [],
	"description": "",
	"content": " Pipelined interrupt flow Pipelining involves a basic change in controlling the interrupt flow: handle_domain_irq() from the IRQ domain API redirects all parent IRQs to the pipeline entry by calling generic_pipeline_irq().\n Redirecting the interrupt flow to the pipeline\n asm_irq_entry -\u0026gt; irqchip_handle_irq() -\u0026gt; handle_domain_irq() -\u0026gt; generic_pipeline_irq() -\u0026gt; irq_flow_handler() \u0026lt;IRQ delivery logic\u0026gt;  IRQ flow handlers Generic flow handlers acknowledge the incoming IRQ event in the hardware as usual, by calling the appropriate irqchip routine (e.g. irq_ack(), irq_eoi()). However, the flow handlers do not immediately invoke the in-band interrupt handlers. Instead, they hand the event over to the pipeline core by calling handle_oob_irq().\nIf an out-of-band handler exists for the interrupt received, handle_oob_irq() invokes it immediately, after switching the execution context to the head stage if not current yet. Otherwise, if the execution context is currently over the root stage and unstalled, the pipeline core delivers it immediately to the in-band handler. In all other cases, the interrupt is deferred, marked as pending into the current CPU\u0026rsquo;s event log, then the IRQ frame is left.\nIn absence of out-of-band handler for the event, the device may keep asserting the interrupt signal until the cause has been lifted in its own registers. For this reason, the flow handlers as modified by the pipeline code may have to mask the interrupt line until the in-band handler has run from the root stage, lifting the interrupt cause. This typically happens with level-triggered interrupts, preventing the device from storming the CPU with a continuous interrupt request.\nSince all of the IRQ handlers sharing an interrupt line are either in-band or out-of-band in a mutually exclusive way, such masking cannot delay out-of-band events.\n/* root stage stalled on entry */ asm_irq_entry ... -\u0026gt; generic_pipeline_irq() ... \u0026lt;IRQ logged, delivery deferred\u0026gt; asm_irq_exit /* * CPU allowed to accept interrupts again with IRQ cause not * acknowledged in device yet =\u0026gt; **IRQ storm**. */ asm_irq_entry ... asm_irq_exit asm_irq_entry ... asm_irq_exit  The logic behind masking interrupt lines until events are processed at some point later - out of the original interrupt context - also applies to the threaded interrupt model. In this case, interrupt lines may be masked until the IRQ thread is scheduled in.\n TBD: How to fixup a new flow handler.\nIRQ chip drivers irqchip drivers need to be specifically adapted for supporting the pipelined interrupt model. The basic task is to ensure that the following struct irq_chip handlers - if defined - can be called from an out-of-band context safely:\n irq_mask() irq_ack() irq_mask_ack() irq_eoi() irq_unmask()  Such handler is deemed safe to be called from out-of-band context when it does not invoke any in-band kernel service, which might cause an invalid context re-entry.\nThe generic IRQ management core serializes calls to irqchip handlers for a given IRQ by serializing access to its interrupt descriptor, acquiring the per-descriptor irq_desc::lock spinlock. Holding irq_desc::lock when running a handler for any IRQ shared between all CPUs ensures that a single CPU handles the event. This - originally - raw spinlock is automatically turned into a mutable spinlock when pipelining interrupts.\nIn addition, there might be inner spinlocks defined by some irqchip drivers for serializing handlers accessing a common interrupt controller hardware for distinct IRQs from multiple CPUs concurrently. Adapting such spinlocked sections found in irqchip drivers to support interrupt pipelining may involve converting the related spinlocks to hard spinlocks.\nOther section of code which were originally serialized by common interrupt disabling may need to be made fully atomic for running consistenly in pipelined interrupt mode. This can be done by introducing hard masking with hard_local_irq_save(), hard_local_irq_restore().\nFinally, IRQCHIP_PIPELINE_SAFE must be added to struct irqchip::flags member of a pipeline-aware irqchip driver, in order to notify the kernel that such controller can operate in pipelined interrupt mode.\n Adapting the ARM GIC driver for interrupt pipelining\n --- a/drivers/irqchip/irq-gic.c +++ b/drivers/irqchip/irq-gic.c @@ -93,7 +93,7 @@ struct gic_chip_data { #ifdef CONFIG_BL_SWITCHER -static DEFINE_RAW_SPINLOCK(cpu_map_lock); +static DEFINE_HARD_SPINLOCK(cpu_map_lock); #define gic_lock_irqsave(f)\t\\ raw_spin_lock_irqsave(\u0026amp;cpu_map_lock, (f)) @@ -424,7 +424,8 @@ static const struct irq_chip gic_chip = { .irq_set_irqchip_state\t= gic_irq_set_irqchip_state, .flags\t= IRQCHIP_SET_TYPE_MASKED | IRQCHIP_SKIP_SET_WAKE | -\tIRQCHIP_MASK_ON_SUSPEND, +\tIRQCHIP_MASK_ON_SUSPEND | +\tIRQCHIP_PIPELINE_SAFE, }; void __init gic_cascade_irq(unsigned int gic_nr, unsigned int irq)  irq_set_chip() will complain loudly with a kernel warning whenever the irqchip descriptor passed does not bear the IRQCHIP_PIPELINE_SAFE flag and CONFIG_IRQ_PIPELINE is enabled. Take this warning as a sure sign that your port of the IRQ pipeline to the target system is incomplete.\n Kernel preemption control (PREEMPT) When pipelining is enabled, preempt_schedule_irq() reconciles the virtual interrupt state - which has not been touched by the assembly level code upon kernel entry - with basic assumptions made by the scheduler core, such as entering with (virtual) interrupts disabled.\nExtended IRQ work API With interrupt pipelining, due to its NMI-like nature, out-of-band code running over the head stage might preempt in-band code over the root stage in the middle of a critical section. For this reason, it would be unsafe to call any in-band routine from an out-of-band context.\nTriggering in-band work handlers from out-of-band code can be done by using the regular irq_work_queue() service. Such work request from the head stage is scheduled for running over the root stage on the issuing CPU as soon as the out-of-band activity quiesces on this processor. As its name implies, the work handler runs in (in-band) interrupt context.\nThe interrupt pipeline forces the use of a synthetic IRQ as a notification signal for the IRQ work machinery, instead of a hardware-specific interrupt vector. This IRQ is labeled \u0026ldquo;in-band work\u0026rdquo; when reported by /proc/interrupts.\n "
},
{
	"uri": "https://xenomai.org/pipeline/porting/arch/",
	"title": "Architecture-specific bits",
	"tags": [],
	"description": "",
	"content": " Interrupt mask virtualization The architecture-specific code which manipulates the interrupt flag in the CPU\u0026rsquo;s state register in arch//include/asm/irqflags.h should be split between real and virtual interrupt control. The real interrupt control operations are inherited from the in-band kernel implementation. The virtual ones should be built upon services provided by the interrupt pipeline core.\n firstly, the original arch_local_* helpers should be renamed as native_* helpers, affecting the hardware interrupt state in the CPU.   Example: introducing the native interrupt state accessors for the ARM architecture\n --- a/arch/arm/include/asm/irqflags.h +++ b/arch/arm/include/asm/irqflags.h #if __LINUX_ARM_ARCH__ \u0026gt;= 6 #define arch_local_irq_save arch_local_irq_save -static inline unsigned long arch_local_irq_save(void) +static inline unsigned long native_irq_save(void) { unsigned long flags; asm volatile( -\t\u0026quot;\tmrs\t%0, \u0026quot; IRQMASK_REG_NAME_R \u0026quot;\t@ arch_local_irq_save\\n\u0026quot; +\t\u0026quot;\tmrs\t%0, \u0026quot; IRQMASK_REG_NAME_R \u0026quot;\t@ native_irq_save\\n\u0026quot; \u0026quot;\tcpsid\ti\u0026quot; : \u0026quot;=r\u0026quot; (flags) : : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;); return flags; } #define arch_local_irq_enable arch_local_irq_enable -static inline void arch_local_irq_enable(void) +static inline void native_irq_enable(void) { asm volatile( -\t\u0026quot;\tcpsie i\t@ arch_local_irq_enable\u0026quot; +\t\u0026quot;\tcpsie i\t@ native_irq_enable\u0026quot; : : : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;); } #define arch_local_irq_disable arch_local_irq_disable -static inline void arch_local_irq_disable(void) +static inline void native_irq_disable(void) { asm volatile( -\t\u0026quot;\tcpsid i\t@ arch_local_irq_disable\u0026quot; +\t\u0026quot;\tcpsid i\t@ native_irq_disable\u0026quot; : : : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;); @@ -69,12 +76,12 @@ static inline void arch_local_irq_disable(void) * Save the current interrupt enable state \u0026amp; disable IRQs */ #define arch_local_irq_save arch_local_irq_save -static inline unsigned long arch_local_irq_save(void) +static inline unsigned long native_irq_save(void) { unsigned long flags, temp; asm volatile( -\t\u0026quot;\tmrs\t%0, cpsr\t@ arch_local_irq_save\\n\u0026quot; +\t\u0026quot;\tmrs\t%0, cpsr\t@ native_irq_save\\n\u0026quot; \u0026quot;\torr\t%1, %0, #128\\n\u0026quot; \u0026quot;\tmsr\tcpsr_c, %1\u0026quot; : \u0026quot;=r\u0026quot; (flags), \u0026quot;=r\u0026quot; (temp) @@ -87,11 +94,11 @@ static inline unsigned long arch_local_irq_save(void) * Enable IRQs */ #define arch_local_irq_enable arch_local_irq_enable -static inline void arch_local_irq_enable(void) +static inline void native_irq_enable(void) { unsigned long temp; asm volatile( -\t\u0026quot;\tmrs\t%0, cpsr\t@ arch_local_irq_enable\\n\u0026quot; +\t\u0026quot;\tmrs\t%0, cpsr\t@ native_irq_enable\\n\u0026quot; \u0026quot;\tbic\t%0, %0, #128\\n\u0026quot; \u0026quot;\tmsr\tcpsr_c, %0\u0026quot; : \u0026quot;=r\u0026quot; (temp) @@ -103,11 +110,11 @@ static inline void arch_local_irq_enable(void) * Disable IRQs */ #define arch_local_irq_disable arch_local_irq_disable -static inline void arch_local_irq_disable(void) +static inline void native_irq_disable(void) { unsigned long temp; asm volatile( -\t\u0026quot;\tmrs\t%0, cpsr\t@ arch_local_irq_disable\\n\u0026quot; +\t\u0026quot;\tmrs\t%0, cpsr\t@ native_irq_disable\\n\u0026quot; \u0026quot;\torr\t%0, %0, #128\\n\u0026quot; \u0026quot;\tmsr\tcpsr_c, %0\u0026quot; : \u0026quot;=r\u0026quot; (temp) @@ -153,11 +160,11 @@ static inline void arch_local_irq_disable(void) * Save the current interrupt enable state. */ #define arch_local_save_flags arch_local_save_flags -static inline unsigned long arch_local_save_flags(void) +static inline unsigned long native_save_flags(void) { unsigned long flags; asm volatile( -\t\u0026quot;\tmrs\t%0, \u0026quot; IRQMASK_REG_NAME_R \u0026quot;\t@ local_save_flags\u0026quot; +\t\u0026quot;\tmrs\t%0, \u0026quot; IRQMASK_REG_NAME_R \u0026quot;\t@ native_save_flags\u0026quot; : \u0026quot;=r\u0026quot; (flags) : : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;); return flags; } @@ -166,21 +173,28 @@ static inline unsigned long arch_local_save_flags(void) * restore saved IRQ \u0026amp; FIQ state */ #define arch_local_irq_restore arch_local_irq_restore -static inline void arch_local_irq_restore(unsigned long flags) +static inline void native_irq_restore(unsigned long flags) { asm volatile( -\t\u0026quot;\tmsr\t\u0026quot; IRQMASK_REG_NAME_W \u0026quot;, %0\t@ local_irq_restore\u0026quot; +\t\u0026quot;\tmsr\t\u0026quot; IRQMASK_REG_NAME_W \u0026quot;, %0\t@ native_irq_restore\u0026quot; : : \u0026quot;r\u0026quot; (flags) : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;); } #define arch_irqs_disabled_flags arch_irqs_disabled_flags -static inline int arch_irqs_disabled_flags(unsigned long flags) +static inline int native_irqs_disabled_flags(unsigned long flags) { return flags \u0026amp; IRQMASK_I_BIT; } +static inline bool native_irqs_disabled(void) +{ +\tunsigned long flags = native_save_flags(); +\treturn native_irqs_disabled_flags(flags); +} + +#include \u0026lt;asm/irq_pipeline.h\u0026gt; #include \u0026lt;asm-generic/irqflags.h\u0026gt; #endif /* ifdef __KERNEL__ */   finally, a new set of arch_local_* helpers should be provided, affecting the virtual interrupt disable flag implemented by the pipeline core for controlling the root stage protection against interrupts. It is good practice to implement this set in a separate file available for inclusion from \u0026lt;asm/irq_pipeline.h\u0026gt;.   Example: providing the virtual interrupt state accessors for the ARM architecture\n--- /dev/null +++ b/arch/arm/include/asm/irq_pipeline.h @@ -0,0 +1,138 @@ +#ifndef _ASM_ARM_IRQ_PIPELINE_H +#define _ASM_ARM_IRQ_PIPELINE_H + +#include \u0026lt;asm-generic/irq_pipeline.h\u0026gt; + +#ifdef CONFIG_IRQ_PIPELINE + +static inline notrace unsigned long arch_local_irq_save(void) +{ +\tint stalled = root_irq_save(); +\tbarrier(); +\treturn arch_irqs_virtual_to_native_flags(stalled); +} + +static inline notrace void arch_local_irq_enable(void) +{ +\tbarrier(); +\troot_irq_enable(); +} + +static inline notrace void arch_local_irq_disable(void) +{ +\troot_irq_disable(); +\tbarrier(); +} + +static inline notrace unsigned long arch_local_save_flags(void) +{ +\tint stalled = root_irqs_disabled(); +\tbarrier(); +\treturn arch_irqs_virtual_to_native_flags(stalled); +} + +static inline int arch_irqs_disabled_flags(unsigned long flags) +{ +\treturn native_irqs_disabled_flags(flags); +} + +static inline notrace void arch_local_irq_restore(unsigned long flags) +{ +\tif (!arch_irqs_disabled_flags(flags)) +\t__root_irq_enable(); +\tbarrier(); +} + +#else /* !CONFIG_IRQ_PIPELINE */ + +static inline unsigned long arch_local_irq_save(void) +{ +\treturn native_irq_save(); +} + +static inline void arch_local_irq_enable(void) +{ +\tnative_irq_enable(); +} + +static inline void arch_local_irq_disable(void) +{ +\tnative_irq_disable(); +} + +static inline unsigned long arch_local_save_flags(void) +{ +\treturn native_save_flags(); +} + +static inline void arch_local_irq_restore(unsigned long flags) +{ +\tnative_irq_restore(flags); +} + +static inline int arch_irqs_disabled_flags(unsigned long flags) +{ +\treturn native_irqs_disabled_flags(flags); +} + +#endif /* !CONFIG_IRQ_PIPELINE */ + +#endif /* _ASM_ARM_IRQ_PIPELINE_H */   This new file should include \u0026lt;asm-generic/irq_pipeline.h\u0026gt; early to get access to the pipeline declarations it needs. This inclusion should be unconditional, even if the kernel is built with CONFIG_IRQ_PIPELINE disabled.\n Providing support for merged interrupt states The generic interrupt pipeline implementation requires the arch-level support code to provide for a pair of helpers aimed at translating the virtual interrupt disable flag to the interrupt bit in the CPU\u0026rsquo;s status register (e.g. PSR_I_BIT for ARM) and conversely. These helpers are used to create combined state words merging the virtual and real interrupt states.\n arch_irqs_virtual_to_native_flags(int stalled) must return a long word remapping the boolean value of @stalled to the CPU\u0026rsquo;s interrupt bit position in the status register. All other bits must be cleared.\n On ARM, this can be expressed as (stalled ? PSR_I_BIT : 0). on x86, that would rather be (stalled ? 0 : X86_EFLAGS_IF).  arch_irqs_native_to_virtual_flags(unsigned long flags) must return a long word remapping the CPU\u0026rsquo;s interrupt bit in @flags to an arbitrary bit position, choosen not to conflict with the former. In other words, the CPU\u0026rsquo;s interrupt state bit received in @flags should be shifted to a free position picked arbitrarily in the return value. All other bits must be cleared.\n On ARM, using bit position 31 to reflect the virtual state, this is expressed as (hard_irqs_disabled_flags(flags) ? (1 \u0026lt;\u0026lt; 31) : 0).\n On any other architecture, the implementation would be similar, using whatever bit position is available which would not conflict with the CPU\u0026rsquo;s interrupt bit position.\n   --- a/arch/arm/include/asm/irqflags.h +++ b/arch/arm/include/asm/irqflags.h /* * CPU interrupt mask handling. */ #ifdef CONFIG_CPU_V7M #define IRQMASK_REG_NAME_R \u0026quot;primask\u0026quot; #define IRQMASK_REG_NAME_W \u0026quot;primask\u0026quot; #define IRQMASK_I_BIT\t1 +#define IRQMASK_I_POS\t0 #else #define IRQMASK_REG_NAME_R \u0026quot;cpsr\u0026quot; #define IRQMASK_REG_NAME_W \u0026quot;cpsr_c\u0026quot; #define IRQMASK_I_BIT\tPSR_I_BIT +#define IRQMASK_I_POS\t7 #endif +#define IRQMASK_i_POS\t31  IRQMASK_i_POS (note the minus \u0026lsquo;i\u0026rsquo;) is the free bit position in the combo word where the ARM port stores the original CPU\u0026rsquo;s interrupt state in the combo word. This position can\u0026rsquo;t conflict with IRQMASK_I_POS, which is an alias to PSR_I_BIT (bit position 0 or 7).\n --- /dev/null +++ b/arch/arm/include/asm/irq_pipeline.h + +static inline notrace +unsigned long arch_irqs_virtual_to_native_flags(int stalled) +{ +\treturn (!!stalled) \u0026lt;\u0026lt; IRQMASK_I_POS; +} +static inline notrace +unsigned long arch_irqs_native_to_virtual_flags(unsigned long flags) +{ +\treturn (!!hard_irqs_disabled_flags(flags)) \u0026lt;\u0026lt; IRQMASK_i_POS; +}  Once all of these changes are in, the generic helpers from  such as local_irq_disable() and local_irq_enable() actually refer to the virtual protection scheme when interrupts are pipelined, which eventually allows to implement interrupt deferral for the protected in-band code running over the root stage.\n Adapting the assembly code to IRQ pipelining Interrupt entry As generic IRQ handling is a requirement for supporting Dovetail, the low-level interrupt handler living in the assembly portion of the architecture code can still deliver all interrupt events to the original C handler provided by the irqchip driver. That handler should in turn invoke:\n handle_domain_irq() for parent device IRQs\n generic_handle_irq() for cascaded device IRQs (decoded from the parent handler)\n  For those routines, the initial task of inserting an interrupt at the head of the pipeline is directly handled from the genirq layer they belong to. This means that there is usually not much to do other than making a quick check in the implementation of the parent IRQ handler, in the relevant irqchip driver, applying the rules of thumb carefully.\nOn some ARM platform equipped with a fairly common GIC controller, that would mean inspecting the function gic_handle_irq() for instance.\n  the arch-specific handle_IPI() or equivalent for special inter-processor interrupts  IPIs must be dealt with by specific changes introduced by the port we will cover later.\nInterrupt exit When interrupt pipelining is disabled, the kernel normally runs an epilogue after each interrupt or exception event was handled. If the event happened while the CPU was running some kernel code, the epilogue would check for a potential rescheduling opportunity in case CONFIG_PREEMPT is enabled. If a user-space task was preempted by the event, additional conditions would be checked for such as a signal pending delivery for that task.\nBecause interrupts are only virtually masked for the in-band code when pipelining is enabled, IRQs can still be taken by the CPU and passed on to the low-level assembly handlers, so that they can enter the interrupt pipeline.\n Running the regular epilogue afer an IRQ is valid only if the kernel was actually accepting interrupts when the event happened (i.e. the virtual interrupt disable flag was clear), and running in-band code.\n In all other cases, except the interrupt pipeline core, the rest of the kernel does not expect those IRQs to ever happen in the first place. Therefore, running the epilogue in such circumstances would be at odds with the kernel\u0026rsquo;s logic. In addition, low-level handlers must have been made aware that they might receive an event under such conditions.\nFor instance, the original ARM code for handling an IRQ which has preempted a kernel context would look like this:\n__irq_svc: svc_entry irq_handler #ifdef CONFIG_PREEMPT ldr\tr8, [tsk, #TI_PREEMPT]\t@ get preempt count ldr\tr0, [tsk, #TI_FLAGS]\t@ get flags teq\tr8, #0\t@ if preempt count != 0 movne\tr0, #0\t@ force flags to 0 tst\tr0, #_TIF_NEED_RESCHED blne\tsvc_preempt #endif  In order to properly handle interrupts in a pipelined delivery model, we have to detect whether the in-band kernel was ready to receive such event, acting upon it accordingly. To this end, the ARM port passes the event to a trampoline routine instead (handle_arch_irq_pipelined()), expecting on return a decision whether or not the epilogue code should run next. In the illustration below, this decision is returned as a boolean status to the caller, non-zero meaning that we may run the epilogue, zero otherwise.\n--- a/arch/arm/kernel/entry-armv.S +++ b/arch/arm/kernel/entry-armv.S .macro\tirq_handler #ifdef CONFIG_MULTI_IRQ_HANDLER -\tldr\tr1, =handle_arch_irq mov\tr0, sp badr\tlr, 9997f +#ifdef CONFIG_IRQ_PIPELINE +\tldr\tr1, =handle_arch_irq_pipelined +\tmov\tpc, r1 +#else\t+\tldr\tr1, =handle_arch_irq ldr\tpc, [r1] -#else +#endif +#elif CONFIG_IRQ_PIPELINE +#error \u0026quot;Legacy IRQ handling not pipelined\u0026quot; +#else\tarch_irq_handler_default #endif 9997: .endm  The trampoline routine added to the original code, first delivers the interrupt to the machine-defined handler, then tells the caller whether the regular epilogue may run for such event.\n--- a/arch/arm/kernel/irq.c +++ b/arch/arm/kernel/irq.c @@ -112,6 +112,15 @@ void __init set_handle_irq(void (*handle_irq)(struct pt_regs *)) } #endif +#ifdef CONFIG_IRQ_PIPELINE +asmlinkage int __exception_irq_entry +handle_arch_irq_pipelined(struct pt_regs *regs) +{ +\thandle_arch_irq(regs); +\treturn on_root_stage() \u0026amp;\u0026amp; !irqs_disabled(); +} +#endif +  Eventually, the low-level assembly handler receiving the interrupt event is adapted, in order to carry out the earlier decision by handle_arch_irq_pipelined(), skipping the epilogue code if required to.\n--- a/arch/arm/kernel/entry-armv.S +++ b/arch/arm/kernel/entry-armv.S __irq_svc: svc_entry irq_handler +#ifdef CONFIG_IRQ_PIPELINE +\ttst\tr0, r0 +\tbeq\t1f +#endif #ifdef CONFIG_PREEMPT ldr\tr8, [tsk, #TI_PREEMPT]\t@ get preempt count blne\tsvc_preempt #endif +1: svc_exit r5, irq = 1\t@ return from exception  Taking the fast exit path when applicable is critical to the stability of the target system to prevent invalid re-entry of the in-band kernel code.\n Fault exit Similarly to the interrupt exit case, the low-level fault handling code must skip the epilogue code when the fault was taken over an out-of-band context. Upon fault, the current interrupt state is not considered for determining whether we should run the epilogue, since a fault may occur independently of such state.\n Running the regular epilogue after a fault is valid only if that fault was triggered by some in-band code, excluding any fault raised by out-of-band code.\n For instance, the original ARM code for returning from an exception event would be modified as follows:\n--- a/arch/arm/kernel/entry-armv.S +++ b/arch/arm/kernel/entry-armv.S @@ -754,7 +772,7 @@ ENTRY(ret_from_exception) UNWIND(.cantunwind\t) get_thread_info tsk mov\twhy, #0 -\tb\tret_to_user +\tret_to_user_pipelined r1 UNWIND(.fnend\t)  With the implementation of ret_to_user_pipelined checking for the current stage, skipping the epilogue if the faulting code was running over an out-of-band context:\n--- a/arch/arm/kernel/entry-header.S +++ b/arch/arm/kernel/entry-header.S +/* + * Branch to the exception epilogue, skipping the in-band work + * if running over the head interrupt stage. + */ +\t.macro ret_to_user_pipelined, tmp +#ifdef CONFIG_IRQ_PIPELINE +\tldr\t\\tmp, [tsk, #TI_LOCAL_FLAGS] +\ttst\t\\tmp, #_TLF_HEAD +\tbne\tfast_ret_to_user +#endif +\tb\tret_to_user +\t.endm +  _TLF_HEAD is a local thread_info flag denoting a current task running out-of-band code over the head stage. If set, the epilogue must be skipped.\nDealing with IPIs Support for inter-processor interrupts is architecture-specific code.\nTBD.\n"
},
{
	"uri": "https://xenomai.org/pipeline/porting/",
	"title": "Porting",
	"tags": [],
	"description": "",
	"content": "How to port this thing.\n"
},
{
	"uri": "https://xenomai.org/pipeline/usage/synthetic/",
	"title": "Synthetic IRQs",
	"tags": [],
	"description": "",
	"content": "The pipeline introduces an additional type of interrupts, which are purely software-originated, with no hardware involvement. These IRQs can be triggered by any kernel code. Synthetic IRQs are inherently per-CPU events.\nBecause the common pipeline flow_ applies to synthetic interrupts, it is possible to attach them to out-of-band and/or in-band handlers, just like device interrupts.\nSynthetic interrupts and regular softirqs differ in essence: the latter only exist in the in-band context, and therefore cannot trigger out-of-band activities.\n Synthetic interrupt vectors are allocated from the synthetic_irq_domain, using the irq_create_direct_mapping() routine.\nFor instance, a synthetic interrupt can be used for triggering an in-band activity on the root stage from the head stage as follows:\n#include \u0026lt;linux/irq_pipeline.h\u0026gt; static irqreturn_t sirq_handler(int sirq, void *dev_id) { do_in_band_work(); return IRQ_HANDLED; } static struct irqaction sirq_action = { .handler = sirq_handler, .name = \u0026quot;In-band synthetic interrupt\u0026quot;, .flags = IRQF_NO_THREAD, }; unsigned int alloc_sirq(void) { unsigned int sirq; sirq = irq_create_direct_mapping(synthetic_irq_domain); if (!sirq) return 0; setup_percpu_irq(sirq, \u0026amp;sirq_action); return sirq; }  Code can schedule the execution of sirq_handler() from the out-of-band context in two different ways:\n Using the common injection service\n irq_pipeline_inject(sirq);   Using the lightweight injection method (requires interrupts to be disabled in the CPU)\n unsigned long flags = hard_local_irqsave(); irq_stage_post_root(sirq); hard_local_irqrestore(flags);  Assuming that no interrupt may be pending in the event log for the head stage at the time this code runs, the method above relies on the invariant that in a pipeline interrupt model, IRQs pending for the root stage will have to wait for the head stage to quiesce before they can be handled. Therefore, it is pointless to try synchronizing the interrupts pending for the root stage from the head stage, which the irq_pipeline_inject() service would do systematically. irq_stage_post_root() simply marks the event as pending in the event log of the root stage for the current CPU, then returns. This event would be played as a result of synchronizing the log automatically when the current CPU switches back to the root stage.\n Conversely, a synthetic interrupt can be handled from the out-of-band context:\nstatic irqreturn_t sirq_oob_handler(int sirq, void *dev_id) { do_out_of_band_work(); return IRQ_HANDLED; } unsigned int alloc_sirq(void) { unsigned int sirq; sirq = irq_create_direct_mapping(synthetic_irq_domain); if (!sirq) return 0; ret = __request_percpu_irq(sirq, sirq_oob_handler, IRQF_OOB, \u0026quot;Out-of-band synthetic interrupt\u0026quot;, dev_id); if (ret) { irq_dispose_mapping(sirq); return 0; } return sirq; }  Code can trigger the immediate execution of sirq_oob_handler() on the head stage as follows:\nirq_pipeline_inject(sirq);  Calling irq_stage_post_head(sirq) from the root stage to trigger an out-of-band event is most often not the right way to do this, because this service would not synchronize the interrupt log before returning. In other words, the sirq event would still be pending for the head stage despite the fact that it should have preempted the root stage before returning to the caller.\n "
},
{
	"uri": "https://xenomai.org/pipeline/usage/pipeline_stop/",
	"title": "Pipeline Stop",
	"tags": [],
	"description": "",
	"content": "In some circumstances, it may be required to stop all activities in the whole machine, forcing all CPUs to come to a stall before running a user-defined handler. A non-pipelined kernel would call the stop_machine() service for this purpose.\nHowever, interrupt pipelining would still allow out-of-band activity to take place on the head stage as stop_machine() would only affect the root stage onto which the in-band kernel code runs. For the purpose of enforcing a complete stall encompassing all execution stages, the stop_machine_pipelined() service has been introduced.\nA typical example of such use can be found in the modifications brought to the ftrace support code on ARM, which make sure that execution is stopped on all CPUs regardless of the interrupt stage, before the kernel .text section can be altered safely:\n Stopping all CPUs before poking into the .text section\n --- a/arch/arm/kernel/ftrace.c +++ b/arch/arm/kernel/ftrace.c @@ -44,7 +44,7 @@ static int __ftrace_modify_code(void *data) void arch_ftrace_update_code(int command) { -\tstop_machine(__ftrace_modify_code, \u0026amp;command, NULL); +\tstop_machine_pipelined(__ftrace_modify_code, \u0026amp;command, NULL); } #ifdef CONFIG_OLD_MCOUNT  stop_machine_pipelined() may be called from in-band code only.\n "
},
{
	"uri": "https://xenomai.org/pipeline/rulesofthumb/",
	"title": "Rules Of Thumb",
	"tags": [],
	"description": "",
	"content": " Dealing with spinlocks Converting regular kernel spinlocks (e.g. spinlock_t, raw_spin_lock_t) to hard spinlocks should involve a careful review of every section covered by such lock. Any such section would then inherit the following requirements:\n no in-band kernel service should be called within the section,\n the section covered by the lock should be short enough to keep interrupt latency low.\n  TBD.\n"
},
{
	"uri": "https://xenomai.org/pipeline/usage/pipeline_inject/",
	"title": "IRQ injection",
	"tags": [],
	"description": "",
	"content": " irq_pipeline_inject()\n irq_stage_post_root()\n irq_stage_post_head()\n  TBD.\n"
},
{
	"uri": "https://xenomai.org/pipeline/porting/timer/",
	"title": "Timer management",
	"tags": [],
	"description": "",
	"content": " Proxy tick device The proxy tick device is a synthetic clock event device for handing over the control of the hardware tick device to a high-precision, out-of-band timing logic, which does not run into delays caused by the in-band kernel code.\nWith this proxy in place, any out-of-band code can gain control over the timer hardware for carrying out its own timing duties. In the same move, it is required to honor the timing requests received from the in-band timer core (i.e. hrtimers) since the latter won\u0026rsquo;t be able to program timer events directly into the hardware while the proxy is active.\nIn other words, the proxy tick device shares the functionality of the actual device between the in-band and out-of-band contexts, with only the latter actually programming the hardware.\nChanges to the clock chip devices The proxy device stacks over a real clock chip device, controlling it. Clock chips which may be controlled by the proxy tick device need their drivers to be specifically adapted for such use, as follows:\n clockevents_handle_event() must be used to invoke the event handler from the interrupt handler, instead of dereferencing struct clock_event_device::event_handler directly.\n struct clock_event_device::irq must be properly set to the actual IRQ number signaling an event from this device.\n struct clock_event_device::features must include CLOCK_EVT_FEAT_PIPELINE.\n __IRQF_TIMER must be set for the action handler of the timer device interrupt.\n   Adapting the ARM architected timer driver to out-of-band timing\n --- a/drivers/clocksource/arm_arch_timer.c +++ b/drivers/clocksource/arm_arch_timer.c @@ -585,7 +585,7 @@ static __always_inline irqreturn_t timer_handler(const int access, if (ctrl \u0026amp; ARCH_TIMER_CTRL_IT_STAT) { ctrl |= ARCH_TIMER_CTRL_IT_MASK; arch_timer_reg_write(access, ARCH_TIMER_REG_CTRL, ctrl, evt); -\tevt-\u0026gt;event_handler(evt); +\tclockevents_handle_event(evt); return IRQ_HANDLED; } @@ -704,7 +704,7 @@ static int arch_timer_set_next_event_phys_mem(unsigned long evt, static void __arch_timer_setup(unsigned type, struct clock_event_device *clk) { -\tclk-\u0026gt;features = CLOCK_EVT_FEAT_ONESHOT; +\tclk-\u0026gt;features = CLOCK_EVT_FEAT_ONESHOT | CLOCK_EVT_FEAT_PIPELINE; if (type == ARCH_TIMER_TYPE_CP15) { if (arch_timer_c3stop)  Only oneshot-capable clock event devices can be shared via the proxy tick device.\n "
},
{
	"uri": "https://xenomai.org/pipeline/usage/stage_escalation/",
	"title": "Stage escalation",
	"tags": [],
	"description": "",
	"content": " irq_stage_escalate()  TBD.\n"
},
{
	"uri": "https://xenomai.org/pipeline/porting/atomic/",
	"title": "Atomic operations",
	"tags": [],
	"description": "",
	"content": "The effect of virtualizing interrupt protection must be reversed for atomic helpers in asm-generic/atomic.h, asm-generic/bitops/atomic.h and asm-generic/cmpxchg-local.h, so that no interrupt can preempt their execution, regardless of the stage their caller live on.\nThis is required to keep those helpers usable on data which might be accessed concurrently from both stages.\nThe usual way to revert such virtualization consists of delimiting the protected section with hard_local_irq_save(), hard_local_irq_restore() calls, in replacement for local_irq_save(), local_irq_restore() respectively.\n"
},
{
	"uri": "https://xenomai.org/pipeline/usage/interrupt_protection/",
	"title": "Interrupt Protection",
	"tags": [],
	"description": "",
	"content": " Disabling interrupts in the CPU The local_irq_save() and local_irq_disable() helpers are no more disabling interrupts in the CPU when interrupt pipelining is enabled, but only disable interrupt events virtually for the root stage.\nA set of helpers is provided for manipulating the interrupt disable flag in the CPU instead. When CONFIG_IRQ_PIPELINE is disabled, this set maps 1:1 over the regular local_irq_*() API.\n   Original/Virtual Non-virtualized call     local_save_flags(flags) flags = hard_local_save_flags()   local_irq_disable() hard_local_irq_disable()   local_irq_enable() hard_local_irq_enable()   local_irq_save(flags) flags = hard_local_irq_save()   local_irq_restore(flags) hard_local_irq_restore(flags)   irqs_disabled() hard_irqs_disabled()   irqs_disabled_flags(flags) hard_irqs_disabled_flags(flags)    Stalling the head stage Just like the root stage is affected by the state of the virtual interrupt disable flag, the interrupt state of the head stage is controlled by a dedicated stall bit flag in the head stage\u0026rsquo;s status. In combination with the interrupt disable bit in the CPU, this software bit controls interrupt delivery to the head stage.\nWhen this stall bit is set, interrupts which might be pending in the head stage\u0026rsquo;s event log of the current CPU are not played. Conversely, the out-of-band handlers attached to pending IRQs are fired when the stall bit is clear. The following table represents the equivalent calls affecting the stall bit for each stage:\n   Root stage operation Head stage operation     local_save_flags(flags) -none-   local_irq_disable() head_irq_disable()   local_irq_enable() head_irq_enable()   local_irq_save(flags) flags = head_irq_save()   local_irq_restore(flags) head_irq_restore(flags)   irqs_disabled() head_irqs_disabled()   irqs_disabled_flags(flags) -none-    Using this set of helpers only makes sense from out-of-band code, typically in the real-time core implementation. Calling them from any other context would be highly suspicious. Adapting in-band code to interrupt pipelining might involve using the hard_local_*() set instead, but only in marginal cases, always with extreme care.\n "
},
{
	"uri": "https://xenomai.org/pipeline/porting/locking/",
	"title": "Locking",
	"tags": [],
	"description": "",
	"content": " Additional spinlock types The pipeline core introduces two spinlock types:\n hard spinlocks manipulate the CPU interrupt mask, and don\u0026rsquo;t affect the kernel preemption state in locking/unlocking operations.  This type of spinlock is useful for implementing a critical section to serialize concurrent accesses from both in-band and out-of-band contexts, i.e. from root and head stages. Obviously, sleeping into a critical section protected by a hard spinlock would be a very bad idea. In other words, hard spinlocks are not subject to virtual interrupt masking, therefore can be used to serialize with out-of-band activities, including from the in-band kernel code. At any rate, those sections ought to be quite short, for keeping latency low.\n mutable spinlocks are used internally by the pipeline core to protect access to IRQ descriptors (struct irq_desc::lock), so that we can keep the original locking scheme of the generic IRQ core unmodified for handling out-of-band interrupts.  Mutable spinlocks behave like hard spinlocks when traversed by the low-level IRQ handling code on entry to the pipeline, or common raw spinlocks otherwise, preserving the kernel (virtualized) interrupt and preemption states as perceived by the in-band context. This type of lock is not meant to be used in any other situation.\nLockdep support The lock validator automatically reconciles the real and virtual interrupt states, so it can deliver proper diagnosis for locking constructs defined in both in-band and out-of-band contexts. This means that hard and mutable spinlocks are included in the validation set when LOCKDEP is enabled.\nThese two additional types are subject to LOCKDEP analysis. However, be aware that latency figures are likely to be really bad when LOCKDEP is enabled, due to the large amount of work the lock validator may have to do with interrupts disabled for the CPU (i.e. hard locking) for enforcing critical sections.\n "
},
{
	"uri": "https://xenomai.org/pipeline/porting/misc/",
	"title": "Misc",
	"tags": [],
	"description": "",
	"content": " printk() support printk() may be called by out-of-band code safely, without encurring extra latency. The output is conveyed like NMI-originated output, which involves some delay until the in-band code resumes, and the console driver(s) can handle it.\nTracing Tracepoints can be traversed by out-of-band code safely. Dynamic tracing is available to a kernel running the pipelined interrupt model too.\n"
},
{
	"uri": "https://xenomai.org/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Introduction to Dovetail Using Linux as a host for lightweight software cores specialized in delivering very short and bounded response times has been a popular way of supporting real-time applications in the embedded space over the years.\nThis design - known as the dual kernel approach - introduces a small real-time infrastructure which schedules time-critical activities independently from the main kernel. Application threads co-managed by this infrastructure still benefit from the ancillary kernel services such as virtual memory management, and can also leverage the rich GPOS feature set Linux provides such as networking, data storage or GUIs.\nAlthough the real-time infrastructure has to present specific driver stack and API implementations to applications, there are nonetheless significant upsides to keeping the real-time core separate from the GPOS infrastructure:\n because the two kernels are independent, real-time activities are not serialized with GPOS operations internally, removing potential delays which might be induced by the non time-critical work. Likewise, there is no requirement for keeping the GPOS operations fine-grained and highly preemptible at any time, which would otherwise induce noticeable overhead on low-end hardware, due to the requirement for pervasive task priority inheritance and IRQ threading.\n the functional isolation of the real-time infrastructure from the rest of the kernel code restricts common bug hunting to the scope of the smaller kernel, excluding most interactions with the very large GPOS kernel base.\n with a dedicated infrastructure providing a specific, well-defined set of real-time services, applications can unambiguously figure out which API calls are available for supporting time-critical work, excluding all the rest as being potentially non-deterministic with respect to response time.\n  This documentation presents the two software layers forming Dovetail: firstly the interrupt pipeline which creates a high-priority execution stage for a real-time infrastructure, and finally the support for shared task control between the Linux kernel and the real-time component over the kthreads and user tasks.\nDovetail is the successor to the I-pipe, the interrupt pipeline implementation Xenomai\u0026rsquo;s Cobalt real-time core currently relies on.\nDovetail only introduces the basic mechanisms for hosting a real-time core into the Linux kernel, enabling the common programming model for its applications in user-space. It does not implement the real-time core per se, which should be provided by a separate kernel component.\n "
},
{
	"uri": "https://xenomai.org/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://xenomai.org/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]