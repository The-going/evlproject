[
{
	"uri": "https://dovetail.xenomai.org/pipeline/usage/stage_push/",
	"title": "Installing the head stage",
	"tags": [],
	"description": "",
	"content": " irq_stage_push()\n irq_stage_pop()\n  TBC.\n"
},
{
	"uri": "https://dovetail.xenomai.org/pipeline/porting/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": " Generic requirements The interrupt pipeline requires the following features to be available from the target Linux kernel:\n Generic IRQ handling (CONFIG_GENERIC_IRQ), which most architectures should support these days.\n IRQ domains (CONFIG_IRQ_DOMAIN).\n Generic clock event abstraction (CONFIG_GENERIC_CLOCKEVENTS).\n  Other assumptions ARM  a target ARM machine port must be allowed to specify its own IRQ handler at run time (CONFIG_MULTI_IRQ_HANDLER).\n only armv6 CPUs and later are supported, excluding older generations of ARM CPUs. Support for ASID (CONFIG_CPU_HAS_ASID) is required.\n machine does not have VIVT cache.\n  armv5 is not supported due to the use of VIVT caches on these CPUs, which don\u0026rsquo;t cope well - at all - with low latency requirements. A work aimed at leveraging the legacy FCSE PID register for reducing the cost of cache invalidation in context switches has been maintained until 2013 by Gilles Chanteperdrix, as part of the legacy I-pipe project, Dovetail\u0026rsquo;s ancestor. This work can still be cloned from this GIT repository.\n "
},
{
	"uri": "https://dovetail.xenomai.org/pipeline/optimistic/",
	"title": "Optimistic Interrupt Protection",
	"tags": [],
	"description": "",
	"content": " Predictable response time of out-of-band handlers to IRQ receipts requires the in-band kernel work not to be allowed to delay them by masking interrupts in the CPU.\nHowever, critical sections delimited this way by the in-band code must still be enforced for the root stage, so that system integrity is not at risk. This means that although out-of-band IRQ handlers may run at any time while the head stage is accepting interrupts, in-band IRQ handlers should be allowed to run only when the root stage is accepting interrupts too. So we need to decouple the interrupt masking and delivery logic which applies to the head stage from the one in effect on the root stage, by implementing a dual interrupt control.\nVirtual interrupt disabling To this end, a software logic managing a virtual interrupt disable flag is introduced by the interrupt pipeline between the hardware and the generic IRQ management layer. This logic can mask IRQs from the perspective of the in-band kernel work when local_irq_save(), local_irq_disable() or any lock-controlled masking operations like spin_lock_irqsave() is called, while still accepting IRQs from the CPU for immediate delivery to out-of-band handlers.\nWhen a real IRQ arrives while interrupts are virtually masked, the event is logged for the receiving CPU, kept there until the virtual interrupt disable flag is cleared at which point it is dispatched as if it just happened. The principle of deferring interrupt delivery based on a software flag coupled to an event log has been originally described as Optimistic interrupt protection in this paper. It was originally intended as a low-overhead technique for manipulating the processor interrupt state, reducing the cost of interrupt masking for the common case of absence of interrupts.\nIn Dovetail\u0026rsquo;s two-stage pipeline, the head stage protects from interrupts by disabling them in the CPU\u0026rsquo;s status register as usual, while the root stage disables interrupts only virtually. A stage for which interrupts are disabled is said to be stalled. Conversely, unstalling a stage means re-enabling interrupts for it.\nObviously, stalling the head stage implicitly means disabling further IRQ receipts for the root stage down the pipeline too.\n Interrupt deferral for the root stage When the root stage is stalled because the virtual interrupt disable flag is set, any IRQ event which was not immediately delivered to the head stage is recorded into a per-CPU log, postponing delivery to the in-band kernel handler.\nSuch delivery is deferred until the in-band kernel code clears the virtual interrupt disable flag by calling local_irq_enable() or any of its variants, which unstalls the root stage. When this happens, the interrupt state is resynchronized by playing the log, firing the in-band handlers for which an IRQ event is pending.\n/* Both stages unstalled on entry */ local_irq_save(flags); \u0026lt;IRQx received: no out-of-band handler\u0026gt; (pipeline logs IRQx event) ... local_irq_restore(flags); (pipeline plays IRQx event) handle_IRQx_interrupt();  If the root stage is unstalled at the time of the IRQ receipt, the in-band handler is immediately invoked, just like with the non-pipelined IRQ model.\nAll interrupts are (seemingly) NMIs From the standpoint of the in-band kernel code (i.e. the one running over the root interrupt stage) , the interrupt pipelining logic virtually turns all device IRQs into NMIs, for running out-of-band handlers.\nFor this reason, out-of-band code may generally NOT re-enter in-band code, for preventing creepy situations like this one:\n/* in-band context */ spin_lock_irqsave(\u0026amp;lock, flags); \u0026lt;IRQx received: out-of-band handler installed\u0026gt; handle_oob_event(); /* attempted re-entry to in-band from out-of-band. */ in_band_routine(); spin_lock_irqsave(\u0026amp;lock, flags); \u0026lt;DEADLOCK\u0026gt; ... ... ... ... spin_unlock irqrestore(\u0026amp;lock, flags);  Even in absence of an attempt to get a spinlock recursively, the outer in-band code in the example above is entitled to assume that no access race can occur on the current CPU while interrupts are masked. Re-entering in-band code from an out-of-band handler would invalidate this assumption.\nIn rare cases, we may need to fix up the in-band kernel routines in order to allow out-of-band handlers to call them. Typically, atomic helpers are such routines, which serialize in-band and out-of-band callers.\nFor all other cases, the IRQ work API is available for scheduling the execution of a routine from the head stage, which will be invoked later from the root stage as soon as it gets back in control on the current CPU.\n"
},
{
	"uri": "https://dovetail.xenomai.org/pipeline/",
	"title": "Interrupt pipeline",
	"tags": [],
	"description": "",
	"content": " The real-time core has to act upon device interrupts with no delay, regardless of the regular kernel operations which may be ongoing when the interrupt is received by the CPU. Therefore, there is a basic requirement for prioritizing interrupt masking and delivery between the real-time core and GPOS operations, while maintaining consistent internal serialization for the kernel.\nHowever, to protect from deadlocks and maintain data integrity, Linux hard disables interrupts around any critical section of code which must not be preempted by interrupt handlers on the same CPU, enforcing a strictly serialized execution among those contexts. The unpredictable delay this may cause before external events can be handled is a major roadblock for kernel components requiring predictable and very short response times to external events, in the range of a few microseconds.\nTo address this issue, Dovetail introduces a mechanism called interrupt pipelining which turns all device IRQs into pseudo-NMIs, only to run NMI-safe interrupt handlers from the perspective of the regular kernel activities.\nTwo-stage IRQ pipeline Interrupt pipelining is a lightweight approach based on the introduction of a separate, high-priority execution stage for running out-of-band interrupt handlers immediately upon IRQ receipt, which cannot be delayed by the in-band, regular kernel work. By immediately, we mean unconditionally, regardless of whether the in-band kernel code had disabled interrupts when the event arrived, using the common local_irq_save(), local_irq_disable() helpers or any of their derivatives. IRQs which have no handlers in the high priority stage may be deferred on the receiving CPU until the out-of-band activity has quiesced on that CPU. Eventually, the preempted in-band code can resume normally, which may involve handling the deferred interrupts.\nIn other words, interrupts are flowing down from the out-of-band to the in-band interrupt stages, which form a two-stage pipeline for prioritizing interrupt delivery. The runtime context of the out-of-band interrupt handlers is known as the head stage of the pipeline, as opposed to the in-band kernel activities sitting on the root stage:\n Flow of interrupts through the pipeline\n Out-of-band In-band IRQ handlers() IRQ handlers() __________ _______________________ ______ . / / . . / / . . / / . . / / . . / / . . / / . ___/ /______________________/ / . [IRQ] -----\u0026gt; _______________________________/ . . . . . . Head . . Root . . Stage . . Stage . _____________________________________________  A real-time core can base its own activities on the head stage, interposing on specific IRQ events, for delivering real-time capabilities to a particular set of applications. Meanwhile, the regular kernel operations keep going over the root stage unaffected, only delayed by short preemption times for running the out-of-band work.\n"
},
{
	"uri": "https://dovetail.xenomai.org/pipeline/usage/irq_handling/",
	"title": "IRQ handling",
	"tags": [],
	"description": "",
	"content": "The driver API to the IRQ subsystem exposes the new interrupt type flag IRQF_OOB, denoting an out-of-band handler with the following routines:\n setup_irq() for early registration of special interrupts request_irq() for device interrupts __request_percpu_irq() for per-CPU interrupts  An IRQ action handler bearing this flag will run from out-of-band context over the head stage, regardless of the current interrupt state of the root stage. If no head stage is present, the flag will be ignored, with the interrupt handler running in-band over the root stage as usual.\nConversely, out-of-band handlers can be dismissed using the regular API, such as:\n free_irq() for device interrupts free_percpu_irq() for per-CPU interrupts  Out-of-band IRQ handling has the following constraints:\n If the IRQ is shared, with multiple action handlers registered for the same event, all other handlers on the same interrupt channel must bear the IRQF_OOB flag too, or the request will fail.  If meeting real-time requirements is your goal, sharing an IRQ line among multiple devices can only be a bad idea. You may want to do that in desparate hardware situations only.\n  Obviously, out-of-band handlers cannot be threaded (IRQF_NO_THREAD is implicit, IRQF_ONESHOT is ignored).   Installing an out-of-band handler for a device interrupt\n #include \u0026lt;linux/interrupt.h\u0026gt; static irqreturn_t oob_interrupt_handler(int irq, void *dev_id) { ... return IRQ_HANDLED; } init __init driver_init_routine(void) { int ret; ... ret = request_irq(DEVICE_IRQ, oob_interrupt_handler, IRQF_OOB, \u0026quot;Out-of-band device IRQ\u0026quot;, device_data); if (ret) goto fail; return 0; fail: /* Unwind upon error. */ ... }  "
},
{
	"uri": "https://dovetail.xenomai.org/altsched/",
	"title": "Alternate Task Control",
	"tags": [],
	"description": "",
	"content": "Dovetail promotes the idea that a dual kernel system should keep the functional overlap between the kernel and the real-time core minimal. To this end, a real-time thread should be merely seen as a regular task with additional scheduling capabilities guaranteeing very low response times. To support such idea, Dovetail enables kthreads and regular user tasks to run alternatively in the out-of-band execution context introduced by the interrupt pipeline (aka head stage), or the common in-band kernel context for GPOS operations (aka root stage).\nAs a result, real-time core applications in user-space benefit from the common Linux programming model - including virtual memory protection -, and still have access to the regular Linux services when carrying out non time-critical work.\nTBC.\n"
},
{
	"uri": "https://dovetail.xenomai.org/pipeline/porting/irqflow/",
	"title": "Interrupt flow",
	"tags": [],
	"description": "",
	"content": " Pipelined interrupt flow Interrupt pipelining involves a basic change in controlling the interrupt flow: handle_domain_irq() from the IRQ domain API redirects all parent IRQs to the pipeline entry by calling generic_pipeline_irq().\n Redirecting the interrupt flow to the pipeline\n asm_irq_entry -\u0026gt; irqchip_handle_irq() -\u0026gt; handle_domain_irq() -\u0026gt; generic_pipeline_irq() -\u0026gt; irq_flow_handler() -\u0026gt; handle_oob_irq()  IRQ flow handling Generic flow handlers acknowledge the incoming IRQ event in the hardware as usual, by calling the appropriate irqchip routine (e.g. irq_ack(), irq_eoi()) according to the interrupt type. However, the flow handlers do not immediately invoke the in-band interrupt handlers. Instead, they hand the event over to the pipeline core by calling handle_oob_irq().\nIf an out-of-band handler exists for the interrupt received, handle_oob_irq() invokes it immediately, after switching the execution context to the head stage if not current yet. Otherwise, the event is marked as pending in the root stage\u0026rsquo;s log for the current CPU.\nThis is important to notice: every interrupt which is not handled by an out-of-band handler will end up into the root stage\u0026rsquo;s event log. This means that all external interrupts must have a handler in the in-band code - which should be the case for a sane kernel anyway.\n Once generic_pipeline_irq() has returned, if the preempted execution context was running over the root stage unstalled, the pipeline core synchronizes the interrupt state immediately, meaning that all IRQs found pending in the root stage\u0026rsquo;s log are immediately delivered to their respective in-band handlers. In all other situations, the IRQ frame is left immediately without running those handlers. The IRQs may remain pending until the in-band code resumes from preemption, then clears the virtual interrupt disable flag, which would cause the interrupt state to be synchronized, running the in-band handlers eventually.\nFor delivering an IRQ to the in-band handlers, the interrupt flow handler is called again by the pipeline core. When this happens, the flow handler processes the interrupt as usual, skipping the call to handle_oob_irq() though.\nYes, you did read it right: interrupt flow handlers may run twice for a single IRQ in Dovetail\u0026rsquo;s pipelined interrupt model: firstly to submit the event immediately to any out-of-band handler which may be interested in it, finally to run the in-band handler(s) accepting that event if it was not delivered to any out-of-band handler. You may construe the meaning of calling handle_oob_irq() as \u0026ldquo;let\u0026rsquo;s poll for any out-of-band handler which might be interested in this event\u0026rdquo;. This also means that any interrupt can have either an in-band or an out-of-band handler, but not both.\n In absence of any out-of-band handler for the event, the device may keep asserting the interrupt signal until the cause has been lifted in its own registers. At the same time, we might not be allowed to run the in-band handler immediately over the current interrupt context if the root stage is currently stalled, we would have to wait for the in-band code to accept interrupts again. However, the interrupt disable bit in the CPU would certainly be cleared in the meantime. For this reason, depending on the interrupt type, the flow handlers as modified by the pipeline code may have to mask the interrupt line until the in-band handler has run from the root stage, lifting the interrupt cause. This typically happens with level-triggered interrupts, preventing the device from storming the CPU with a continuous interrupt request.\nSince all of the IRQ handlers sharing an interrupt line are either in-band or out-of-band in a mutually exclusive way, such masking cannot delay out-of-band events.\n The pathological case of deferring level-triggered IRQs\n /* root stage stalled on entry, no OOB handler */ asm_irq_entry ... -\u0026gt; generic_pipeline_irq() ... \u0026lt;IRQ logged, delivery deferred\u0026gt; asm_irq_exit /* * CPU allowed to accept interrupts again with IRQ cause not * acknowledged in device yet =\u0026gt; **IRQ storm**. */ asm_irq_entry ... asm_irq_exit asm_irq_entry ... asm_irq_exit  The logic behind masking interrupt lines until events are processed at some point later - out of the original interrupt context - also applies to the threaded interrupt model. In this case, interrupt lines may be masked until the IRQ thread is scheduled in, after the interrupt handler clears the event cause eventually.\n The logic for adapting flow handlers dealing with any kind of interrupt to pipelining can be decomposed in the following steps:\n (optionally) enter the critical section protected by the IRQ descriptor lock, if the interrupt is shared among processors (e.g. device interrupts). If so, check if the interrupt handler may run on the current CPU (irq_may_run()). By definition, no locking would be required for per-CPU interrupts.\n check whether we are entering the pipeline in order to deliver the interrupt to any out-of-band handler registered for it. on_pipeline_entry() returns a boolean value denoting this situation.\n if on pipeline entry, we should pass the event on to the pipeline core by calling handle_oob_irq(). Upon return, this routine tells the caller whether any out-of-band handler was fired for the event.\n if so, we may assume that the interrupt cause is now cleared in the device, and we may leave the flow handler, after having restored the interrupt line into a normal state. In case of a level-triggered interrupt which has been masked on entry to the flow handler, we need to unmask the line before leaving.\n if no out-of-band handler was called, we should have performed any acknowledge and/or EOI to release the interrupt line, while leaving it masked if required before exiting the flow handler. In case of a level-triggered interrupt, we do want to leave it masked for solving the pathological case with interrupt deferral explained earlier.\n  if not on pipeline entry (i.e. second entry of the flow handler), then we must be running over the root stage, accepting interrupts, therefore we should fire the in-band handler(s) for the incoming event.\n  The original code for handling level-triggered interrupts is adapted to interrupt pipelining according to the rules above, as follows:\n Example: adapting the handler dealing with level-triggered IRQs\n --- a/kernel/irq/chip.c +++ b/kernel/irq/chip.c void handle_level_irq(struct irq_desc *desc) { raw_spin_lock(\u0026amp;desc-\u0026gt;lock); mask_ack_irq(desc); if (!irq_may_run(desc)) goto out_unlock; +\tif (on_pipeline_entry()) { +\tif (handle_oob_irq(desc)) +\tgoto out_unmask; +\tgoto out_unlock; +\t} + desc-\u0026gt;istate \u0026amp;= ~(IRQS_REPLAY | IRQS_WAITING); /* @@ -642,7 +686,7 @@ void handle_level_irq(struct irq_desc *desc) kstat_incr_irqs_this_cpu(desc); handle_irq_event(desc); - +out_unmask: cond_unmask_irq(desc); out_unlock: raw_spin_unlock(\u0026amp;desc-\u0026gt;lock); }  This change reads as follows:\n on entering the pipeline, which means immediately over the interrupt frame context set up by the CPU for receiving the event, tell the pipeline core about the incoming IRQ.\n if this IRQ was handled by an out-of-band handler (handle_oob_irq() returns true), consider the event to have been fully processed, unmasking the interrupt line before leaving. We can\u0026rsquo;t do more than this, simply because the in-band kernel code might expect not to receive any interrupt at this point (i.e. the virtual interrupt disable flag might be set for the root stage).\n otherwise, keep the interrupt line masked until handle_level_irq() is called again from a safe context for handling in-band interrupts, at which point the event should be delivered to the (regular) in-band interrupt handler. We have to keep the line masked to prevent the IRQ storm which would certainly happen otherwise, since no handler has cleared the cause of the interrupt event in the device yet.\n  IRQ chip drivers irqchip drivers need to be specifically adapted for supporting the pipelined interrupt model. The basic task is to ensure that the following struct irq_chip handlers - if defined - can be called from an out-of-band context safely:\n irq_mask() irq_ack() irq_mask_ack() irq_eoi() irq_unmask()  Such handler is deemed safe to be called from out-of-band context when it does not invoke any in-band kernel service, which might cause an invalid context re-entry.\nThe generic IRQ management core serializes calls to irqchip handlers for a given IRQ by serializing access to its interrupt descriptor, acquiring the per-descriptor irq_desc::lock spinlock. Holding irq_desc::lock when running a handler for any IRQ shared between all CPUs ensures that a single CPU handles the event. This - originally - raw spinlock is automatically turned into a mutable spinlock when pipelining interrupts.\nIn addition, there might be inner spinlocks defined by some irqchip drivers for serializing handlers accessing a common interrupt controller hardware for distinct IRQs from multiple CPUs concurrently. Adapting such spinlocked sections found in irqchip drivers to support interrupt pipelining may involve converting the related spinlocks to hard spinlocks.\nOther section of code which were originally serialized by common interrupt disabling may need to be made fully atomic for running consistenly in pipelined interrupt mode. This can be done by introducing hard masking with hard_local_irq_save(), hard_local_irq_restore().\nFinally, IRQCHIP_PIPELINE_SAFE must be added to struct irqchip::flags member of a pipeline-aware irqchip driver, in order to notify the kernel that such controller can operate in pipelined interrupt mode.\n Adapting the ARM GIC driver to interrupt pipelining\n --- a/drivers/irqchip/irq-gic.c +++ b/drivers/irqchip/irq-gic.c @@ -93,7 +93,7 @@ struct gic_chip_data { #ifdef CONFIG_BL_SWITCHER -static DEFINE_RAW_SPINLOCK(cpu_map_lock); +static DEFINE_HARD_SPINLOCK(cpu_map_lock); #define gic_lock_irqsave(f)\t\\ raw_spin_lock_irqsave(\u0026amp;cpu_map_lock, (f)) @@ -424,7 +424,8 @@ static const struct irq_chip gic_chip = { .irq_set_irqchip_state\t= gic_irq_set_irqchip_state, .flags\t= IRQCHIP_SET_TYPE_MASKED | IRQCHIP_SKIP_SET_WAKE | -\tIRQCHIP_MASK_ON_SUSPEND, +\tIRQCHIP_MASK_ON_SUSPEND | +\tIRQCHIP_PIPELINE_SAFE, }; void __init gic_cascade_irq(unsigned int gic_nr, unsigned int irq)  In some (rare) cases, we might have a bit more work for adapting an interrupt chip driver. For instance, we might have to convert a sleeping spinlock to a raw spinlock first, so that we can convert the latter to a hard spinlock eventually. Hard spinlocks like raw ones should be manipulated via the raw_spin_lock() API, unlike sleeping spinlocks. This change makes even more sense as sleeping is not allowed while running in the pipeline entry context anyway.\n Adapting the BCM2835 pin control driver to interrupt pipelining\n --- a/drivers/pinctrl/bcm/pinctrl-bcm2835.c +++ b/drivers/pinctrl/bcm/pinctrl-bcm2835.c @@ -90,7 +90,7 @@ struct bcm2835_pinctrl { struct gpio_chip gpio_chip; struct pinctrl_gpio_range gpio_range; -\tspinlock_t irq_lock[BCM2835_NUM_BANKS]; +\thard_spinlock_t irq_lock[BCM2835_NUM_BANKS]; }; /* pins are just named GPIO0..GPIO53 */ @@ -461,10 +461,10 @@ static void bcm2835_gpio_irq_enable(struct irq_data *data) unsigned bank = GPIO_REG_OFFSET(gpio); unsigned long flags; -\tspin_lock_irqsave(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); +\traw_spin_lock_irqsave(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); set_bit(offset, \u0026amp;pc-\u0026gt;enabled_irq_map[bank]); bcm2835_gpio_irq_config(pc, gpio, true); -\tspin_unlock_irqrestore(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); +\traw_spin_unlock_irqrestore(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); } static void bcm2835_gpio_irq_disable(struct irq_data *data) @@ -476,12 +476,12 @@ static void bcm2835_gpio_irq_disable(struct irq_data *data) unsigned bank = GPIO_REG_OFFSET(gpio); unsigned long flags; -\tspin_lock_irqsave(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); +\traw_spin_lock_irqsave(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); bcm2835_gpio_irq_config(pc, gpio, false); /* Clear events that were latched prior to clearing event sources */ bcm2835_gpio_set_bit(pc, GPEDS0, gpio); clear_bit(offset, \u0026amp;pc-\u0026gt;enabled_irq_map[bank]); -\tspin_unlock_irqrestore(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); +\traw_spin_unlock_irqrestore(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); } static int __bcm2835_gpio_irq_set_type_disabled(struct bcm2835_pinctrl *pc, @@ -584,7 +584,7 @@ static int bcm2835_gpio_irq_set_type(struct irq_data *data, unsigned int type) unsigned long flags; int ret; -\tspin_lock_irqsave(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); +\traw_spin_lock_irqsave(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); if (test_bit(offset, \u0026amp;pc-\u0026gt;enabled_irq_map[bank])) ret = __bcm2835_gpio_irq_set_type_enabled(pc, gpio, type); @@ -596,7 +596,7 @@ static int bcm2835_gpio_irq_set_type(struct irq_data *data, unsigned int type) else irq_set_handler_locked(data, handle_level_irq); -\tspin_unlock_irqrestore(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); +\traw_spin_unlock_irqrestore(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); return ret; } @@ -618,6 +618,7 @@ static struct irq_chip bcm2835_gpio_irq_chip = { .irq_ack = bcm2835_gpio_irq_ack, .irq_mask = bcm2835_gpio_irq_disable, .irq_unmask = bcm2835_gpio_irq_enable, +\t.flags = IRQCHIP_PIPELINE_SAFE, }; static int bcm2835_pctl_get_groups_count(struct pinctrl_dev *pctldev) @@ -1047,7 +1048,7 @@ static int bcm2835_pinctrl_probe(struct platform_device *pdev) for_each_set_bit(offset, \u0026amp;events, 32) bcm2835_gpio_wr(pc, GPEDS0 + i * 4, BIT(offset)); -\tspin_lock_init(\u0026amp;pc-\u0026gt;irq_lock[i]); +\traw_spin_lock_init(\u0026amp;pc-\u0026gt;irq_lock[i]); } err = gpiochip_add_data(\u0026amp;pc-\u0026gt;gpio_chip, pc);  irq_set_chip() will complain loudly with a kernel warning whenever the irqchip descriptor passed does not bear the IRQCHIP_PIPELINE_SAFE flag and CONFIG_IRQ_PIPELINE is enabled. Take this warning as a sure sign that your port of the IRQ pipeline to the target system is incomplete.\n Kernel preemption control (PREEMPT) When pipelining is enabled, preempt_schedule_irq() reconciles the virtual interrupt state - which has not been touched by the assembly level code upon kernel entry - with basic assumptions made by the scheduler core, such as entering with (virtual) interrupts disabled.\nExtended IRQ work API Due to the NMI-type nature of interrupts running out-of-band code, such code might preempt in-band activities over the root stage in the middle of a critical section. For this reason, it would be unsafe to call any in-band routine from an out-of-band context.\nHowever, we may schedule execution of in-band work handlers from out-of-band code, using the regular irq_work_queue() service which has been extended by the IRQ pipeline core. Such work request from the head stage is scheduled for running over the root stage on the issuing CPU as soon as the out-of-band activity quiesces on this processor. As its name implies, the work handler runs in (in-band) interrupt context.\nThe interrupt pipeline forces the use of a synthetic IRQ as a notification signal for the IRQ work machinery, instead of a hardware-specific interrupt vector. This special IRQ is labeled in-band work when reported by /proc/interrupts.\n "
},
{
	"uri": "https://dovetail.xenomai.org/pipeline/porting/",
	"title": "Porting",
	"tags": [],
	"description": "",
	"content": "How to port this thing.\n"
},
{
	"uri": "https://dovetail.xenomai.org/contributing/",
	"title": "Contributing to Dovetail",
	"tags": [],
	"description": "",
	"content": " Submitting patches Working on Dovetail should be customary Linux kernel development happening out-of-tree at the present time, so the same rules apply when submitting patches, except that:\n your code should be based on the tip of the Dovetail master branch at the time of the submission (see below for the location of our GIT tree).\n you should send all contributions and communicate via the Xenomai mailing list, not the LKML.\n  GIT trees You can clone the Dovetail code base indifferently from those URLs:\n git://lab.xenomai.org/linux-dovetail.git https://lab.xenomai.org/linux-dovetail.git  In addition, a cgit server is running on that tree at https://lab.xenomai.org/linux-dovetail.git.\nOrganization and workflow Our basic goal is to keep the Dovetail development as close as possible to the tip of the mainline kernel.\nWe are not in the business of maintaining vendor ports, or long-term maintenance releases of Dovetail, although anyone would be welcome to take on such task. However, we are very much in the business of lowering the engineering cost of maintaining a current dual kernel interface for the Linux kernel, by aiming at a lean and mean implementation.\n To this end, the following process is implemented:\n Development takes place in the master branch. This branch is always based on an official release milestone of the mainline kernel. This may be:\n a base stable release (e.g. v4.18) a release candidate (e.g. v4.19-rc1)  When the mainline kernel reaches the next release milestone we are interested in, a snapshot of our master branch is taken, named after the upstream release it was based on. Snapshot branches are named snapshot/\u0026lt;upstream-release\u0026gt;-dtl in our GIT tree.\n Once a snapshot is taken, some of the Dovetail-related commits in the master branch may be rearranged (moved and/or squashed), so as to present Dovetail as a clean-cut, incremental set of changes on top of the upstream kernel release, where logically bound modifications are grouped. On the other hand, individual commits are preserved in snapshots which are frozen branches.\n Our master branch is eventually rebased on the next upstream release we are interested in.\n  At times, we may convert some of the older snapshot branches to plain tags if/when having too many branches hanging around becomes impractical.\n "
},
{
	"uri": "https://dovetail.xenomai.org/pipeline/usage/",
	"title": "Usage",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://dovetail.xenomai.org/pipeline/porting/arch/",
	"title": "Architecture-specific bits",
	"tags": [],
	"description": "",
	"content": " Interrupt mask virtualization The architecture-specific code which manipulates the interrupt flag in the CPU\u0026rsquo;s state register in arch//include/asm/irqflags.h should be split between real and virtual interrupt control. The real interrupt control operations are inherited from the in-band kernel implementation. The virtual ones should be built upon services provided by the interrupt pipeline core.\n firstly, the original arch_local_* helpers should be renamed as native_* helpers, affecting the hardware interrupt state in the CPU.   Example: introducing the native interrupt state accessors for the ARM architecture\n --- a/arch/arm/include/asm/irqflags.h +++ b/arch/arm/include/asm/irqflags.h #if __LINUX_ARM_ARCH__ \u0026gt;= 6 #define arch_local_irq_save arch_local_irq_save -static inline unsigned long arch_local_irq_save(void) +static inline unsigned long native_irq_save(void) { unsigned long flags; asm volatile( -\t\u0026quot;\tmrs\t%0, \u0026quot; IRQMASK_REG_NAME_R \u0026quot;\t@ arch_local_irq_save\\n\u0026quot; +\t\u0026quot;\tmrs\t%0, \u0026quot; IRQMASK_REG_NAME_R \u0026quot;\t@ native_irq_save\\n\u0026quot; \u0026quot;\tcpsid\ti\u0026quot; : \u0026quot;=r\u0026quot; (flags) : : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;); return flags; } #define arch_local_irq_enable arch_local_irq_enable -static inline void arch_local_irq_enable(void) +static inline void native_irq_enable(void) { asm volatile( -\t\u0026quot;\tcpsie i\t@ arch_local_irq_enable\u0026quot; +\t\u0026quot;\tcpsie i\t@ native_irq_enable\u0026quot; : : : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;); } #define arch_local_irq_disable arch_local_irq_disable -static inline void arch_local_irq_disable(void) +static inline void native_irq_disable(void) { asm volatile( -\t\u0026quot;\tcpsid i\t@ arch_local_irq_disable\u0026quot; +\t\u0026quot;\tcpsid i\t@ native_irq_disable\u0026quot; : : : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;); @@ -69,12 +76,12 @@ static inline void arch_local_irq_disable(void) * Save the current interrupt enable state \u0026amp; disable IRQs */ #define arch_local_irq_save arch_local_irq_save -static inline unsigned long arch_local_irq_save(void) +static inline unsigned long native_irq_save(void) { unsigned long flags, temp; asm volatile( -\t\u0026quot;\tmrs\t%0, cpsr\t@ arch_local_irq_save\\n\u0026quot; +\t\u0026quot;\tmrs\t%0, cpsr\t@ native_irq_save\\n\u0026quot; \u0026quot;\torr\t%1, %0, #128\\n\u0026quot; \u0026quot;\tmsr\tcpsr_c, %1\u0026quot; : \u0026quot;=r\u0026quot; (flags), \u0026quot;=r\u0026quot; (temp) @@ -87,11 +94,11 @@ static inline unsigned long arch_local_irq_save(void) * Enable IRQs */ #define arch_local_irq_enable arch_local_irq_enable -static inline void arch_local_irq_enable(void) +static inline void native_irq_enable(void) { unsigned long temp; asm volatile( -\t\u0026quot;\tmrs\t%0, cpsr\t@ arch_local_irq_enable\\n\u0026quot; +\t\u0026quot;\tmrs\t%0, cpsr\t@ native_irq_enable\\n\u0026quot; \u0026quot;\tbic\t%0, %0, #128\\n\u0026quot; \u0026quot;\tmsr\tcpsr_c, %0\u0026quot; : \u0026quot;=r\u0026quot; (temp) @@ -103,11 +110,11 @@ static inline void arch_local_irq_enable(void) * Disable IRQs */ #define arch_local_irq_disable arch_local_irq_disable -static inline void arch_local_irq_disable(void) +static inline void native_irq_disable(void) { unsigned long temp; asm volatile( -\t\u0026quot;\tmrs\t%0, cpsr\t@ arch_local_irq_disable\\n\u0026quot; +\t\u0026quot;\tmrs\t%0, cpsr\t@ native_irq_disable\\n\u0026quot; \u0026quot;\torr\t%0, %0, #128\\n\u0026quot; \u0026quot;\tmsr\tcpsr_c, %0\u0026quot; : \u0026quot;=r\u0026quot; (temp) @@ -153,11 +160,11 @@ static inline void arch_local_irq_disable(void) * Save the current interrupt enable state. */ #define arch_local_save_flags arch_local_save_flags -static inline unsigned long arch_local_save_flags(void) +static inline unsigned long native_save_flags(void) { unsigned long flags; asm volatile( -\t\u0026quot;\tmrs\t%0, \u0026quot; IRQMASK_REG_NAME_R \u0026quot;\t@ local_save_flags\u0026quot; +\t\u0026quot;\tmrs\t%0, \u0026quot; IRQMASK_REG_NAME_R \u0026quot;\t@ native_save_flags\u0026quot; : \u0026quot;=r\u0026quot; (flags) : : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;); return flags; } @@ -166,21 +173,28 @@ static inline unsigned long arch_local_save_flags(void) * restore saved IRQ \u0026amp; FIQ state */ #define arch_local_irq_restore arch_local_irq_restore -static inline void arch_local_irq_restore(unsigned long flags) +static inline void native_irq_restore(unsigned long flags) { asm volatile( -\t\u0026quot;\tmsr\t\u0026quot; IRQMASK_REG_NAME_W \u0026quot;, %0\t@ local_irq_restore\u0026quot; +\t\u0026quot;\tmsr\t\u0026quot; IRQMASK_REG_NAME_W \u0026quot;, %0\t@ native_irq_restore\u0026quot; : : \u0026quot;r\u0026quot; (flags) : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;); } #define arch_irqs_disabled_flags arch_irqs_disabled_flags -static inline int arch_irqs_disabled_flags(unsigned long flags) +static inline int native_irqs_disabled_flags(unsigned long flags) { return flags \u0026amp; IRQMASK_I_BIT; } +static inline bool native_irqs_disabled(void) +{ +\tunsigned long flags = native_save_flags(); +\treturn native_irqs_disabled_flags(flags); +} + +#include \u0026lt;asm/irq_pipeline.h\u0026gt; #include \u0026lt;asm-generic/irqflags.h\u0026gt; #endif /* ifdef __KERNEL__ */   finally, a new set of arch_local_* helpers should be provided, affecting the virtual interrupt disable flag implemented by the pipeline core for controlling the root stage protection against interrupts. It is good practice to implement this set in a separate file available for inclusion from \u0026lt;asm/irq_pipeline.h\u0026gt;.   Example: providing the virtual interrupt state accessors for the ARM architecture\n--- /dev/null +++ b/arch/arm/include/asm/irq_pipeline.h @@ -0,0 +1,138 @@ +#ifndef _ASM_ARM_IRQ_PIPELINE_H +#define _ASM_ARM_IRQ_PIPELINE_H + +#include \u0026lt;asm-generic/irq_pipeline.h\u0026gt; + +#ifdef CONFIG_IRQ_PIPELINE + +static inline notrace unsigned long arch_local_irq_save(void) +{ +\tint stalled = root_irq_save(); +\tbarrier(); +\treturn arch_irqs_virtual_to_native_flags(stalled); +} + +static inline notrace void arch_local_irq_enable(void) +{ +\tbarrier(); +\troot_irq_enable(); +} + +static inline notrace void arch_local_irq_disable(void) +{ +\troot_irq_disable(); +\tbarrier(); +} + +static inline notrace unsigned long arch_local_save_flags(void) +{ +\tint stalled = root_irqs_disabled(); +\tbarrier(); +\treturn arch_irqs_virtual_to_native_flags(stalled); +} + +static inline int arch_irqs_disabled_flags(unsigned long flags) +{ +\treturn native_irqs_disabled_flags(flags); +} + +static inline notrace void arch_local_irq_restore(unsigned long flags) +{ +\tif (!arch_irqs_disabled_flags(flags)) +\t__root_irq_enable(); +\tbarrier(); +} + +#else /* !CONFIG_IRQ_PIPELINE */ + +static inline unsigned long arch_local_irq_save(void) +{ +\treturn native_irq_save(); +} + +static inline void arch_local_irq_enable(void) +{ +\tnative_irq_enable(); +} + +static inline void arch_local_irq_disable(void) +{ +\tnative_irq_disable(); +} + +static inline unsigned long arch_local_save_flags(void) +{ +\treturn native_save_flags(); +} + +static inline void arch_local_irq_restore(unsigned long flags) +{ +\tnative_irq_restore(flags); +} + +static inline int arch_irqs_disabled_flags(unsigned long flags) +{ +\treturn native_irqs_disabled_flags(flags); +} + +#endif /* !CONFIG_IRQ_PIPELINE */ + +#endif /* _ASM_ARM_IRQ_PIPELINE_H */   This new file should include \u0026lt;asm-generic/irq_pipeline.h\u0026gt; early to get access to the pipeline declarations it needs. This inclusion should be unconditional, even if the kernel is built with CONFIG_IRQ_PIPELINE disabled.\n Providing support for merged interrupt states The generic interrupt pipeline implementation requires the arch-level support code to provide for a pair of helpers aimed at translating the virtual interrupt disable flag to the interrupt bit in the CPU\u0026rsquo;s status register (e.g. PSR_I_BIT for ARM) and conversely. These helpers are used to create combined state words merging the virtual and real interrupt states.\n arch_irqs_virtual_to_native_flags(int stalled) must return a long word remapping the boolean value of @stalled to the CPU\u0026rsquo;s interrupt bit position in the status register. All other bits must be cleared.\n On ARM, this can be expressed as (stalled ? PSR_I_BIT : 0). on x86, that would rather be (stalled ? 0 : X86_EFLAGS_IF).  arch_irqs_native_to_virtual_flags(unsigned long flags) must return a long word remapping the CPU\u0026rsquo;s interrupt bit in @flags to an arbitrary bit position, choosen not to conflict with the former. In other words, the CPU\u0026rsquo;s interrupt state bit received in @flags should be shifted to a free position picked arbitrarily in the return value. All other bits must be cleared.\n On ARM, using bit position 31 to reflect the virtual state, this is expressed as (hard_irqs_disabled_flags(flags) ? (1 \u0026lt;\u0026lt; 31) : 0).\n On any other architecture, the implementation would be similar, using whatever bit position is available which would not conflict with the CPU\u0026rsquo;s interrupt bit position.\n   --- a/arch/arm/include/asm/irqflags.h +++ b/arch/arm/include/asm/irqflags.h /* * CPU interrupt mask handling. */ #ifdef CONFIG_CPU_V7M #define IRQMASK_REG_NAME_R \u0026quot;primask\u0026quot; #define IRQMASK_REG_NAME_W \u0026quot;primask\u0026quot; #define IRQMASK_I_BIT\t1 +#define IRQMASK_I_POS\t0 #else #define IRQMASK_REG_NAME_R \u0026quot;cpsr\u0026quot; #define IRQMASK_REG_NAME_W \u0026quot;cpsr_c\u0026quot; #define IRQMASK_I_BIT\tPSR_I_BIT +#define IRQMASK_I_POS\t7 #endif +#define IRQMASK_i_POS\t31  IRQMASK_i_POS (note the minus \u0026lsquo;i\u0026rsquo;) is the free bit position in the combo word where the ARM port stores the original CPU\u0026rsquo;s interrupt state in the combo word. This position can\u0026rsquo;t conflict with IRQMASK_I_POS, which is an alias to PSR_I_BIT (bit position 0 or 7).\n --- /dev/null +++ b/arch/arm/include/asm/irq_pipeline.h + +static inline notrace +unsigned long arch_irqs_virtual_to_native_flags(int stalled) +{ +\treturn (!!stalled) \u0026lt;\u0026lt; IRQMASK_I_POS; +} +static inline notrace +unsigned long arch_irqs_native_to_virtual_flags(unsigned long flags) +{ +\treturn (!!hard_irqs_disabled_flags(flags)) \u0026lt;\u0026lt; IRQMASK_i_POS; +}  Once all of these changes are in, the generic helpers from  such as local_irq_disable() and local_irq_enable() actually refer to the virtual protection scheme when interrupts are pipelined, which eventually allows to implement interrupt deferral for the protected in-band code running over the root stage.\n Adapting the assembly code to IRQ pipelining Interrupt entry As generic IRQ handling is a requirement for supporting Dovetail, the low-level interrupt handler living in the assembly portion of the architecture code can still deliver all interrupt events to the original C handler provided by the irqchip driver. That handler should in turn invoke:\n handle_domain_irq() for parent device IRQs\n generic_handle_irq() for cascaded device IRQs (decoded from the parent handler)\n  For those routines, the initial task of inserting an interrupt at the head of the pipeline is directly handled from the genirq layer they belong to. This means that there is usually not much to do other than making a quick check in the implementation of the parent IRQ handler, in the relevant irqchip driver, applying the rules of thumb carefully.\nOn some ARM platform equipped with a fairly common GIC controller, that would mean inspecting the function gic_handle_irq() for instance.\n  the arch-specific handle_IPI() or equivalent for special inter-processor interrupts  IPIs must be dealt with by specific changes introduced by the port we will cover later.\nInterrupt exit When interrupt pipelining is disabled, the kernel normally runs an epilogue after each interrupt or exception event was handled. If the event happened while the CPU was running some kernel code, the epilogue would check for a potential rescheduling opportunity in case CONFIG_PREEMPT is enabled. If a user-space task was preempted by the event, additional conditions would be checked for such as a signal pending delivery for that task.\nBecause interrupts are only virtually masked for the in-band code when pipelining is enabled, IRQs can still be taken by the CPU and passed on to the low-level assembly handlers, so that they can enter the interrupt pipeline.\n Running the regular epilogue afer an IRQ is valid only if the kernel was actually accepting interrupts when the event happened (i.e. the virtual interrupt disable flag was clear), and running in-band code.\n In all other cases, except for the interrupt pipeline core, the rest of the kernel does not expect those IRQs to ever happen in the first place. Therefore, running the epilogue in such circumstances would be at odds with the kernel\u0026rsquo;s logic. In addition, low-level handlers must have been made aware that they might receive an event under such conditions.\nFor instance, the original ARM code for handling an IRQ which has preempted a kernel context would look like this:\n__irq_svc: svc_entry irq_handler #ifdef CONFIG_PREEMPT ldr\tr8, [tsk, #TI_PREEMPT]\t@ get preempt count ldr\tr0, [tsk, #TI_FLAGS]\t@ get flags teq\tr8, #0\t@ if preempt count != 0 movne\tr0, #0\t@ force flags to 0 tst\tr0, #_TIF_NEED_RESCHED blne\tsvc_preempt #endif  In order to properly handle interrupts in a pipelined delivery model, we have to detect whether the in-band kernel was ready to receive such event, acting upon it accordingly. To this end, the ARM port passes the event to a trampoline routine instead (handle_arch_irq_pipelined()), expecting on return a decision whether or not the epilogue code should run next. In the illustration below, this decision is returned as a boolean status to the caller, non-zero meaning that we may run the epilogue, zero otherwise.\n--- a/arch/arm/kernel/entry-armv.S +++ b/arch/arm/kernel/entry-armv.S .macro\tirq_handler #ifdef CONFIG_MULTI_IRQ_HANDLER -\tldr\tr1, =handle_arch_irq mov\tr0, sp badr\tlr, 9997f +#ifdef CONFIG_IRQ_PIPELINE +\tldr\tr1, =handle_arch_irq_pipelined +\tmov\tpc, r1 +#else\t+\tldr\tr1, =handle_arch_irq ldr\tpc, [r1] -#else +#endif +#elif CONFIG_IRQ_PIPELINE +#error \u0026quot;Legacy IRQ handling not pipelined\u0026quot; +#else\tarch_irq_handler_default #endif 9997: .endm  The trampoline routine added to the original code, first delivers the interrupt to the machine-defined handler, then tells the caller whether the regular epilogue may run for such event.\n--- a/arch/arm/kernel/irq.c +++ b/arch/arm/kernel/irq.c @@ -112,6 +112,15 @@ void __init set_handle_irq(void (*handle_irq)(struct pt_regs *)) } #endif +#ifdef CONFIG_IRQ_PIPELINE +asmlinkage int __exception_irq_entry +handle_arch_irq_pipelined(struct pt_regs *regs) +{ +\thandle_arch_irq(regs); +\treturn on_root_stage() \u0026amp;\u0026amp; !irqs_disabled(); +} +#endif +  Eventually, the low-level assembly handler receiving the interrupt event is adapted, in order to carry out the earlier decision by handle_arch_irq_pipelined(), skipping the epilogue code if required to.\n--- a/arch/arm/kernel/entry-armv.S +++ b/arch/arm/kernel/entry-armv.S __irq_svc: svc_entry irq_handler +#ifdef CONFIG_IRQ_PIPELINE +\ttst\tr0, r0 +\tbeq\t1f +#endif #ifdef CONFIG_PREEMPT ldr\tr8, [tsk, #TI_PREEMPT]\t@ get preempt count blne\tsvc_preempt #endif +1: svc_exit r5, irq = 1\t@ return from exception  Taking the fast exit path when applicable is critical to the stability of the target system to prevent invalid re-entry of the in-band kernel code.\n Fault exit Similarly to the interrupt exit case, the low-level fault handling code must skip the epilogue code when the fault was taken over an out-of-band context. Upon fault, the current interrupt state is not considered for determining whether we should run the epilogue, since a fault may occur independently of such state.\n Running the regular epilogue after a fault is valid only if that fault was triggered by some in-band code, excluding any fault raised by out-of-band code.\n For instance, the original ARM code for returning from an exception event would be modified as follows:\n--- a/arch/arm/kernel/entry-armv.S +++ b/arch/arm/kernel/entry-armv.S @@ -754,7 +772,7 @@ ENTRY(ret_from_exception) UNWIND(.cantunwind\t) get_thread_info tsk mov\twhy, #0 -\tb\tret_to_user +\tret_to_user_pipelined r1 UNWIND(.fnend\t)  With the implementation of ret_to_user_pipelined checking for the current stage, skipping the epilogue if the faulting code was running over an out-of-band context:\n--- a/arch/arm/kernel/entry-header.S +++ b/arch/arm/kernel/entry-header.S +/* + * Branch to the exception epilogue, skipping the in-band work + * if running over the head interrupt stage. + */ +\t.macro ret_to_user_pipelined, tmp +#ifdef CONFIG_IRQ_PIPELINE +\tldr\t\\tmp, [tsk, #TI_LOCAL_FLAGS] +\ttst\t\\tmp, #_TLF_HEAD +\tbne\tfast_ret_to_user +#endif +\tb\tret_to_user +\t.endm +  _TLF_HEAD is a local thread_info flag denoting a current task running out-of-band code over the head stage. If set, the epilogue must be skipped.\nReconciling the virtual interrupt state to the epilogue logic A tricky issue to address when pipelining interrupts is about making sure that the logic from the epilogue routine (e.g. do_work_pending(), do_notify_resume()) actually runs in the expected (virtual) interrupt state for the root stage.\nReconciling the virtual interrupt state to the in-band logic dealing with interrupts is required because in a pipelined interrupt model, the virtual interrupt state of the root stage does not necessarily reflect the CPU\u0026rsquo;s interrupt state on entry to the early assembly code handling the IRQ events. Typically, a CPU would always automatically disable interrupts hardware-wise when taking an IRQ, which may contradict the software-managed virtual state until both are eventually reconciled.\nThose rules of thumb should be kept in mind when adapting the epilogue routine to interrupt pipelining:\n most often, such routine is supposed to be entered with (hard) interrupts off when called from the assembly code which handles kernel entry/exit transitions (e.g. arch/arm/kernel/entry-common.S). Therefore, this routine may have to reconcile the virtual interrupt state with such expectation, since according to the interrupt exit rules we discussed earlier, such state has to be originally enabled (i.e. the root stall bit is clear) for the epilogue code to run in the first place.\n conversely, we must keep the hard interrupt state consistent upon return from the epilogue code with the one received on entry. Typically, hard interrupts must be disabled before leaving this code if we entered it that way.\n likewise, we must also keep the virtual interrupt state consistent upon return of the epilogue code with the one received on entry. In other words, the stall bit of the root stage must be restored to its original state on entry before leaving this code.\n schedule() must be called with interrupts virtually disabled for the root stage, but the CPU\u0026rsquo;s interrupt state should allow for IRQs to be taken in order to minimize latency for the head stage.\n generally speaking, while we may need the root stage to be stalled when the in-band kernel code expects this, we still want most of the epilogue code to run with hard interrupts enabled to shorten the interrupt latency for the head stage, where co-kernels live.\n  Dealing with IPIs Support for inter-processor interrupts is architecture-specific code.\nTBC.\n"
},
{
	"uri": "https://dovetail.xenomai.org/pipeline/usage/synthetic/",
	"title": "Synthetic IRQs",
	"tags": [],
	"description": "",
	"content": " The pipeline introduces an additional type of interrupts, which are purely software-originated, with no hardware involvement. These IRQs can be triggered by any kernel code. Synthetic IRQs are inherently per-CPU events. Because the common pipeline flow applies to synthetic interrupts, it is possible to attach them to out-of-band and/or in-band handlers, just like device interrupts.\nSynthetic interrupts abide by the normal rules with respect to interrupt masking: such IRQs may be deferred until the stage they should be handled from is unstalled.\nSynthetic interrupts and regular softirqs differ in essence: the latter only exist in the in-band context, and therefore cannot trigger out-of-band activities. Synthetic interrupts used to be called virtual IRQs (or virq for short) by the legacy I-pipe implementation, Dovetail\u0026rsquo;s ancestor; such rename clears the confusion with the way abstract interrupt numbers defined within interrupt domains may be called elsewhere in the kernel code base (i.e. virtual interrupts too).\n Allocating synthetic interrupts Synthetic interrupt vectors are allocated from the synthetic_irq_domain, using the irq_create_direct_mapping() routine.\nA synthetic interrupt handler can be installed for running on the root stage upon a scheduling request (i.e. being posted) from an out-of-band context as follows:\n#include \u0026lt;linux/irq_pipeline.h\u0026gt; static irqreturn_t sirq_handler(int sirq, void *dev_id) { do_in_band_work(); return IRQ_HANDLED; } static struct irqaction sirq_action = { .handler = sirq_handler, .name = \u0026quot;In-band synthetic interrupt\u0026quot;, .flags = IRQF_NO_THREAD, }; unsigned int alloc_sirq(void) { unsigned int sirq; sirq = irq_create_direct_mapping(synthetic_irq_domain); if (!sirq) return 0; setup_percpu_irq(sirq, \u0026amp;sirq_action); return sirq; }  A synthetic interrupt handler can be installed for running from the head stage upon a trigger from an in-band context as follows:\nstatic irqreturn_t sirq_oob_handler(int sirq, void *dev_id) { do_out_of_band_work(); return IRQ_HANDLED; } unsigned int alloc_sirq(void) { unsigned int sirq; sirq = irq_create_direct_mapping(synthetic_irq_domain); if (!sirq) return 0; ret = __request_percpu_irq(sirq, sirq_oob_handler, IRQF_OOB, \u0026quot;Out-of-band synthetic interrupt\u0026quot;, dev_id); if (ret) { irq_dispose_mapping(sirq); return 0; } return sirq; }  Scheduling in-band execution of a synthetic interrupt handler The execution of sirq_handler() in the in-band context can be scheduled (or posted) from the out-of-band context in two different ways:\nUsing the common injection service irq_pipeline_inject(sirq);  Using the lightweight injection method (requires interrupts to be disabled in the CPU) unsigned long flags = hard_local_irqsave(); irq_stage_post_root(sirq); hard_local_irqrestore(flags);  Assuming that no interrupt may be pending in the event log for the head stage at the time this code runs, the second method relies on the invariant that in a pipeline interrupt model, IRQs pending for the root stage will have to wait for the head stage to quiesce before they can be handled. Therefore, it is pointless to check for synchronizing the interrupts pending for the root stage from the head stage, which the irq_pipeline_inject() service would do systematically. irq_stage_post_root() simply marks the event as pending in the event log of the root stage for the current CPU, then returns. This event would be played as a result of synchronizing the log automatically when the current CPU switches back to the root stage.\n It is also valid to post a synthetic interrupt to be handled on the root stage from an in-band context, using irq_pipeline_inject(). In such a case, the normal rules of interrupt delivery apply, depending on the state of the virtual interrupt disable flag for the root stage: the IRQ is immediately delivered, with the call to irq_pipeline_inject() returning only after the handler has run.\nTriggering out-of-band execution of a synthetic interrupt handler Conversely, the execution of sirq_handler() on the head stage can be triggered from the in-band context as follows:\nirq_pipeline_inject(sirq);  Since the head stage has precedence over the root stage for execution of any pending event, this IRQ is immediately delivered, with the call to irq_pipeline_inject() returning only after the handler has run.\nIt is also valid to post a synthetic interrupt to be handled on the head stage from an out-of-band context, using irq_pipeline_inject(). In such a case, the normal rules of interrupt delivery apply, depending on the state of the virtual interrupt disable flag for the head stage.\nCalling irq_stage_post_head(sirq) from the root stage to trigger an out-of-band event is most often not the right way to do this, because this service would not synchronize the interrupt log before returning. In other words, the sirq event would still be pending for the head stage despite the fact that it should have preempted the root stage before returning to the caller.\n "
},
{
	"uri": "https://dovetail.xenomai.org/status/",
	"title": "Status",
	"tags": [],
	"description": "",
	"content": "This page summarizes the status of the ongoing ports of Dovetail.\n Target kernel release\n 4.19-rc5\n ARM SoC\n  Soc (Board) IRQ pipeline 1 Alternate task control Steely2   i.MX6qp (SabreSD)      i.MX7D (SabreSD)      Cannes2-STiH410 (B2260)      Cyclone V SoC FPGA (DevKit)       ARM64 SoC\n  Soc (Board) IRQ pipeline 1 Alternate task control Steely2   virt (QEMU)      BCM2837 (Raspberry 3 Model B)      Kirin 620 (HiKey LeMaker)      \n1 Means that we pass the pipeline torture test (see CONFIG_IRQ_PIPELINE_TORTURE_TEST).\n2 Steely is an actively developed Xenomai Cobalt derivative we have been using as a workhorse (or guinea pig at times) for developing and improving Dovetail. Steely is nowhere near production software, we try fundamental changes there for going beyond Cobalt\u0026rsquo;s current limitations, which could not be merged into the mature Xenomai code base at this point. When this box is checked, the corresponding Dovetail port has been tested successfully with a Steely real-time core.\n"
},
{
	"uri": "https://dovetail.xenomai.org/pipeline/usage/pipeline_inject/",
	"title": "IRQ injection",
	"tags": [],
	"description": "",
	"content": " irq_pipeline_inject()\n irq_stage_post_root()\n irq_stage_post_head()\n irq_pipeline_send_remote()\n  TBC.\n"
},
{
	"uri": "https://dovetail.xenomai.org/pipeline/rulesofthumb/",
	"title": "Rules Of Thumb",
	"tags": [],
	"description": "",
	"content": " Turn on debug options in the kernel configuration! During the development phase, do yourself a favour: turn on CONFIG_DEBUG_IRQ_PIPELINE and CONFIG_DEBUG_DOVETAIL.\nThe first one will catch many nasty issues, such as calling unsafe in-band code from out-of-band context. The second one checks the integrity of the alternate task control mechanism, detecting issues in the architecture port.\nThe runtime overhead induced by enabling these options is marginal. Just don\u0026rsquo;t port Dovetail or implement out-of-band client code without them enabled in your target kernel, seriously.\nSerialize stages when accessing shared data If some writable data is shared between in-band and out-of-band code, you have to guard against out-of-band code preempting or racing with the in-band code which accesses the same data. This is required to prevent dirty reads and dirty writes:\n one the same CPU, by disabling interrupts in the CPU.\n from different CPUs, by using hard or mutable spinlocks.\n  Check that the pipeline torture tests pass Before any consideration is made to implement out-of-band code on a platform, check that interrupt pipelining is sane there, by enabling CONFIG_IRQ_PIPELINE_TORTURE_TEST in the configuration. As its name suggests, this option enables test code which excercizes the interrupt pipeline core, and related features such as the proxy tick device.\nKnow how to differentiate safe from unsafe in-band code Not all in-band kernel code is safe to be called from out-of-band context, actually most of it is unsafe for doing so.\nA code is deemed safe in this respect when you are 101% sure that it never does, directly or indirectly, any of the following:\n attempts to reschedule in-band wise, meaning that schedule() would end up being called. The rule of thumb is that any section of code traversing the regular might_sleep() check cannot be called from out-of-band context.\n takes a spinlock from any regular type like raw_spinlock_t or spinlock_t. The former would affect the virtual interrupt disable flag which is invalid outside of the in-band context, the latter might reschedule if CONFIG_PREEMPT is enabled.\n  In the early days of dual kernel support in Linux, some people would mistakenly invoke the do_gettimeofday() routine from an out-of-band context in order to get a wallclock timestamp for their real-time code. Doing so would create a deadlock situation if some in-band code running do_gettimeofday() is preempted by the out-of-band code re-entering the same routine on the same CPU. The out-of-band code would then wait spinning indefinitely for the in-band context to leave the routine - which won\u0026rsquo;t happen by design - leading to a lockup. Nowadays, enabling CONFIG_DEBUG_IRQ_PIPELINE would be enough to detect such mistake early enough to preserve your mental health.\n Careful with disabling interrupts in the CPU When pipelining is enabled, use hard interrupt protection with caution, especially from in-band code. Not only this might send latency figures over the top, but this might even cause random lockups would a rescheduling happen while interrupts are hard disabled.\nDealing with spinlocks Converting regular kernel spinlocks (e.g. spinlock_t, raw_spin_lock_t) to hard spinlocks should involve a careful review of every section covered by such lock. Any such section would then inherit the following requirements:\n no unsafe in-band kernel service should be called within the section.\n the section covered by the lock should be short enough to keep interrupt latency low.\n  Enable RAW_PRINTK support for printk-like debugging Unless you are lucky enough to have an ICE for debugging hard issues involving out-of-band contexts, you might have to resort to basic printk-style debugging. If so, do NOT rely on the regular printk() routine for this, this won\u0026rsquo;t work in most cases. See how to provide for a printk-like raw debug channel over a serial console instead.\n"
},
{
	"uri": "https://dovetail.xenomai.org/pipeline/porting/timer/",
	"title": "Timer management",
	"tags": [],
	"description": "",
	"content": " Proxy tick device The proxy tick device is a synthetic clock event device for handing over the control of the hardware tick device to a high-precision, out-of-band timing logic, which cannot be delayed by the in-band kernel code. With this proxy in place, any out-of-band code can gain control over the timer hardware for carrying out its own timing duties. In the same move, it is required to honor the timing requests received from the in-band timer core (i.e. hrtimers) since the latter won\u0026rsquo;t be able to program timer events directly into the hardware while the proxy is active.\nIn other words, the proxy tick device shares the functionality of the actual device between the in-band and out-of-band contexts, with only the latter actually programming the hardware.\nAdapting clock chip devices for proxying The proxy tick device borrows a real clock chip device from the in-band kernel, controlling it under the hood while substituting for the current tick device. Clock chips which may be controlled by the proxy tick device need their drivers to be specifically adapted for such use, as follows:\n clockevents_handle_event() must be substituted to any open-coded invocation of the event handler in the interrupt handler.\n struct clock_event_device::irq must be properly set to the actual IRQ number signaling an event from this device.\n struct clock_event_device::features must include CLOCK_EVT_FEAT_PIPELINE.\n __IRQF_TIMER must be set for the action handler of the timer device interrupt.\n   Adapting the ARM architected timer driver to out-of-band timing\n --- a/drivers/clocksource/arm_arch_timer.c +++ b/drivers/clocksource/arm_arch_timer.c @@ -585,7 +585,7 @@ static __always_inline irqreturn_t timer_handler(const int access, if (ctrl \u0026amp; ARCH_TIMER_CTRL_IT_STAT) { ctrl |= ARCH_TIMER_CTRL_IT_MASK; arch_timer_reg_write(access, ARCH_TIMER_REG_CTRL, ctrl, evt); -\tevt-\u0026gt;event_handler(evt); +\tclockevents_handle_event(evt); return IRQ_HANDLED; } @@ -704,7 +704,7 @@ static int arch_timer_set_next_event_phys_mem(unsigned long evt, static void __arch_timer_setup(unsigned type, struct clock_event_device *clk) { -\tclk-\u0026gt;features = CLOCK_EVT_FEAT_ONESHOT; +\tclk-\u0026gt;features = CLOCK_EVT_FEAT_ONESHOT | CLOCK_EVT_FEAT_PIPELINE; if (type == ARCH_TIMER_TYPE_CP15) { if (arch_timer_c3stop)  Only oneshot-capable clock event devices can be shared via the proxy tick device.\n  Adapting the ARM Global timer driver to out-of-band timing\n --- a/drivers/clocksource/arm_global_timer.c +++ b/drivers/clocksource/arm_global_timer.c @@ -156,11 +156,11 @@ static irqreturn_t gt_clockevent_interrupt(int irq, void *dev_id) *\tthe Global Timer flag _after_ having incremented *\tthe Comparator register\tvalue to a higher value. */ -\tif (clockevent_state_oneshot(evt)) +\tif (clockevent_is_oob(evt) || clockevent_state_oneshot(evt)) gt_compare_set(ULONG_MAX, 0); writel_relaxed(GT_INT_STATUS_EVENT_FLAG, gt_base + GT_INT_STATUS); -\tevt-\u0026gt;event_handler(evt); +\tclockevents_handle_event(evt); return IRQ_HANDLED; } @@ -171,7 +171,7 @@ static int gt_starting_cpu(unsigned int cpu) clk-\u0026gt;name = \u0026quot;arm_global_timer\u0026quot;; clk-\u0026gt;features = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT | -\tCLOCK_EVT_FEAT_PERCPU; +\tCLOCK_EVT_FEAT_PERCPU | CLOCK_EVT_FEAT_PIPELINE; clk-\u0026gt;set_state_shutdown = gt_clockevent_shutdown; clk-\u0026gt;set_state_periodic = gt_clockevent_set_periodic; clk-\u0026gt;set_state_oneshot = gt_clockevent_shutdown; @@ -195,11 +195,6 @@ static int gt_dying_cpu(unsigned int cpu) return 0; } @@ -302,8 +307,8 @@ static int __init global_timer_of_register(struct device_node *np) goto out_clk; } -\terr = request_percpu_irq(gt_ppi, gt_clockevent_interrupt, -\t\u0026quot;gt\u0026quot;, gt_evt); +\terr = __request_percpu_irq(gt_ppi, gt_clockevent_interrupt, +\tIRQF_TIMER, \u0026quot;gt\u0026quot;, gt_evt); if (err) { pr_warn(\u0026quot;global-timer: can't register interrupt %d (%d)\\n\u0026quot;, gt_ppi, err);  This is another example of adapting an existing clock chip driver for serving out-of-band timing requests, with a subtle change in the way we should test for the current state of the clock device in the interrupt handler:\n a real/original device (such as the ARM global timer in this example) is switched to detached mode when it is controlled by the proxy tick driver. Therefore, testing the original device state for clockevent_state_oneshot() always leads to false.\n since a real device controlled by the proxy for receiving out-of-band events has to be driven in one-shot mode under the hood, one should always check for clockevent_state_oob() in addition to clockevent_state_oneshot(), so that we do apply the work-around as expected.\n  Failing to fix up the way we test for the clock device state would certainly lead to an interrupt storm with any ARM global timer suffering erratum 740657, quickly locking up the board.\n Theory of operations Calling tick_install_proxy() creates an instance of the proxy tick device on each CPU mentioned in the cpumask it receives. This routine is also passed a pointer to a struct proxy_tick_ops operation descriptor (ops), defining a few handlers the caller should provide for installing and managing the proxy device.\n The proxy operation descriptor\n struct proxy_tick_ops { void (*register_device)(struct clock_event_device *proxy_ced, struct clock_event_device *real_ced); void (*unregister_device)(struct clock_event_device *proxy_ced, struct clock_event_device *real_ced); void (*handle_event)(struct clock_event_device *real_ced); };  tick_install_proxy() first invokes ops-\u0026gt;register_device() for doing the prep work for the synthetic device, allowing the client code to chose its settings before registering it, typically by a call to clockevents_config_and_register(). This device registration handler should fill the clock event device structure pointed by proxy_ced, defining the proxy device characteristics from the standpoint of the in-band kernel, just like a clock chip driver would do. real_ced is the actual clock event device being substituted for.\nConversely, ops-\u0026gt;unregister_device() is an optional handler called by tick_uninstall_proxy() for dismantling a proxy device. NULL may be given if the co-kernel has no specific action to take upon such event. In any case, tick_uninstall_proxy() ensures that the proxy is fully detached and all the related resources are freed before returning.\nAlthough this is not strictly required, it is highly expected that register_device() gives a better rating to the proxy device than the original tick device\u0026rsquo;s, so that the in-band kernel would substitute the former for the latter.\n  A register_device() handler preparing a proxy device\n static void proxy_device_register(struct clock_event_device *proxy_ced, struct clock_event_device *real_ced) { /* * We know how to deal with delays expressed as counts of * nanosecs directly for programming events (i.e. no need * to translate into cycles). */ proxy_ced-\u0026gt;features |= CLOCK_EVT_FEAT_KTIME; /* * The handler which would receive in-band timing * requests. */ proxy_ced-\u0026gt;set_next_ktime = proxy_set_next_ktime; proxy_ced-\u0026gt;set_next_event = NULL; /* * Make sure we substitute for the real device: * advertise a better rating. */ proxy_ced-\u0026gt;rating = real_ced-\u0026gt;rating + 1; proxy_ced-\u0026gt;min_delta_ns = 1; proxy_ced-\u0026gt;max_delta_ns = KTIME_MAX; proxy_ced-\u0026gt;min_delta_ticks = 1; proxy_ced-\u0026gt;max_delta_ticks = ULONG_MAX; /* All set, register now. */ clockevents_register_device(proxy_ced); }  As illustrated above, the set_next_event() or set_next_ktime() member should be set in the structure pointed by proxy_ced with the address of a handler which receives timer requests from the in-band kernel. This handler is normally implemented by the co-kernel which takes control over the timer hardware via the proxy device. Whenever the co-kernel determines that a tick is due for an outstanding request received from such handler, it should call tick_notify_proxy() to signal the event to the in-band kernel.\nWe add CLOCK_EVT_FEAT_KTIME to the proxy device flags because the co-kernel managing this device uses nanoseconds internally for expressing delays. For this reason, we want the in-band kernel to send timer requests to the co-kernel by passing delays as a count of nanoseconds to set_next_ktime() directly, without any conversion.\n Once the user-supplied ops-\u0026gt;register_device() handler returns, the following events happen in sequence:\n with a rating most likely set to be higher than the current tick device\u0026rsquo;s (i.e. the real device), the proxy device substitutes for the former.\n the real device is detached from the clockevent core. However, it is left in a functional state. The set_next_event() handler of this device is redirected to the user-supplied ops-\u0026gt;handle_event() handler. This way, every tick received from the real device would be passed on to the latter.\n the proxy device starts controlling the real device under the hood to carry out timing requests from the in-band kernel. When the hrtimer layer from the in-band kernel wants to program the next shot of the current tick device, it invokes the set_next_event() handler of the proxy device, which was defined by the caller. This handler should be scheduling in-band ticks at the requested time based on its own timer management.\n the timer interrupt triggered by the real device is switched to out-of-band handling. As a result, ops-\u0026gt;handle_event() receives tick events sent by the real device hardware directly from the head stage of the interrupt pipeline, over the out-of-band context. This ensures high-precision timing. From that point, the out-of-band code can carry out its own timing duties, in addition to honoring the in-band kernel requests for timing.\n  Step 3. involves emulating ticks scheduled by the in-band kernel by a software logic controlled by some out-of-band timer management, paced by the real ticks received as described in step 4. When this logic decides than the next in-band tick is due, it should call tick_notify_proxy() to trigger the corresponding event for the in-band kernel, which would honor the pending (hr)timer request.\n Under the hood\n clockevents_program_event() [ROOT STAGE] proxy_dev-\u0026gt;set_next_event(proxy_dev) proxy_set_next_ktime(proxy_dev) (out-of-band timing logic) real_dev-\u0026gt;set_next_event(real_dev) ... \u0026lt;hardware tick event\u0026gt; oob_timer_handler() [HEAD STAGE] clockevents_handle_event(real_dev) ops-\u0026gt;handle_event(proxy_dev) (out-of-band timing logic) tick_notify_proxy() /* schedules hrtimer tick */ ... \u0026lt;synthetic tick event\u0026gt; proxy_irq_handler(proxy_dev) [ROOT stage] clockevents_handle_event(proxy_dev) hrtimer_interrupt(proxy_dev)  "
},
{
	"uri": "https://dovetail.xenomai.org/about/",
	"title": "About this site",
	"tags": [],
	"description": "",
	"content": "This is the Dovetail project web site, part of the Xenomai project.\nThis site is built with Hugo static site generator, using the Hugo Learn Theme.\n"
},
{
	"uri": "https://dovetail.xenomai.org/pipeline/usage/stage_escalation/",
	"title": "Stage escalation",
	"tags": [],
	"description": "",
	"content": " irq_stage_escalate()  TBC.\n"
},
{
	"uri": "https://dovetail.xenomai.org/pipeline/porting/atomic/",
	"title": "Atomic operations",
	"tags": [],
	"description": "",
	"content": "The effect of virtualizing interrupt protection must be reversed for atomic helpers in asm-generic/atomic.h, asm-generic/bitops/atomic.h and asm-generic/cmpxchg-local.h, so that no interrupt can preempt their execution, regardless of the stage their caller live on.\nThis is required to keep those helpers usable on data which might be accessed from both stages.\nThe usual way to revert such virtualization consists of delimiting the protected section with hard_local_irq_save(), hard_local_irq_restore() calls, in replacement for local_irq_save(), local_irq_restore() respectively.\n Restoring strict serialization for operations on generic atomic counters\n --- a/include/asm-generic/atomic.h +++ b/include/asm-generic/atomic.h @@ -80,9 +80,9 @@ static inline void atomic_##op(int i, atomic_t *v)\t\\ {\t\\ unsigned long flags;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\ v-\u0026gt;counter = v-\u0026gt;counter c_op i;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\ } #define ATOMIC_OP_RETURN(op, c_op)\t\\ @@ -91,9 +91,9 @@ static inline int atomic_##op##_return(int i, atomic_t *v)\t\\ unsigned long flags;\t\\ int ret;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\ ret = (v-\u0026gt;counter = v-\u0026gt;counter c_op i);\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\ \\ return ret;\t\\ } @@ -104,10 +104,10 @@ static inline int atomic_fetch_##op(int i, atomic_t *v)\t\\ unsigned long flags;\t\\ int ret;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\ ret = v-\u0026gt;counter;\t\\ v-\u0026gt;counter = v-\u0026gt;counter c_op i;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\ \\ return ret;\t\\ }  Likewise, such operations may exist in architecture-specific code, overriding their generic definitions. For instance, the ARM port defines its own version of atomic operations for which real interrupt protection has to be reinstated:\n Restoring strict serialization for operations on atomic counters for ARM\n --- a/arch/arm/include/asm/atomic.h +++ b/arch/arm/include/asm/atomic.h @@ -168,9 +168,9 @@ static inline void atomic_##op(int i, atomic_t *v)\t\\ {\t\\ unsigned long flags;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\ v-\u0026gt;counter c_op i;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\ }\t\\ #define ATOMIC_OP_RETURN(op, c_op, asm_op)\t\\ @@ -179,10 +179,10 @@ static inline int atomic_##op##_return(int i, atomic_t *v)\t\\ unsigned long flags;\t\\ int val;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\ v-\u0026gt;counter c_op i;\t\\ val = v-\u0026gt;counter;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\ \\ return val;\t\\ } @@ -193,10 +193,10 @@ static inline int atomic_fetch_##op(int i, atomic_t *v)\t\\ unsigned long flags;\t\\ int val;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\ val = v-\u0026gt;counter;\t\\ v-\u0026gt;counter c_op i;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\ \\ return val;\t\\ } @@ -206,11 +206,11 @@ static inline int atomic_cmpxchg(atomic_t *v, int old, int new) int ret; unsigned long flags; -\traw_local_irq_save(flags); +\tflags = hard_local_irq_save(); ret = v-\u0026gt;counter; if (likely(ret == old)) v-\u0026gt;counter = new; -\traw_local_irq_restore(flags); +\thard_local_irq_restore(flags); return ret; }  "
},
{
	"uri": "https://dovetail.xenomai.org/pipeline/porting/rawprintk/",
	"title": "Raw printk support",
	"tags": [],
	"description": "",
	"content": " Unless you are lucky enough to have an ICE for debugging hard issues involving out-of-band contexts, you might have to resort to basic printk-style debugging over a serial line. If so, do NOT rely on the regular printk() routine for this, this won\u0026rsquo;t work in most cases, potentially adding even more sources of lockups to the whole gloomy picture. Although the printk() machinery can be used from out-of-band context, the output is deferred until the root stage gets back in control, which means that:\n you can\u0026rsquo;t reliably trace out-of-band code on the spot, deferred output issued from an out-of-band context, or from a section of code running with interrupts disabled in the CPU may appear after subsequent in-band messages under some circumstances, due to a buffering effect.\n if the debug traces are sent at high pace (e.g. from an out-of-band IRQ handler every few hundreds of microseconds), the machine is likely to come to a stall due to the massive output the heavy printk() machinery would have to handle, leading to an apparent lockup.\n  The only sane option for printk-like debugging in demanding out-of-band context is using the raw_printk() routine for issuing raw debug messages to a serial console, so that you may get some sensible feedback for understanding what is going on with the execution flow. This feature should be enabled by turning on CONFIG_RAW_PRINTK, otherwise all output sent to raw_printk() is discarded.\nBecause a regular (in-band) serial console driver won\u0026rsquo;t be usable from out-of-band context, enabling raw printk support requires adapting the serial console driver your platform is using, by adding a raw write handler to the console description. Just like the regular write() handler, the raw output handler receives a console pointer, the character string to output and its length as parameters. This handler should send the characters to the UART as quickly as possible, with little to no preparation.\nAll output formatted by the generic raw_printk() routine is passed to the raw write handler of the current serial console driver if present. Calls to the raw output handler are serialized in raw_printk() by holding a hard spinlock, which means that interrupts are disabled in the CPU when running the handler.\nA raw write handler is normally derived from the regular write handler for the same serial console device, skipping any in-band locking construct, only waiting for the bare minimum time for the output to drain in the UART since we want to keep interrupt latency low.\n  Adding RAW_PRINTK support to the AMBA PL011 serial driver\n --- a/drivers/tty/serial/amba-pl011.c +++ b/drivers/tty/serial/amba-pl011.c @@ -2206,6 +2206,40 @@ static void pl011_console_putchar(struct uart_port *port, int ch) pl011_write(ch, uap, REG_DR); } +#ifdef CONFIG_RAW_PRINTK + +/* + * The uart clk stays on all along in the current implementation, + * despite what pl011_console_write() suggests, so for the time being, + * just emit the characters assuming the chip is clocked. If the clock + * ends up being turned off after writing, we may need to clk_enable() + * it at console setup, relying on the non-zero enable_count for + * keeping pl011_console_write() from disabling it. + */ +static void +pl011_console_write_raw(struct console *co, const char *s, unsigned int count) +{ +\tstruct uart_amba_port *uap = amba_ports[co-\u0026gt;index]; +\tunsigned int old_cr, new_cr, status; + +\told_cr = readw(uap-\u0026gt;port.membase + UART011_CR); +\tnew_cr = old_cr \u0026amp; ~UART011_CR_CTSEN; +\tnew_cr |= UART01x_CR_UARTEN | UART011_CR_TXE; +\twritew(new_cr, uap-\u0026gt;port.membase + UART011_CR); + +\twhile (count-- \u0026gt; 0) { +\tif (*s == '\\n') +\tpl011_console_putchar(\u0026amp;uap-\u0026gt;port, '\\r'); +\tpl011_console_putchar(\u0026amp;uap-\u0026gt;port, *s++); +\t} +\tdo +\tstatus = readw(uap-\u0026gt;port.membase + UART01x_FR); +\twhile (status \u0026amp; UART01x_FR_BUSY); +\twritew(old_cr, uap-\u0026gt;port.membase + UART011_CR); +} + +#endif /* !CONFIG_RAW_PRINTK */ + static void pl011_console_write(struct console *co, const char *s, unsigned int count) { @@ -2406,6 +2440,9 @@ static struct console amba_console = { .device\t= uart_console_device, .setup\t= pl011_console_setup, .match\t= pl011_console_match, +#ifdef CONFIG_RAW_PRINTK +\t.write_raw\t= pl011_console_write_raw, +#endif .flags\t= CON_PRINTBUFFER | CON_ANYTIME, .index\t= -1, .data\t= \u0026amp;amba_reg,  ARM-specific raw console driver The vanilla ARM kernel port already provides an UART-based raw output routine called printascii() when CONFIG_DEBUG_LL is enabled, provided the right debug UART channel is defined too (CONFIG_DEBUG_UART_xx).\nWhen CONFIG_RAW_PRINTK and CONFIG_DEBUG_LL are both defined in the kernel configuration, the ARM implementation of Dovetail automatically registers a special console device for emitting debug output (see arch/arm/kernel/raw_printk.c), which redirects calls to its raw write handler by raw_printk() to printascii(). In other words, if CONFIG_DEBUG_LL already provides you with a functional debug output channel, you don\u0026rsquo;t need the active serial console driver to implement a raw write handler for enabling raw_printk(), the raw console device should handle raw_printk() requests just fine.\nEnabling CONFIG_DEBUG_LL with a wrong UART debug channel is a common cause of lockup at boot. You do want to make sure that the proper CONFIG_DEBUG_UART_xx symbol matching your hardware is selected along with CONFIG_DEBUG_LL.\n "
},
{
	"uri": "https://dovetail.xenomai.org/pipeline/usage/interrupt_protection/",
	"title": "Interrupt Protection",
	"tags": [],
	"description": "",
	"content": " Disabling interrupts in the CPU The local_irq_save() and local_irq_disable() helpers are no more disabling interrupts in the CPU when interrupt pipelining is enabled, but only disable interrupt events virtually for the root stage.\nA set of helpers is provided for manipulating the interrupt disable flag in the CPU instead. When CONFIG_IRQ_PIPELINE is disabled, this set maps 1:1 over the regular local_irq_*() API.\n   Original/Virtual Non-virtualized call     local_save_flags(flags) flags = hard_local_save_flags()   local_irq_disable() hard_local_irq_disable()   local_irq_enable() hard_local_irq_enable()   local_irq_save(flags) flags = hard_local_irq_save()   local_irq_restore(flags) hard_local_irq_restore(flags)   irqs_disabled() hard_irqs_disabled()   irqs_disabled_flags(flags) hard_irqs_disabled_flags(flags)    Stalling the head stage Just like the root stage is affected by the state of the virtual interrupt disable flag, the interrupt state of the head stage is controlled by a dedicated stall bit flag in the head stage\u0026rsquo;s status. In combination with the interrupt disable bit in the CPU, this software bit controls interrupt delivery to the head stage.\nWhen this stall bit is set, interrupts which might be pending in the head stage\u0026rsquo;s event log of the current CPU are not played. Conversely, the out-of-band handlers attached to pending IRQs are fired when the stall bit is clear. The following table represents the equivalent calls affecting the stall bit for each stage:\n   Root stage operation Head stage operation     local_save_flags(flags) -none-   local_irq_disable() head_irq_disable()   local_irq_enable() head_irq_enable()   local_irq_save(flags) flags = head_irq_save()   local_irq_restore(flags) head_irq_restore(flags)   irqs_disabled() head_irqs_disabled()   irqs_disabled_flags(flags) -none-    Using this set of helpers only makes sense from out-of-band code, typically in the real-time core implementation. Calling them from any other context would be highly suspicious. Adapting in-band code to interrupt pipelining might involve using the hard_local_*() set instead, but only in marginal cases, always with extreme care.\n "
},
{
	"uri": "https://dovetail.xenomai.org/pipeline/porting/misc/",
	"title": "Misc",
	"tags": [],
	"description": "",
	"content": " printk() support printk() may be called by out-of-band code safely, without encurring extra latency. The output is conveyed like NMI-originated output, which involves some delay until the in-band code resumes, and the console driver(s) can handle it.\nTracing Tracepoints can be traversed by out-of-band code safely. Dynamic tracing is available to a kernel running the pipelined interrupt model too.\n"
},
{
	"uri": "https://dovetail.xenomai.org/pipeline/usage/locking/",
	"title": "Locking",
	"tags": [],
	"description": "",
	"content": " Additional spinlock types The pipeline core introduces two spinlock types:\n hard spinlocks manipulate the CPU interrupt mask, and don\u0026rsquo;t affect the kernel preemption state in locking/unlocking operations.  This type of spinlock is useful for implementing a critical section to serialize concurrent accesses from both in-band and out-of-band contexts, i.e. from root and head stages. Obviously, sleeping into a critical section protected by a hard spinlock would be a very bad idea. In other words, hard spinlocks are not subject to virtual interrupt masking, therefore can be used to serialize with out-of-band activities, including from the in-band kernel code. At any rate, those sections ought to be quite short, for keeping latency low.\n mutable spinlocks are used internally by the pipeline core to protect access to IRQ descriptors (struct irq_desc::lock), so that we can keep the original locking scheme of the generic IRQ core unmodified for handling out-of-band interrupts.  Mutable spinlocks behave like hard spinlocks when traversed by the low-level IRQ handling code on entry to the pipeline, or common raw spinlocks otherwise, preserving the kernel (virtualized) interrupt and preemption states as perceived by the in-band context. This type of lock is not meant to be used in any other situation.\nLockdep support The lock validator automatically reconciles the real and virtual interrupt states, so it can deliver proper diagnosis for locking constructs defined in both in-band and out-of-band contexts. This means that hard and mutable spinlocks are included in the validation set when LOCKDEP is enabled.\nThese two additional types are subject to LOCKDEP analysis. However, be aware that latency figures are likely to be really bad when LOCKDEP is enabled, due to the large amount of work the lock validator may have to do with interrupts disabled for the CPU (i.e. hard locking) for enforcing critical sections.\n "
},
{
	"uri": "https://dovetail.xenomai.org/pipeline/porting/devnotes/",
	"title": "Developer&#39;s Notes",
	"tags": [],
	"description": "",
	"content": " Generic Fundamentally preemption-safe contexts Over a few contexts, we may traverse code using unprotected, preemption-sensitive accessors such as percpu() without disabling preemption specifically, because either one condition is true;\n if preempt_count() bears either of the PIPELINE_MASK or STAGE_MASK bits, which turns preemption off, therefore CPU migration cannot happen (debug_smp_processor_id() and preempt checks in percpu accessors would detect such context properly too).\n if we are running over the context of the root stage\u0026rsquo;s event log syncer (irq_stage_sync_current()) playing a deferred interrupt, in which case the virtual interrupt disable bit is set, so no CPU migration may occur either.\n  For instance, the following contexts qualify:\n clockevents_handle_event(), which should either be called from the head stage - therefore STAGE_MASK is set - when the proxy tick device is active on the CPU, and/or from the root stage playing a timer interrupt event from the corresponding device.\n any IRQ flow handler from kernel/irq/chip.c. When called from generic_pipeline_irq() for pushing an external event to the pipeline, on_pipeline_entry() is true, which indicates that PIPELINE_MASK is set. When called for playing a deferred interrupt on the root stage, the virtual interrupt disable bit is set.\n  Checking for out-of-band interrupt property The IRQF_OOB action flag should NOT be used for testing whether an interrupt is out-of-band, because out-of-band handling may be turned on/off dynamically on an IRQ descriptor using irq_switch_oob(), which would not translate to IRQF_OOB being set/cleared for the attached action handlers.\nirq_is_oob() is the right way to check for out-of-band handling.\nstop_machine() hard disables interrupts The regular stop_machine() services guarantees that all online CPUs are spinning non-preemptible in a known code location before a subset of them may safely run a stop-context function. This service is typically useful for live patching the kernel code, or changing global memory mappings, so that no activity could run in parallel until the system has returned to a stable state after all stop-context operations have completed.\nWhen interrupt pipelining is enabled, Dovetail provides the same guarantee by restoring hard interrupt disabling where virtualizing the interrupt disable flag would defeat it.\nAs those lines are written, all stop_machine() use cases must also exclude any head stage activity (e.g. ftrace live patching the kernel code for installing tracepoints), or happen before any such activity can ever take place (e.g. KPTI boot mappings). Dovetail makes a basic assumption that stop_machine() could not get in the way of latency-sensitive processes, simply because the latter could not keep running safely until a call to the former has completed anyway.\nHowever, one should keep an eye on stop_machine() usage upstream, identifying new callers which might cause unwanted latency spots under specific circumstances (maybe even abusing the interface).\nVirtual interrupt disable state breakage When some WARN_ON() triggers due to a wrong interrupt disable state (e.g. entering the softirqs/bh code with IRQs unexpectedly [virtually] disabled), this may be due to the CPU and virtual interrupt states being out-of-sync when traversing the epilogue code after a syscall, IRQ or trap has been handled during the latest kernel entry.\nTypically, do_work_pending() or do_notify_resume() should make sure to reconcile both states in the work loop, and also to restore the virtual state they received on entry before returning to their caller.\nThe routines just mentioned always enter from their assembly call site with interrupts hard disabled in the CPU. However, they may be entered with the virtual interrupt state enabled or disabled, depending on the kind of event which led to them eventually. Typically, a system call epilogue would always enter with the virtual state enabled, but a fault might also occur when the virtual state is disabled though. The epilogue routine called for finalizing some IRQ handling must enter with the virtual state enabled, since the latter is a pre-requisite for running such code.\n Losing the timer tick The symptom of a common issue in a Dovetail port is losing the timer interrupt when the out-of-band (co-)kernel takes control over the tick device, causing the in-band kernel to stall. After some time spent hanging, the in-band kernel may eventually complain about a RCU stall situation with a message like _INFO: rcupreempt detected stalls on CPUs/tasks followed by stack dump(s). In other cases, the machine may simply lock up due to an interrupt storm.\nThis is typical of timer interrupt events not flowing down normally to the in-band kernel anymore because something went wrong as soon as the proxy tick device replaced the regular device for serving in-band timing requests. When this happens, we should check the following code spots for bugs:\n the timer acknowledge code is wrong once called from the head stage, which is going to be the case as soon as a co-kernel installs the proxy tick device for interposing on the timer. Being wrong here means performing actions which are not legit from such a context.\n the irqchip driver managing the interrupt event for the timer tick is wrong somehow, causing such interrupt to stay masked or stuck for some reason whenever it is switched to out-of-band mode. You need to double-check the implementation of the chip handlers, considering the effects and requirements of interrupt pipelining.\n power management (CONFIG_CPUIDLE) gets in the way, often due to the infamous C3STOP misfeature turning off the original timer hardware controlled by the proxy device. A detailed explanation is given in Documentation/irq_pipeline.rst when discussing the few changes to the scheduler core for supporting the Dovetail interface. If this is acceptable from a power saving perspective, having the co-kernel prevent the in-band kernel from entering a deeper C-state is enough to fix the issue, by overriding the irq_cpuidle_control() routine as follows:\n  bool irq_cpuidle_control(struct cpuidle_device *dev, struct cpuidle_state *state) { /* * Deny entering sleep state if this entails stopping the * timer (i.e. C3STOP misfeature). */ if (state \u0026amp;\u0026amp; (state-\u0026gt;flags \u0026amp; CPUIDLE_FLAG_TIMER_STOP)) return false; return true; }  Printk-debugging such timer issue requires enabling raw printk() support, you won\u0026rsquo;t get away with tracing the kernel behavior using the plain printk() routine for this, because most of the output would remain stuck into a buffer, never reaching the console driver before the board hangs eventually.\n ARM Context assumption with outer L2 cache There is no reason for the outer cache to be invalidated/flushed/cleaned from an out-of-band context, all cache maintenance operations must happen from in-band code. Therefore, we neither need nor want to convert the spinlock serializing access to the cache maintenance operations for L2 to a hard lock.\nConversion to hard lock may have cause latency to skyrocket on some i.MX6 hardware, equipped with PL22x cache units, or PL31x with errata 588369 or 727915 for particular hardware revisions, as each background operation was awaited for completion for working around some silicon bug, with hard irqs disabled.\n "
},
{
	"uri": "https://dovetail.xenomai.org/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Introducing Dovetail Using Linux as a host for lightweight software cores specialized in delivering very short and bounded response times has been a popular way of supporting real-time applications in the embedded space over the years.\nThis design - known as the dual kernel approach - introduces a small real-time infrastructure which schedules time-critical activities independently from the main kernel. Application threads co-managed by this infrastructure still benefit from the ancillary kernel services such as virtual memory management, and can also leverage the rich GPOS feature set Linux provides such as networking, data storage or GUIs.\nAlthough the real-time infrastructure has to present specific driver stack and API implementations to applications, there are nonetheless significant upsides to keeping the real-time core separate from the GPOS infrastructure:\n because the two kernels are independent, real-time activities are not serialized with GPOS operations internally, removing potential delays which might be induced by the non time-critical work. Likewise, there is no requirement for keeping the GPOS operations fine-grained and highly preemptible at any time, which would otherwise induce noticeable overhead on low-end hardware, due to the requirement for pervasive task priority inheritance and IRQ threading.\n the functional isolation of the real-time infrastructure from the rest of the kernel code restricts common bug hunting to the scope of the smaller kernel, excluding most interactions with the very large GPOS kernel base.\n with a dedicated infrastructure providing a specific, well-defined set of real-time services, applications can unambiguously figure out which API calls are available for supporting time-critical work, excluding all the rest as being potentially non-deterministic with respect to response time.\n  This documentation presents Dovetail, an effort to integrate the support for interfacing Linux with a real-time component deeply inside the host kernel\u0026rsquo;s logic.\nThe two software layers forming Dovetail are described: firstly the interrupt pipeline which creates a high-priority execution stage for a real-time infrastructure, and finally the support for alternate task control between the Linux kernel and the real-time component over the kthreads and user tasks.\nAlthough both layers are likely to be needed for implementing a real-time component, only the interrupt pipeline has to be enabled in the early stage of porting Dovetail. Support for alternate task control builds upon the latter, and may - and should - be postponed until the pipeline is fully functional on the target architecture or platform. The code base is specifically maintained in a way which allows such incremental process.\nDovetail only introduces the basic mechanisms for hosting a real-time core into the Linux kernel, enabling the common programming model for its applications in user-space. It does not implement the real-time core per se, which should be provided by a separate kernel component.\n Why do we need this? Dovetail is the successor to the I-pipe, the interrupt pipeline implementation Xenomai\u0026rsquo;s Cobalt real-time core currently relies on. The rationale behind this effort is about securing the maintenance of this key component of Xenomai so that it could be maintained with common kernel development knowledge, at a fraction of the engineering and maintenance cost native preemption requires. For several reasons, the I-pipe does not qualify.\nMaintaining the I-pipe proved to be difficult over the years as changes to the mainline kernel regularly caused non-trivial code conflicts, sometimes nasty regressions to us downstream. Although the concept of interrupt pipelining proved to be correct in delivering short response time with reasonably limited changes to the original kernel, the way this mechanism is currently integrated into the mainline code shows its age, as explained in this document.\nAudience The intended audience of this document is people having common kernel development knowledge, who may be interested in building a real-time component on Dovetail, porting it to their architecture or platform of choice. Knowing about the basics of interrupt flow, IRQ chip and clock event device drivers in the kernel tree would be a requirement for porting this code.\nHowever, this document is not suited for getting one\u0026rsquo;s feet wet with kernel development. Many assumptions about pre-existing knowledge of the reader regarding kernel fundamentals are made.\n"
},
{
	"uri": "https://dovetail.xenomai.org/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://dovetail.xenomai.org/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]