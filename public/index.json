[
{
	"uri": "https://evenless.org/core/kernel-api/file/",
	"title": "File description",
	"tags": [],
	"description": "",
	"content": " int evl_open_file(struct evl_file *efilp, struct file *filp) void evl_release_file(struct evl_file *efilp) void evl_get_fileref(struct evl_file *efilp) struct evl_file *evl_get_file(unsigned int fd) void evl_put_file(struct evl_file *efilp) "
},
{
	"uri": "https://evenless.org/dovetail/pipeline/optimistic/",
	"title": "Optimistic Interrupt Protection",
	"tags": [],
	"description": "",
	"content": " Predictable response time of out-of-band handlers to IRQ receipts requires the in-band kernel work not to be allowed to delay them by masking interrupts in the CPU.\nHowever, critical sections delimited this way by the in-band code must still be enforced for the in-band stage, so that system integrity is not at risk. This means that although out-of-band IRQ handlers may run at any time while the oob stage is accepting interrupts, in-band IRQ handlers should be allowed to run only when the in-band stage is accepting interrupts too. So we need to decouple the interrupt masking and delivery logic which applies to the oob stage from the one in effect on the in-band stage, by implementing a dual interrupt control.\nVirtual interrupt disabling To this end, a software logic managing a virtual interrupt disable flag is introduced by the interrupt pipeline between the hardware and the generic IRQ management layer. This logic can mask IRQs from the perspective of the in-band kernel work when local_irq_save(), local_irq_disable() or any lock-controlled masking operations like spin_lock_irqsave() is called, while still accepting IRQs from the CPU for immediate delivery to out-of-band handlers.\nWhen a real IRQ arrives while interrupts are virtually masked, the event is logged for the receiving CPU, kept there until the virtual interrupt disable flag is cleared at which point it is dispatched as if it just happened. The principle of deferring interrupt delivery based on a software flag coupled to an event log has been originally described as Optimistic interrupt protection in this paper. It was originally intended as a low-overhead technique for manipulating the processor interrupt state, reducing the cost of interrupt masking for the common case of absence of interrupts.\nIn Dovetail\u0026rsquo;s two-stage pipeline, the oob stage protects from interrupts by disabling them in the CPU\u0026rsquo;s status register as usual, while the in-band stage disables interrupts only virtually. A stage for which interrupts are disabled is said to be stalled. Conversely, unstalling a stage means re-enabling interrupts for it.\nObviously, stalling the oob stage implicitly means disabling further IRQ receipts for the in-band stage down the pipeline too.\n Interrupt deferral for the in-band stage When the in-band stage is stalled because the virtual interrupt disable flag is set, any IRQ event which was not immediately delivered to the oob stage is recorded into a per-CPU log, postponing delivery to the in-band kernel handler.\nSuch delivery is deferred until the in-band kernel code clears the virtual interrupt disable flag by calling local_irq_enable() or any of its variants, which unstalls the in-band stage. When this happens, the interrupt state is resynchronized by playing the log, firing the in-band handlers for which an IRQ event is pending.\n/* Both stages unstalled on entry */ local_irq_save(flags); \u0026lt;IRQx received: no out-of-band handler\u0026gt; (pipeline logs IRQx event) ... local_irq_restore(flags); (pipeline plays IRQx event) handle_IRQx_interrupt();  If the in-band stage is unstalled at the time of the IRQ receipt, the in-band handler is immediately invoked, just like with the non-pipelined IRQ model.\nAll interrupts are (seemingly) NMIs From the standpoint of the in-band kernel code (i.e. the one running over the in-band interrupt stage) , the interrupt pipelining logic virtually turns all device IRQs into NMIs, for running out-of-band handlers.\nFor this reason, out-of-band code may generally NOT re-enter in-band code, for preventing creepy situations like this one:\n/* in-band context */ spin_lock_irqsave(\u0026amp;lock, flags); \u0026lt;IRQx received: out-of-band handler installed\u0026gt; handle_oob_event(); /* attempted re-entry to in-band from out-of-band. */ in_band_routine(); spin_lock_irqsave(\u0026amp;lock, flags); \u0026lt;DEADLOCK\u0026gt; ... ... ... ... spin_unlock irqrestore(\u0026amp;lock, flags);  Even in absence of an attempt to get a spinlock recursively, the outer in-band code in the example above is entitled to assume that no access race can occur on the current CPU while interrupts are masked. Re-entering in-band code from an out-of-band handler would invalidate this assumption.\nIn rare cases, we may need to fix up the in-band kernel routines in order to allow out-of-band handlers to call them. Typically, atomic helpers are such routines, which serialize in-band and out-of-band callers.\nFor all other cases, the IRQ work API is available for scheduling the execution of a routine from the oob stage, which will be invoked later from the in-band stage as soon as it gets back in control on the current CPU.\n"
},
{
	"uri": "https://evenless.org/dovetail/pipeline/porting/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": " Generic requirements The interrupt pipeline requires the following features to be available from the target Linux kernel:\n Generic IRQ handling (CONFIG_GENERIC_IRQ), which most architectures should support these days.\n IRQ domains (CONFIG_IRQ_DOMAIN).\n Generic clock event abstraction (CONFIG_GENERIC_CLOCKEVENTS).\n  Other assumptions ARM  a target ARM machine port must be allowed to specify its own IRQ handler at run time (CONFIG_MULTI_IRQ_HANDLER).\n only armv6 CPUs and later are supported, excluding older generations of ARM CPUs. Support for ASID (CONFIG_CPU_HAS_ASID) is required.\n machine does not have VIVT cache.\n  armv5 is not supported due to the use of VIVT caches on these CPUs, which don\u0026rsquo;t cope well - at all - with low latency requirements. A work aimed at leveraging the legacy FCSE PID register for reducing the cost of cache invalidation in context switches has been maintained until 2013 by Gilles Chanteperdrix, as part of the legacy I-pipe project, Dovetail\u0026rsquo;s ancestor. This work can still be cloned from this GIT repository.\n "
},
{
	"uri": "https://evenless.org/dovetail/pipeline/porting/",
	"title": "Porting",
	"tags": [],
	"description": "",
	"content": "How to port this thing.\n"
},
{
	"uri": "https://evenless.org/dovetail/",
	"title": "Dovetail interface",
	"tags": [],
	"description": "",
	"content": " Introducing Dovetail Using Linux as a host for lightweight software cores specialized in delivering very short and bounded response times has been a popular way of supporting real-time applications in the embedded space over the years.\nThis dual kernel design introduces a small real-time infrastructure into the Linux code, which immediately handles time-critical, out-of-band activities independently from the ongoing main kernel work. Application threads co-managed by this infrastructure still benefit from the common kernel services such as virtual memory management; they can leverage the rich GPOS feature set Linux provides such as networking, data storage or GUIs too.\nThere are significant upsides to keeping the real-time core separate from the GPOS infrastructure:\n because the two kernels are independent, real-time activities are not serialized with GPOS operations internally, removing potential delays which might be induced by the non time-critical work. Likewise, there is no requirement for keeping the GPOS operations fine-grained and highly preemptible at any time, which would otherwise induce noticeable overhead on low-end hardware, due to the need for pervasive task priority inheritance and IRQ threading.\n when debugging a real-time issue, the functional isolation of the real-time infrastructure from the rest of the kernel code restricts bug hunting to the scope of the small autonomous core, excluding most interactions with the very large GPOS kernel base.\n with a dedicated infrastructure providing a specific, well-defined set of real-time services, applications can unambiguously figure out which API calls are available for supporting time-critical work, excluding all the rest as being potentially non-deterministic with respect to response time.\n  This documentation presents Dovetail, an interface for integrating any short of autonomous software core into the kernel, such as a real-time core. The two main software layers forming Dovetail are described:\n first the interrupt pipeline which creates a high-priority execution stage for an autonomous software core to run on.\n then, support for alternate scheduling between the main kernel and the autonomous software core, for sharing kthreads and user tasks.\n  Although both layers are likely to be needed for implementing some autonomous core, only the interrupt pipeline has to be enabled in the early stage of porting Dovetail. Support for alternate scheduling builds upon the latter, and may - and should - be postponed until the pipeline is fully functional on the target architecture or platform. The code base is specifically maintained in a way which allows such incremental process.\nDovetail only introduces the basic mechanisms for hosting an autonomous core into the kernel, enabling the common programming model for its applications in user-space. It does not implement the software core per se, which should be provided by a separate kernel component instead, such as the EVL core.\n Why do we need this? Dovetail is the successor to the I-pipe, the interrupt pipeline implementation Xenomai\u0026rsquo;s Cobalt real-time core currently relies on. The rationale behind this effort is about securing the maintenance of this key component of dual kernel systems such as Xenomai, so that they could be maintained with common kernel development knowledge, at a fraction of the engineering and maintenance cost native preemption requires. For several reasons, the I-pipe does not qualify.\nMaintaining the I-pipe proved to be difficult over the years as changes to the mainline kernel regularly caused non-trivial code conflicts, sometimes nasty regressions to us downstream. Although the concept of interrupt pipelining proved to be correct in delivering short response time with reasonably limited changes to the original kernel, the way this mechanism is currently integrated into the mainline code shows its age, as explained in this document.\nDeveloping Dovetail Working on Dovetail is customary Linux kernel development, following the common set of rules and guidelines which prevails with the mainline kernel.\nThe development tip of the Dovetail interface is maintained in the dovetail/master branch of the following GIT repository which tracks the mainline kernel:\n git://git.evenless.org/linux-evl.git https://git.evenless.org/linux-evl.git  Audience This document is intended to people having common kernel development knowledge, who may be interested in building an autonomous software core on Dovetail, porting it to their architecture or platform of choice. Knowing about the basics of interrupt flow, IRQ chip and clock event device drivers in the kernel tree would be a requirement for porting this code.\nHowever, this document is not suited for getting one\u0026rsquo;s feet wet with kernel development, as it assumes that the reader is already familiar with kernel fundamentals.\n"
},
{
	"uri": "https://evenless.org/core/user-api/thread/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Thread element The main kernel\u0026rsquo;s thread is the basic execution unit in EVL. The most common kind of EVL threads is a regular POSIX thread started by pthread_create(3) which has attached itself to the EVL core by a call to evl_attach_self(). Once a POSIX thread attached itself to EVL, it can:\n request real-time services to the core, exclusively by calling routines available from the EVL library . In this case, and only in this one, you get real-time guarantees for the caller. This is what time-critical processing loops are supposed to use. Such request may switch the calling thread to the out-of-band execution stage, for running under EVL\u0026rsquo;s supervision in order to ensure real-time behaviour.\n invoke services from your favourite C library (glibc, musl, uClibc etc.), which may end up issuing system calls to the main kernel for carrying out the job. EVL may have to demote the caller automatically from the EVL context to a runtime mode which is compatible with using the main kernel services. As a result of this, you get NO help from EVL for keeping short and bounded latency figures anymore, but you do have access to any feature the main kernel provides. This mode is normally reserved to initialization and cleanup phases of your application. If you end up using them in the middle of a would-be time-critical loop, well, something is seriously wrong in this code.\n  A thread which is being scheduled by EVL instead of the main kernel is said to be running out-of-band, as defined by Dovetail. It remains in this mode until it asks for a service which the main kernel provides. Conversely, a thread which is being scheduled by the main kernel instead of EVL is said to be running in-band, as defined by Dovetail. It remains in this mode until it asks for a service which EVL can only provide to the caller when it runs out-of-band.\n Thread services  int evl_attach_self(const char *fmt, ...)  EVL does not actually create threads; instead, it enables a regular POSIX thread to invoke its real-time services once this thread has attached to the EVL core. evl_attach_self() is the library call which requests such attachment. All you need to provide is a unique thread name, which will be the name of the device element representing that thread in the file system.\nThere is no requirement as of when evl_attach_self() should be called in a thread\u0026rsquo;s execution flow. You just have to call it before it starts requesting EVL services. Note that the main thread of a process is no different from any other thread to EVL. It may call evl_attach_self() whenever you see fit, or not at all if you don\u0026rsquo;t plan to request EVL services from this context.\nfmtA printf-like format string to generate the thread name. A common way of generating unique thread names is to add the calling process\u0026rsquo;s pid somewhere into the format string as illustrated in the example. The generated name is used to form a file path, referring to the new thread element\u0026rsquo;s device in the file system. So this name must contain only valid characters in this context, excluding slashes.\n\n...The optional variable argument list completing the format.\n\nevl_attach_self() returns the file descriptor of the newly attached thread on success. You may use this fd to submit requests for the newly attached thread in other calls from the EVL library which ask for a thread file descriptor. If the call fails, a negated error code is returned instead:\n -EEXIST The generated name is conflicting with an existing thread\u0026rsquo;s name.\n -EINVAL The generated name is badly formed, likely containing invalid character(s), such as a slash. Keep in mind that it should be usable as a basename of a device element\u0026rsquo;s file path.\n -ENAMETOOLONG The overall length of the device element\u0026rsquo;s file path including the generated name exceeds PATH_MAX.\n -EMFILE The per-process limit on the number of open file descriptors has been reached.\n -ENFILE The system-wide limit on the total number of open files has been reached.\n -EPERM The caller is not allowed to lock memory via a call to mlockall(2). Since memory locking is a requirement for running EVL threads, no joy.\n -ENOMEM No memory available, whether the kernel could not lock down all of the calling process\u0026rsquo;s virtual address space into RAM, or some other reason related to some process or driver eating way too much virtual or physical memory. You may start panicking.\n -ENOSYS The EVL core is not enabled in the running kernel.\n  #include \u0026lt;evl/thread.h\u0026gt; static void *byte_crunching_thread(void *arg) { int efd; /* Attach the current thread to the EVL core. */ efd = evl_attach_self(\u0026quot;cruncher-%d\u0026quot;, getpid()); ... }  As a result of this call, you should see a new device appear into the /dev/evl/thread hierarchy, e.g.:\n$ ls -l /dev/evl/thread total 0 crw-rw---- 1 root root 246, 1 Jan 1 1970 clone crw-rw---- 1 root root 246, 1 Jan 1 1970 cruncher-2712   You can revert the attachment to EVL at any time by calling evl_detach_self() from the context of the thread to detach.\n Closing all the file descriptors referring to an EVL thread is not enough to drop its attachment to the EVL core. It merely prevents to submit any further request for the original thread via calls taking file descriptors. You would still have to call evl_detach_self() from the context of this thread to fully detach it.\n If a valid file descriptor is still referring to a detached thread, or after the thread has exited, any request submitted for that thread using such fd would receive -ESTALE.\n An EVL thread which exits is automatically detached from the EVL core, you don\u0026rsquo;t have to call evl_detach_self() explicitly before exiting your thread.\n The EVL core drops the kernel resources attached to a thread once it has detached itself or has exited, and only after all the file descriptors referring to that thread have been closed.\n The EVL library sets the O_CLOEXEC flag on the file descriptor referring to the newly attached thread before returning from evl_attach_self().\n   int evl_detach_self(void)  evl_detach_self() reverts the action of evl_attach_self(), detaching the calling thread from the EVL core. Once this operation has succeeded, the current thread cannot submit EVL requests anymore. This call returns zero on success, or a negated error code if something went wrong:\n-EPERM The current thread is not attached to the EVL core.\n#include \u0026lt;evl/thread.h\u0026gt; static void *byte_crunching_thread(void *arg) { int efd; /* Attach the current thread to the EVL core. */ efd = evl_attach_self(\u0026quot;cruncher-%d\u0026quot;, getpid()); ... /* Then detach it. */ evl_detach_self(); ... }   You can re-attach the detached thread to EVL at any time by calling evl_attach_self() again.\n If a valid file descriptor is still referring to a detached thread, or after the thread has exited, any request submitted for that thread using such descriptor would receive -ESTALE.\n An EVL thread which exits is automatically detached from the EVL core, you don\u0026rsquo;t have to call evl_detach_self() explicitly before exiting your thread.\n The EVL core drops the kernel resources attached to a thread once it has detached itself or has exited, and only after all the file descriptors referring to that thread have been closed.\n   int evl_get_self(void)  evl_get_self() returns the file descriptor obtained for the current thread after a successful call to evl_attach_self(). You may use this fd to submit requests for the current thread in other calls from the EVL library which ask for a thread file descriptor. This call returns a valid file descriptor referring to the caller on success, or a negated error code if something went wrong:\n-EPERM The current thread is not attached to the EVL core.\n#include \u0026lt;evl/thread.h\u0026gt; static void get_caller_info(void) { struct evl_thread_state statebuf; int efd, ret; /* Fetch the current thread's _fd_. */ efd = evl_get_self(); ... /* Retrieve the caller's state information. */ ret = evl_get_state(efd, \u0026amp;statebuf); ... }  evl_get_self() will fail after a call to evl_detach_self().\n int evl_switch_oob(void)  Applications are unlikely to ever use this call explicitly: it switches the calling thread to the out-of-band execution stage, for running under EVL\u0026rsquo;s supervision which ensures real-time behaviour. Any EVL service which requires it will enforce such switch if and when required automatically, so in most cases there should be no point in dealing with this manually in applications.\nevl_switch_oob() is defined for the rare circumstances where some high-level API based on the EVL core library might have to enforce a particular execution stage, based on a deep knowledge of how EVL works internally. Entering a syscall-free section of code for which the out-of-band mode needs to be guaranteed on entry would be the only valid reason to call evl_switch_oob(). This call returns zero on success, or a negated error code if something went wrong:\n-EPERM The current thread is not attached to the EVL core.\nForcing the current execution stage between in-band and out-of-band stages is a heavyweight operation: this entails two thread context switches both ways, as the switching thread is offloaded to the opposite scheduler. You really don\u0026rsquo;t want to force this explicitly unless you definitely have to and fully understand the implications of it runtime-wise. Bottom line is that calling a main kernel service from within a time-critical code is a clear indication that something is wrong in such code. This invalidates the reason why a time-critical code would need to switch back to out-of-band mode eagerly.\n  int evl_switch_inband(void)  Applications are unlikely to ever use this call explicitly: it switches the calling thread to the in-band execution stage, for running under the main kernel supervision. Any EVL thread which issues a system call to the main kernel will be switched to the in-band context automatically, so in most cases there should be no point in dealing with this manually in applications.\nevl_switch_inband() is defined for the rare circumstances where some high-level API based on the EVL core library might have to enforce a particular execution stage, based on a deep knowledge of how EVL works internally. Entering a syscall-free section of code for which the in-band mode needs to be guaranteed on entry would be the only valid reason to call evl_switch_inband(). This call returns zero on success, or a negated error code if something went wrong:\n-EPERM The current thread is not attached to the EVL core.\nForcing the current execution stage between in-band and out-of-band stages is a heavyweight operation: this entails two thread context switches both ways, as the switching thread is offloaded to the opposite scheduler. You really don\u0026rsquo;t want to force this explicitly unless you definitely have to and fully understand the implications of it runtime-wise. Bottom line is that switching the execution stage to in-band from within a time-critical code is a clear indication that something is wrong in such code.\n  int evl_set_schedattr(int efd, const struct evl_sched_attrs *attrs)  This call changes the scheduling attributes for the thread referred to by efd in the EVL core.\nefdA file descriptor referring to the target thread, as returned by evl_attach_self(), evl_get_self(), or opening a thread element device in /dev/evl/thread using open(2).\n\nattrsA structure defining the new set of attributes, which depends on the scheduling policy mentioned in attrs-\u0026gt;sched_policy. EVL currently implement the following policies:\n\n SCHED_FIFO, which is the common first-in, first-out real-time policy.\n SCHED_RR, defining a real-time, round-robin policy in which each member of the class is allotted an individual time quantum before the CPU is given to the next thread.\n SCHED_QUOTA, which enforces a limitation on the CPU consumption of threads over a fixed period of time, known as the global quota period. Threads undergoing this policy are pooled in groups, with each group being given a share of the period.\n SCHED_WEAK, which is a non real-time policy allowing its members to run in-band most of the time, while retaining the ability to request EVL services, at the expense of briefly switching to the out-of-band execution stage on demand.\n  evl_sched_attrs() returns zero on success, otherwise a negated error code is returned:\n-EBADF efd is not a valid thread descriptor.\n-EINVAL Some of the parameters in attrs are wrong. Check attrs-\u0026gt;sched_policy, and the policy-specific information may EVL expect for more.\n-ESTALE efd refers to a stale thread, see these notes.\n#include \u0026lt;evl/thread.h\u0026gt; int change_self_schedparams(void) { struct evl_sched_attrs attrs; int ret; attrs.sched_policy = SCHED_FIFO; attrs.sched_priority = 90; return evl_set_schedattr(evl_get_self(), \u0026amp;attrs); }  evl_set_schedattr() immediately changes the scheduling attributes the EVL core uses for the target thread when it runs in out-of-band context. Later on, the next time such thread transitions from out-of-band to in-band context, the main kernel will apply an extrapolated version of those changes to its own scheduler as well.\nThe extrapolation of the out-of-band scheduling attributes passed to evl_set_schedattr() to the in-band ones applied by the mainline kernel works as follows:\n   out-of-band policy in-band policy     SCHED_FIFO, prio SCHED_FIFO, prio   SCHED_RR, prio SCHED_FIFO, prio   SCHED_WEAK, prio \u0026gt; 0 SCHED_FIFO   SCHED_WEAK, prio == 0 SCHED_OTHER    Calling pthread_setschedparam() from the C library does not affect the scheduling attributes of an EVL thread. It only affects the scheduling parameters of such thread from the standpoint of the main kernel. Because the C library may cache the current scheduling attributes for the in-band context of a thread - glibc does so typically - the cached value may not reflect the actual scheduling attributes of the thread after this call.\n  int evl_get_schedattr(int efd, struct evl_sched_attrs *attrs)   int evl_get_state(int efd, struct evl_thread_state *statebuf)  Can EVL threads run in kernel space? Yes. Drivers may create kernel-based EVL threads backed by regular kthreads, using EVL\u0026rsquo;s kernel API. The attachment phase is hidden inside the API call starting the EVL kthread in this case. Most of the notions explained in this document apply to them too, except that there is no system call interface between the EVL core and the kthread. So nothing can prevent EVL kthreads from calling the main kernel services from the wrong context.\nWhere do thread devices live? Each time a new thread element is created, it appears into the /dev/evl/thread hierarchy, e.g.:\n$ ls -l /dev/evl/threads total 0 crw-rw---- 1 root root 246, 1 Jan 1 1970 clone crw-rw---- 1 root root 244, 0 Mar 1 11:26 rtk1@0:1682 crw-rw---- 1 root root 244, 18 Mar 1 11:26 rtk1@1:1682 crw-rw---- 1 root root 244, 36 Mar 1 11:26 rtk1@2:1682 crw-rw---- 1 root root 244, 54 Mar 1 11:26 rtk1@3:1682 crw-rw---- 1 root root 244, 1 Mar 1 11:26 rtk2@0:1682 crw-rw---- 1 root root 244, 19 Mar 1 11:26 rtk2@1:1682 crw-rw---- 1 root root 244, 37 Mar 1 11:26 rtk2@2:1682 crw-rw---- 1 root root 244, 55 Mar 1 11:26 rtk2@3:1682 (snip) crw-rw---- 1 root root 244, 9 Mar 1 11:26 rtus_ufps0-10:1682 crw-rw---- 1 root root 244, 8 Mar 1 11:26 rtus_ufps0-9:1682 crw-rw---- 1 root root 244, 27 Mar 1 11:26 rtus_ufps1-10:1682 crw-rw---- 1 root root 244, 26 Mar 1 11:26 rtus_ufps1-9:1682 crw-rw---- 1 root root 244, 45 Mar 1 11:26 rtus_ufps2-10:1682 crw-rw---- 1 root root 244, 44 Mar 1 11:26 rtus_ufps2-9:1682 crw-rw---- 1 root root 244, 63 Mar 1 11:26 rtus_ufps3-10:1682 crw-rw---- 1 root root 244, 62 Mar 1 11:26 rtus_ufps3-9:1682  The clone file is a special device which allows the EVL library to request the creation of a thread element. This is for internal use only.\n How to reach a remote EVL thread? If you need to submit requests for an EVL thread which belongs to a different process, you only need to open the device file representing this element in /dev/evl/threads, then use the file descriptor just obtained in the thread-related request you want to send it. For instance, we could change the scheduling parameters of an EVL kernel thread named rtk1@3:1682 from a companion application in userland as follows:\n\tstruct evl_sched_attrs attrs; int efd, ret; efd = open(\u0026quot;/dev/evl/thread/rtk1@3:1682\u0026quot;, O_RDWR); /* skipping checks */ attrs.sched_policy = SCHED_FIFO; attrs.sched_priority = 90; ret = evl_set_schedattr(efd, \u0026amp;attrs); /* skipping checks */  Where to look for thread information? Since every element is backed by a regular character device, the place to look for thread attributes is in the /sysfs hierarchy, where such device is living. For instance, we can have a look at the attributes exported by the sampling thread of EVL\u0026rsquo;s latmus utility like this:\ncd /sys/devices/virtual/thread/lat-sampler:2136 # ls -l total 0 -r--r--r-- 1 root root 4096 Mar 1 12:01 pid -r--r--r-- 1 root root 4096 Mar 1 12:01 sched -r--r--r-- 1 root root 4096 Mar 1 12:01 state -r--r--r-- 1 root root 4096 Mar 1 12:01 stats -r--r--r-- 1 root root 4096 Mar 1 12:01 timeout # cat pid sched state stats timeout 2140 0 90 90 rt 0x8002 1 311156 311175 46999122352 0 0  "
},
{
	"uri": "https://evenless.org/core/kernel-api/kthread/",
	"title": "Kernel threads",
	"tags": [],
	"description": "",
	"content": " evl_run_kthread(__kthread, __fn, __priority, __fmt, __args\u0026hellip;) evl_run_kthread_on_cpu(__kthread, __cpu, __fn, __priority, __fmt, __args\u0026hellip;) void evl_cancel_kthread(struct evl_kthread *kthread) int evl_kthread_should_stop(void) evl_set_kthread_priority(struct evl_kthread *thread, int priority) int evl_sleep_until(ktime_t timeout) int evl_sleep(ktime_t delay) int evl_set_thread_period(struct evl_clock *clock, ktime_t idate, ktime_t period) int evl_wait_thread_period(unsigned long *overruns_r) "
},
{
	"uri": "https://evenless.org/core/user-api/",
	"title": "The EVL library",
	"tags": [],
	"description": "",
	"content": "The EVL interface library in user-space - aka libevl - is fairly compact, with roughly 50 routines, which is less than half of Xenomai\u0026rsquo;s POSIX interface. This API gives access to the real-time services implemented by the EVL core in kernel space, nothing more.\n Do not expect a full-blown glibc-like library with a truckload of services and utilities, this is mostly a set of system call wrappers.\n Do not expect a POSIX-compliant interface, this is purely an ad hoc one, closely matching the semantics of the EVL core.\n  Some may only need the basic features readily available from libevl in their applications, others may want to implement high level service libraries based on those features, YMMV.\n"
},
{
	"uri": "https://evenless.org/core/",
	"title": "The EVL core",
	"tags": [],
	"description": "",
	"content": " Make it ordinary, make it simple The EVL core is an autonomous software core which is hosted by the kernel, delivering real-time services to applications having to meet stringent timing requirements. This small core is built like any ordinary feature of the Linux kernel, not as a foreign extension slapped on top of it. Dovetail plays an important part here, as it hides the nitty-gritty details of embedding a companion core into the kernel.\nThe user-space interface to this core is the EVL library (libevl.so), which implements the basic system call wrappers, along with the fundamental thread synchronization services. No bells and whistles, only the basic stuff. The intent is to provide simple mechanisms, complex semantics and policies can and should be implemented in high level APIs based on this library running in userland.\nElements As the name suggests, elements are the basic features we may require from the EVL core for supporting real-time applications in this dual kernel environment. Also, only the kernel could provide such features in an efficient way, pure user-space code could not deliver. So far, it looks like we need only four elements:\n thread. As the basic execution unit, we want it to be runnable either in real-time mode or regular GPOS mode alternatively, which exactly maps to Dovetail\u0026rsquo;s out-of-band and inband contexts.\n monitor. This element has the same purpose than the main kernel\u0026rsquo;s futex, which is about providing an integrated - although much simpler - set of fundamental thread synchronization features. It is used by the EVL library to implement mutexes, condition variables and semaphores in user-space.\n clock. We may find platform-specific clock devices in addition to the core ones defined by the architecture, for which ad hoc drivers should be written. The clock element ensures that all clock drivers present the same interface to applications in user-space. In addition, this element can export individual software timers to applications which comes in handy for running periodic loops or waiting for oneshot events on a specific time base.\n cross-buffer. A cross-buffer (aka xbuf) is a bi-directional communication channel for exchanging data between out-of-band and in-band thread contexts, without impacting the real-time performance on the out-of-band side. Any kind of thread (EVL or regular) can wait/poll for input from the other side. Cross-buffers serve the same purpose than Xenomai\u0026rsquo;s message pipes implemented by the XDDP socket protocol.\n  Everything is a file The nice thing about the file semantics is that it may solve general problems for our embedded real-time core:\n it can organize resource management for EVL\u0026rsquo;s kernel objects. If every element we export to the user is represented by a file, we can leave the hard work of managing the creation and release process to the VFS, tracking references to every element from file descriptors.\n if a file backing some element can be obtained by opening a device present in the file system, we are done with providing applications a way to share this element between multiple processes: these processes would only need to open the same device file for sharing the underlying element.\n we can benefit from the file permission, monitoring and auditing logic attached to files for our own elements.\n  Now, one might wonder: since the main kernel would be involved in creating and deleting elements, wouldn\u0026rsquo;t this prevent us from doing so in mere real-time mode? Short answer: surely it would, and this is just fine. Nearly two decades after Xenomai v1, I\u0026rsquo;m still to find the first practical use case which would require this. As a matter of fact, those potentially heavyweight operations can and should happen when the application is not busy running time-critical code.\nThe above translates as follows in EVL:\n Each time an application creates a new element, a character device appears in the file system hierarchy under a directory named /dev/evl/element_type/. By opening the device file, the application receives a file descriptor which can be used for controlling and/or exchanging data with the underlying element. This is definitely a regular file descriptor, on a regular character device.\n Since every element is backed by a kernel device, we may also bind udev rules to events of interest on such element. We may also export the internal state of any element via the /sysfs, which is much better than stuffing /proc with even more ad hoc files for the same purpose.\n Since every file opened on the same device refers to the same EVL element, we have our handle for sharing elements between processes.\n Even local resources created by the EVL core passed to applications which are not elements are also backed by a file, like clock-based individual timers.\n Using file descriptors, the application can monitor events occurring on an arbitrary set of elements with a single EVL system call, just like one would do using poll(2), epoll(7) or select(2).\n  Basic utilities We also need EVL to provide a few ancillary services, so that high level APIs or even applications could build over them. So far, the following came to mind:\n synchronous I/O multiplexing. Since everything is a file in EVL, and we have file descriptors to play with in applications for referring to them, providing the EVL equivalent of [e]poll just makes sense.\n file proxy. Linux-based dual kernel systems are nasty by design: the huge set of GPOS features is always visible to applications but they should not to use it when they carry out real-time work with the help of the autonomous core, or risk unbounded response time. Because such exclusion includes manipulating files created by the main kernel: so much for using printf(3) in some time-sensitive loop in this case. The file proxy should help in easing the pain, by channeling file output through the vfs in a way that keeps the real-time side happy. Although the proxy is technically part of the element framework in EVL because it is represented by a device in the file system, applications could resort to a pure user-space implementation for roughly the same purpose, even if in a much more convoluted way. So the file proxy is considered a utility, not an element per se.\n trace channel. We need a way to emit traces from applications through the main kernel\u0026rsquo;s FTRACE mechanism for debugging purpose, directly from the real-time context. Dovetail readily enables tracing from the out-of-band stage, so all we need is an interface here.\n  We could not use the file proxy to relay the traces through the trace_marker file, because we want the tracepoint to appear in the output stream at the exact time the code running out-of-band issued it. On the contrary, channeling the trace data through the proxy would mean to defer the trace output, until the main kernel resumes execution, which would make the trace data useless.\n EVL device drivers are (almost) common drivers EVL does not introduce any specific driver model. It exports a dedicated kernel API for implementing real-time I/O operations in common character device drivers. In fact, the EVL core is composed of a set of such drivers, implementing each class of elements and utilities.\nDeveloping the EVL core Just like Dovetail, developing the EVL core is customary Linux kernel development, with the addition of dual kernel specific background.\nThe development tip of the EVL core is maintained in the evl/master branch of the following GIT repository which tracks the mainline kernel:\n git://git.evenless.org/linux-evl.git https://git.evenless.org/linux-evl.git  This branch is routinely rebased over Dovetail\u0026rsquo;s dovetail/master branch from the same repository.\n"
},
{
	"uri": "https://evenless.org/dovetail/pipeline/porting/irqflow/",
	"title": "Interrupt flow",
	"tags": [],
	"description": "",
	"content": " Adapting the generic interrupt management (genirq) Interrupt pipelining involves a basic change in controlling the interrupt flow: handle_domain_irq() from the IRQ domain API redirects all parent IRQs to the pipeline entry by calling generic_pipeline_irq(), instead of generic_handle_irq().\nGeneric flow handlers acknowledge the incoming IRQ event in the hardware as usual, by calling the appropriate irqchip routine (e.g. irq_ack(), irq_eoi()) according to the interrupt type. However, the flow handlers do not immediately invoke the in-band interrupt handlers. Instead, they hand the event over to the pipeline core by calling handle_oob_irq().\nIf an out-of-band handler exists for the interrupt received, handle_oob_irq() invokes it immediately, after switching the execution context to the oob stage if not current yet. Otherwise, the event is marked as pending in the in-band stage\u0026rsquo;s log for the current CPU.\nThe execution flow throughout the kernel code in Dovetail\u0026rsquo;s pipelined interrupt model is illustrated by the following figure. Note the two-step process: first we try delivering the incoming IRQ to any out-of-band handler if present, then we may play any IRQ pending in the current per-CPU log, among which non-OOB events may reside.\nAs illustrated above, interrupt flow handlers may run twice for a single IRQ in Dovetail\u0026rsquo;s pipelined interrupt model:\n first to submit the event immediately to any out-of-band handler which may be interested in it. This is achieved by calling handle_oob_irq(), whose role is to invoke such handler(s) if present, or schedule an in-band handling of the IRQ event if not.\n finally to run the in-band handler(s) accepting the IRQ event if it was not delivered to any out-of-band handler. To deliver the event to any in-band handler(s), the interrupt flow handler is called again by the pipeline core. When this happens, the flow handler processes the interrupt as usual, skipping the call to handle_oob_irq() though.\n  Any incoming IRQ event is either dispatched to one or more out-of-band handlers, or one or more in-band handlers, but never to a mix of them. Also, because every interrupt which is not handled by an out-of-band handler will end up into the in-band stage\u0026rsquo;s event log unconditionally, all external interrupts must have a handler in the in-band code - which should be the case for a sane kernel anyway.\n Once generic_pipeline_irq() has returned, if the preempted execution context was running over the in-band stage unstalled, the pipeline core synchronizes the interrupt state immediately, meaning that all IRQs found pending in the in-band stage\u0026rsquo;s log are immediately delivered to their respective in-band handlers. In all other situations, the IRQ frame is left immediately without running those handlers. The IRQs may remain pending until the in-band code resumes from preemption, then clears the virtual interrupt disable flag, which would cause the interrupt state to be synchronized, running the in-band handlers eventually.\nDeferring level-triggered IRQs In absence of any out-of-band handler for the event, the device may keep asserting the interrupt signal until the cause has been lifted in its own registers. At the same time, we might not be allowed to run the in-band handler immediately over the current interrupt context if the in-band stage is currently stalled, we would have to wait for the in-band code to accept interrupts again. However, the interrupt disable bit in the CPU would certainly be cleared in the meantime. For this reason, depending on the interrupt type, the flow handlers as modified by the pipeline code may have to mask the interrupt line until the in-band handler has run from the in-band stage, lifting the interrupt cause. This typically happens with level-triggered interrupts, preventing the device from storming the CPU with a continuous interrupt request.\n The pathological case\n/* no OOB handler, in-band stage stalled on entry leading to deferred dispatch to handler */ asm_irq_entry ... -\u0026gt; generic_pipeline_irq() ... \u0026lt;IRQ logged, delivery deferred\u0026gt; asm_irq_exit /* * CPU allowed to accept interrupts again with IRQ cause not * acknowledged in device yet =\u0026gt; **IRQ storm**. */ asm_irq_entry ... asm_irq_exit asm_irq_entry ... asm_irq_exit   Since all of the IRQ handlers sharing an interrupt line are either in-band or out-of-band in a mutually exclusive way, such masking cannot delay out-of-band events.\nThe logic behind masking interrupt lines until events are processed at some point later - out of the original interrupt context - applies exactly the same to the threaded interrupt model (i.e. IRQF_THREAD). In this case, interrupt lines may be masked until the IRQ thread is scheduled in, after the interrupt handler clears the event cause eventually.\n Adapting the interrupt flow handlers to pipelining The logic for adapting flow handlers dealing with interrupt pipelining is composed of the following steps:\n (optionally) enter the critical section protected by the IRQ descriptor lock, if the interrupt is shared among processors (e.g. device interrupts). If so, check if the interrupt handler may run on the current CPU (irq_may_run()). By definition, no locking would be required for per-CPU interrupts.\n check whether we are entering the pipeline in order to deliver the interrupt to any out-of-band handler registered for it. on_pipeline_entry() returns a boolean value denoting this situation.\n if on pipeline entry, we should pass the event on to the pipeline core by calling handle_oob_irq(). Upon return, this routine tells the caller whether any out-of-band handler was fired for the event.\n if so, we may assume that the interrupt cause is now cleared in the device, and we may leave the flow handler, after having restored the interrupt line into a normal state. In case of a level-triggered interrupt which has been masked on entry to the flow handler, we need to unmask the line before leaving.\n if no out-of-band handler was called, we should have performed any acknowledge and/or EOI to release the interrupt line in the controller, while leaving it masked if required before exiting the flow handler. In case of a level-triggered interrupt, we do want to leave it masked for solving the pathological case with interrupt deferral explained earlier.\n  if not on pipeline entry (i.e. second entry of the flow handler), then we must be running over the in-band stage, accepting interrupts, therefore we should fire the in-band handler(s) for the incoming event.\n   Example: adapting the handler dealing with level-triggered IRQs\n --- a/kernel/irq/chip.c +++ b/kernel/irq/chip.c void handle_level_irq(struct irq_desc *desc) { raw_spin_lock(\u0026amp;desc-\u0026gt;lock); mask_ack_irq(desc); if (!irq_may_run(desc)) goto out_unlock; +\tif (on_pipeline_entry()) { +\tif (handle_oob_irq(desc)) +\tgoto out_unmask; +\tgoto out_unlock; +\t} + desc-\u0026gt;istate \u0026amp;= ~(IRQS_REPLAY | IRQS_WAITING); /* @@ -642,7 +686,7 @@ void handle_level_irq(struct irq_desc *desc) kstat_incr_irqs_this_cpu(desc); handle_irq_event(desc); - +out_unmask: cond_unmask_irq(desc); out_unlock: raw_spin_unlock(\u0026amp;desc-\u0026gt;lock); }  This change reads as follows:\n on entering the pipeline, which means immediately over the interrupt frame context set up by the CPU for receiving the event, tell the pipeline core about the incoming IRQ.\n if this IRQ was handled by an out-of-band handler (handle_oob_irq() returns true), consider the event to have been fully processed, unmasking the interrupt line before leaving. We can\u0026rsquo;t do more than this, simply because the in-band kernel code might expect not to receive any interrupt at this point (i.e. the virtual interrupt disable flag might be set for the in-band stage).\n otherwise, keep the interrupt line masked until handle_level_irq() is called again from a safe context for handling in-band interrupts, at which point the event should be delivered to the in-band interrupt handler of the main kernel. We have to keep the line masked to prevent the IRQ storm which would certainly happen otherwise, since no handler has cleared the cause of the interrupt event in the device yet.\n  Fixing up the IRQ chip drivers We must make sure that the following handlers exported by irqchip drivers can operate over the out-of-band context safely:\n irq_mask() irq_ack() irq_mask_ack() irq_eoi() irq_unmask()  In most cases, no change is required, because genirq ensures that a single CPU handles a given IRQ event by holding the per-descriptor irq_desc::lock spinlock across calls to those irqchip handlers, and such lock is automatically turned into a mutable spinlock when pipelining interrupts. In other words, those handlers are running interrupt-free as their non-pipelined implementation expects it.\nHowever, there might other reasons to fix up some of those handlers:\n they must not invoke any in-band kernel service, which might cause an invalid context re-entry.\n there may be inner spinlocks locally defined by some irqchip drivers for serializing access to a common interrupt controller hardware for distinct IRQs being handled by multiple CPUs concurrently. Adapting such spinlocked sections found in irqchip drivers to support interrupt pipelining may involve converting the related spinlocks to hard spinlocks.\n  Other section of code which were originally serialized by common interrupt disabling may need to be made fully atomic for running consistenly in pipelined interrupt mode. This can be done by introducing hard masking, converting local_irq_save() calls to hard_local_irq_save(), conversely local_irq_restore() to hard_local_irq_restore().\nFinally, IRQCHIP_PIPELINE_SAFE must be added to the struct irqchip::flags member of a pipeline-aware irqchip driver, in order to notify the kernel that such controller can operate in pipelined interrupt mode. Even if you did not introduce any other change to support pipelining, this one is required: it tells the kernel that you did review the code for that purpose.\n Adapting the ARM GIC driver to interrupt pipelining\n --- a/drivers/irqchip/irq-gic.c +++ b/drivers/irqchip/irq-gic.c @@ -93,7 +93,7 @@ struct gic_chip_data { #ifdef CONFIG_BL_SWITCHER -static DEFINE_RAW_SPINLOCK(cpu_map_lock); +static DEFINE_HARD_SPINLOCK(cpu_map_lock); #define gic_lock_irqsave(f)\t\\ raw_spin_lock_irqsave(\u0026amp;cpu_map_lock, (f)) @@ -424,7 +424,8 @@ static const struct irq_chip gic_chip = { .irq_set_irqchip_state\t= gic_irq_set_irqchip_state, .flags\t= IRQCHIP_SET_TYPE_MASKED | IRQCHIP_SKIP_SET_WAKE | -\tIRQCHIP_MASK_ON_SUSPEND, +\tIRQCHIP_MASK_ON_SUSPEND | +\tIRQCHIP_PIPELINE_SAFE, }; void __init gic_cascade_irq(unsigned int gic_nr, unsigned int irq)  In some (rare) cases, we might have a bit more work for adapting an interrupt chip driver. For instance, we might have to convert a sleeping spinlock to a raw spinlock first, so that we can convert the latter to a hard spinlock eventually. Hard spinlocks like raw ones should be manipulated via the raw_spin_lock() API, unlike sleeping spinlocks. This change makes even more sense as sleeping is not allowed while running in the pipeline entry context anyway.\n Adapting the BCM2835 pin control driver to interrupt pipelining\n --- a/drivers/pinctrl/bcm/pinctrl-bcm2835.c +++ b/drivers/pinctrl/bcm/pinctrl-bcm2835.c @@ -90,7 +90,7 @@ struct bcm2835_pinctrl { struct gpio_chip gpio_chip; struct pinctrl_gpio_range gpio_range; -\tspinlock_t irq_lock[BCM2835_NUM_BANKS]; +\thard_spinlock_t irq_lock[BCM2835_NUM_BANKS]; }; /* pins are just named GPIO0..GPIO53 */ @@ -461,10 +461,10 @@ static void bcm2835_gpio_irq_enable(struct irq_data *data) unsigned bank = GPIO_REG_OFFSET(gpio); unsigned long flags; -\tspin_lock_irqsave(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); +\traw_spin_lock_irqsave(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); set_bit(offset, \u0026amp;pc-\u0026gt;enabled_irq_map[bank]); bcm2835_gpio_irq_config(pc, gpio, true); -\tspin_unlock_irqrestore(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); +\traw_spin_unlock_irqrestore(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); } static void bcm2835_gpio_irq_disable(struct irq_data *data) @@ -476,12 +476,12 @@ static void bcm2835_gpio_irq_disable(struct irq_data *data) unsigned bank = GPIO_REG_OFFSET(gpio); unsigned long flags; -\tspin_lock_irqsave(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); +\traw_spin_lock_irqsave(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); bcm2835_gpio_irq_config(pc, gpio, false); /* Clear events that were latched prior to clearing event sources */ bcm2835_gpio_set_bit(pc, GPEDS0, gpio); clear_bit(offset, \u0026amp;pc-\u0026gt;enabled_irq_map[bank]); -\tspin_unlock_irqrestore(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); +\traw_spin_unlock_irqrestore(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); } static int __bcm2835_gpio_irq_set_type_disabled(struct bcm2835_pinctrl *pc, @@ -584,7 +584,7 @@ static int bcm2835_gpio_irq_set_type(struct irq_data *data, unsigned int type) unsigned long flags; int ret; -\tspin_lock_irqsave(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); +\traw_spin_lock_irqsave(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); if (test_bit(offset, \u0026amp;pc-\u0026gt;enabled_irq_map[bank])) ret = __bcm2835_gpio_irq_set_type_enabled(pc, gpio, type); @@ -596,7 +596,7 @@ static int bcm2835_gpio_irq_set_type(struct irq_data *data, unsigned int type) else irq_set_handler_locked(data, handle_level_irq); -\tspin_unlock_irqrestore(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); +\traw_spin_unlock_irqrestore(\u0026amp;pc-\u0026gt;irq_lock[bank], flags); return ret; } @@ -618,6 +618,7 @@ static struct irq_chip bcm2835_gpio_irq_chip = { .irq_ack = bcm2835_gpio_irq_ack, .irq_mask = bcm2835_gpio_irq_disable, .irq_unmask = bcm2835_gpio_irq_enable, +\t.flags = IRQCHIP_PIPELINE_SAFE, }; static int bcm2835_pctl_get_groups_count(struct pinctrl_dev *pctldev) @@ -1047,7 +1048,7 @@ static int bcm2835_pinctrl_probe(struct platform_device *pdev) for_each_set_bit(offset, \u0026amp;events, 32) bcm2835_gpio_wr(pc, GPEDS0 + i * 4, BIT(offset)); -\tspin_lock_init(\u0026amp;pc-\u0026gt;irq_lock[i]); +\traw_spin_lock_init(\u0026amp;pc-\u0026gt;irq_lock[i]); } err = gpiochip_add_data(\u0026amp;pc-\u0026gt;gpio_chip, pc);  irq_set_chip() will complain loudly with a kernel warning whenever the irqchip descriptor passed does not bear the IRQCHIP_PIPELINE_SAFE flag and CONFIG_IRQ_PIPELINE is enabled. Take this warning as a sure sign that your port of the IRQ pipeline to the target system is incomplete.\n Kernel preemption control (PREEMPT) When pipelining is enabled, preempt_schedule_irq() reconciles the virtual interrupt state - which has not been touched by the assembly level code upon kernel entry - with basic assumptions made by the scheduler core, such as entering with (virtual) interrupts disabled.\nExtended IRQ work API Due to the NMI-type nature of interrupts running out-of-band code from the standpoint of the main kernel, such code might preempt in-band activities in the middle of a critical section. For this reason, it would be unsafe to call any in-band routine from an out-of-band context.\nHowever, we may schedule execution of in-band work handlers from out-of-band code, using the regular irq_work_queue() service which has been extended by the IRQ pipeline core. Such work request from the oob stage is scheduled for running on the in-band stage on the issuing CPU as soon as the out-of-band activity quiesces on this processor. As its name implies, the work handler runs in (in-band) interrupt context.\nThe interrupt pipeline forces the use of a synthetic IRQ as a notification signal for the IRQ work machinery, instead of a hardware-specific interrupt vector. This special IRQ is labeled in-band work when reported by /proc/interrupts.\n "
},
{
	"uri": "https://evenless.org/dovetail/pipeline/",
	"title": "Interrupt pipeline",
	"tags": [],
	"description": "",
	"content": " The autonomous core has to act upon device interrupts with no delay, regardless of the other kernel operations which may be ongoing when the interrupt is received by the CPU. Therefore, there is a basic requirement for prioritizing interrupt masking and delivery between the autonomous core and GPOS operations, while maintaining consistent internal serialization for the kernel.\nHowever, to protect from deadlocks and maintain data integrity, Linux hard disables interrupts around any critical section of code which must not be preempted by interrupt handlers on the same CPU, enforcing a strictly serialized execution among those contexts. The unpredictable delay this may cause before external events can be handled is a major roadblock for kernel components requiring predictable and very short response times to external events, in the range of a few microseconds.\nTo address this issue, Dovetail introduces a mechanism called interrupt pipelining which turns all device IRQs into pseudo-NMIs, only to run NMI-safe interrupt handlers from the perspective of the main kernel activities. This is achieved by substituting real interrupt masking in a CPU by a software-based, virtual interrupt masking when the in-band stage is active on such CPU. This way, the autonomous core can receive IRQs as long as it did not mask interrupts in the CPU, regardless of the virtual interrupt state maintained by the in-band side. Dovetail monitors the virtual state to decide when IRQ events should be allowed to flow down to the in-band stage where the main kernel executes. This way, the assumptions the in-band code makes about running interrupt-free or not are still valid.\nTwo-stage IRQ pipeline Interrupt pipelining is a lightweight approach based on the introduction of a separate, high-priority execution stage for running out-of-band interrupt handlers immediately upon IRQ receipt, which cannot be delayed by the in-band, main kernel work. By immediately, we mean unconditionally, regardless of whether the in-band kernel code had disabled interrupts when the event arrived, using the common local_irq_save(), local_irq_disable() helpers or any of their derivatives. IRQs which have no handlers in the high priority stage may be deferred on the receiving CPU until the out-of-band activity has quiesced on that CPU. Eventually, the preempted in-band code can resume normally, which may involve handling the deferred interrupts.\nIn other words, interrupts are flowing down from the out-of-band to the in-band interrupt stages, which form a two-stage pipeline for prioritizing interrupt delivery. The runtime context of the out-of-band interrupt handlers is known as the oob stage of the pipeline, as opposed to the in-band kernel activities sitting on the in-band stage:\nAn autonomous core can base its own activities on the oob stage, interposing on specific IRQ events, for delivering real-time capabilities to a particular set of applications. Meanwhile, the main kernel operations keep going over the in-band stage unaffected, only delayed by short preemption times for running the out-of-band work.\n"
},
{
	"uri": "https://evenless.org/core/kernel-api/mutex/",
	"title": "Kernel mutexes",
	"tags": [],
	"description": "",
	"content": " void evl_init_kmutex(struct evl_kmutex *kmutex) void evl_destroy_kmutex(struct evl_kmutex *kmutex) int evl_trylock_kmutex(struct evl_kmutex *kmutex) int evl_lock_kmutex(struct evl_kmutex *kmutex) void evl_unlock_kmutex(struct evl_kmutex *kmutex) DEFINE_EVL_MUTEX(__name) "
},
{
	"uri": "https://evenless.org/core/user-api/mutex/",
	"title": "Mutex",
	"tags": [],
	"description": "",
	"content": " int evl_new_mutex(struct evl_mutex *mutex, int clockfd, const char *fmt, \u0026hellip;) int evl_new_mutex_ceiling(struct evl_mutex *mutex, int clockfd, unsigned int ceiling, const char *fmt, \u0026hellip;) int evl_open_mutex(struct evl_mutex *mutex, const char *fmt, \u0026hellip;) int evl_lock(struct evl_mutex *mutex) int evl_timedlock(struct evl_mutex *mutex, const struct timespec *timeout) int evl_trylock(struct evl_mutex *mutex) int evl_unlock(struct evl_mutex *mutex) int evl_set_mutex_ceiling(struct evl_mutex *mutex, unsigned int ceiling) int evl_get_mutex_ceiling(struct evl_mutex *mutex) int evl_close_mutex(struct evl_mutex *mutex) EVL_MUTEX_INITIALIZER(__name, __clock) EVL_MUTEX_CEILING_INITIALIZER(__name, __clock, __ceiling) "
},
{
	"uri": "https://evenless.org/dovetail/altsched/",
	"title": "Alternate scheduling",
	"tags": [],
	"description": "",
	"content": " Dovetail promotes the idea that a dual kernel system should keep the functional overlap between the kernel and the autonomous core minimal. To this end, a task from such core should be merely seen as a regular task with additional scheduling capabilities guaranteeing very low and bounded response times. To support such idea, Dovetail enables kthreads and regular user tasks to run alternatively in the out-of-band execution context introduced by the interrupt pipeline (aka oob stage), or the common in-band kernel context for GPOS operations (aka in-band stage).\nAs a result, autonomous core applications in user-space benefit from the common Linux programming model - including virtual memory protection -, and still have access to the main kernel services when carrying out non time-critical work.\nDovetail provides mechanisms to autonomous cores for supporting this as follows:\n services for moving Linux tasks back and forth between the in-band and out-of-band stages in an orderly and safe way, properly synchronizing the operation between the two schedulers involved.\n notifications about in-band events the autonomous core may want to know about for maintaining its own version of the current task\u0026rsquo;s state.\n notifications about events such as faults, syscalls and other exceptions issued from the out-of-band execution stage.\n integrated support for performing context switches between out-of-band tasks, including memory context and FPU management.\n  Theory of operations  a Linux task running in user-space, or a kernel thread, need to initialize the alternate scheduling feature for themselves with a call to dovetail_init_altsched(). For instance, the EVL core does so as part of the attachment process of a thread to the autonomous core when evl_attach_self() is called by the application.\n once the alternate scheduling feature is initialized, the task should enable it by a call to dovetail_start_altsched(). From this point, that task:\n can switch between the in-band and out-of-band execution stages freely.\n can emit out-of-band system calls the autonomous core can handle.\n is tracked by Dovetail\u0026rsquo;s notification system, so that in-band and out-of-band events which may involve such task are dispatched to the core.\n can be involved in Dovetail-based context switches triggered by the autonomous core, independently from the scheduling operations carried out by the main kernel.\n  Conversely, dovetail_stop_altsched() disables the notifications for a task, which is likely to be detached from the autonomous core later on.\n at any point in time, any Linux task is either controlled by the main kernel or by the autonomous core, scheduling-wise. There cannot be any overlap (for obvious reasons):\n a task which is currently scheduled by the autonomous core runs or sleeps on the out-of-band stage. At the same time, such task is deemed to be sleeping in TASK_INTERRUPTIBLE state for the main kernel.\n conversely, a task which is controlled by the main kernel runs or sleeps on the in-band stage. It must be considered as blocked on the other side. For instance, the EVL core defines the T_INBAND blocking condition for representing such state.\n  since the out-of-band stage receives interrupts first, regardless of any interrupt masking which may be in effect for the ongoing in-band work, the autonomous core can run high-priority interrupt handlers then reschedule with no delay.\n tasks may switch back and forth between the in-band and out-of-band stages at will. However, this comes at a cost:\n the process of switching stages is heavyweight, it includes a double scheduling operation at least (i.e. one to suspend on the exited stage, one to resume from the opposite stage).\n only the out-of-band stage guarantees bounded response times to external and internal events. Therefore, a task which leaves the out-of-band stage for resuming in-band looses such guarantee, until it has fully switched back to out-of-band context at some point later.\n  at any point in time, the autonomous core keeps the CPU busy until no more task it knows about is runnable on that CPU, on the out-of-band stage. When the out-of-band activity quiesces, the core is expected to relinquish the CPU to the in-band stage by scheduling in the context which was originally preempted. This is commonly done by having a task placeholder with the lowest possible priority represent the main kernel and its in-band context linked to each per-CPU run queue maintained by the autonomous core.\n once the in-band context resumes, interrupt events which have no out-of-band handlers are delivered to the regular in-band IRQ handlers installed by the main kernel, if the virtual masking state allows it.\n   void dovetail_init_altsched(struct dovetail_altsched_context *p)  This call initializes the alternate scheduling context for the current task; this should be done once, before the task calls dovetail_start_altsched().\npThe alternate scheduling context is kept in a per-task structure of type _dovetail_altschedcontext which should be maintained by the autonomous core. This can be done as part of the per-task context management facility Dovetail introduces.\n\n void dovetail_start_altsched(void)  This call tells the kernel that the current task may request alternate scheduling operations any time from now on, such as switching out-of-band or back in-band. It also activates the event notifier for the task, which allows it to emit out-of-band system calls to the core.\n void dovetail_stop_altsched(void)  This call disables the event notifier for the current task, which must be done before dismantling the alternate scheduling support for that task in the autonomous core.\nWhat you really need to know at this point There is a not-so-subtle but somewhat confusing distinction between running a Linux task out-of-band, and running whatever code from the out-of-band interrupt stage.\n In the first case, the task is not controlled by the main kernel scheduler anymore, but runs under the supervision of the autonomous core. This is obtained by performing an out-of-band switch for a task.\n In the other case, the current underlying context (task or whatever else) may or may not be controlled by the main kernel, but the interrupt pipeline machinery has switched the CPU to out-of-band mode, which means that only interrupts bearing the IRQF_OOB flag are delivered. Typically, run_oob_call() is a service provided by the interrupt pipeline which executes a function call over this context, without requiring the calling task to be scheduled by the autonomous core.\n  You will also find references to pseudo-routines called core_schedule(), core_suspend_task() or core_resume_task() in various places. Don\u0026rsquo;t look for them into the code, they don\u0026rsquo;t actually exist: those routines are mere placeholders for the corresponding services you would provide in your autonomous core. For instance, the EVL core implements them as evl_schedule(), evl_suspend_thread() and evl_release_thread().\nYou may want to keep this in mind when going through the rest of this document.\nMigrating between execution stages Out-of-band switch Switching out-of-band is the operation by which a Linux task moves under the control of the alternate scheduler brought in the main kernel by the autonomous core. From that point, the scheduling decisions are made by this core regarding that task.\nThere are two reasons a Linux task may switch out-of-band:\n either it has requested it explicitly using a system call provided by the autonomous core such as EVL\u0026rsquo;s evl_switch_oob().\n or the autonomous core has forced such transition, in response to some request this task did, such as issuing a system call which can only be handled from the out-of-band stage. This behavior is enforced by the EVL core too.\n  Using Dovetail, a task which executes on the in-band stage moves out-of-band following this sequence of actions:\n this task calls dovetail_leave_inband(), which prepares for the transition, puts the caller to sleep (TASK_INTERRUPTIBLE) then reschedules immediately. At this point, the migrating task is in flight to the out-of-band stage. schedule() resumes the next in-band task which should run on the current CPU.\n as the next in-band task context resumes, the scheduling tail code checks for any task pending transition to out-of-band stage, before the CPU is fully relinquished to the resuming in-band task. This check is performed by the inband_switch_tail() call present in the main scheduler. Such call has two purposes:\n detect when a task is in flight to the out-of-band stage, so that we can notify the autonomous core for finalizing the migration process.\n detect when a task is resuming on the out-of-band stage, which happens when the autonomous core switches context back to the current task, once the migration process is complete.\n   When switching out-of-band, case #1 is met, which triggers a call to the resume_oob_task() handler the autonomous core should implement for completing the transition. This would typically mean: unblock the migrating task from the standpoint of its own scheduler, then reschedule. In the following flowchart, core_resume_task() and core_schedule() stand for these two operations, with each dotted link representing a context switch:\ngraph LR; S(\"start transition\") -- A style S fill:#99ccff; A[\"dovetail_leave_inband()\"] -- B[\"schedule()\"] style A fill:#99ccff; style B fill:#99ccff; B -.- C[\"inband_switch_tail()\"] C -- D{task in flight?} D --|Yes| E[\"resume_oob_task()\"] style E fill:#99ccff; D --|No| F{out-of-band?} E -- G[\"core_resume_task()\"] style G fill:#99ccff; G -- H[\"core_schedule()\"] style H fill:#ff950e; F --|Yes| I(transition complete) style I fill:#ff950e; F --|No| J(regular switch tail) style J fill:#99ccff; H -.- C  At the end of this process, we should have observed a double context switch, with the migrating task offloaded to the out-of-band scheduler:\nevl_switch_oob() implements the switch to out-of-band context in the EVL core, with support from evl_release_thread() and evl_schedule() for resuming and rescheduling threads respectively.\n In-band switch Switching in-band is the operation by which a Linux task moves under the control of the main scheduler, coming from the out-of-band execution stage. From that point, the scheduling decisions are made by the main kernel regarding that task.\nThere are several reasons a Linux task which was running out-of-band so far may have to switch in-band:\n it has requested it explicitly using a system call provided by the autonomous core such as EVL\u0026rsquo;s evl_switch_inband().\n the autonomous core has forced such transition for a task running in user-space:\n in response to some request this task did, such as issuing a regular system call which can only be handled from the in-band stage. Typically, this behavior is enforced by the EVL core.\n because the task received a synchronous fault or exception, such as a memory access violation, FPU exception and so on. A demotion is required, because handling such events directly from the out-of-band stage would require a fair amount of code duplication, and most likely raise all sorts of funky conflicts between the out-of-band handlers and several in-band sub-systems. Besides, there is not much point in expecting real-time guarantees from a code that basically fixes up a situation caused by a dysfunctioning application in the first place.\n a signal is pending for the task. Because the main kernel logic may require signals to be acknowledged by the recipient, we have to transition through the in-band stage to make sure the pending signal(s) will be delivered asap.\n   Using Dovetail, a task which executes on the out-of-band stage moves in-band following this sequence of actions:\n the execution of an in-band handler is scheduled from the context of the migrating task on the out-of-band stage. Once it runs, this handler should call wake_up_process() to unblock that task from the standpoint of the main kernel scheduler, since it is sleeping in TASK_INTERRUPTIBLE state there. Typically, the irq_work mechanism can be used for this, because:\n as extended by the interrupt pipeline support, this interface can be used from the out-of-band stage.\n the handler is guaranteed to run on the CPU from which the request was issued. Because the in-band work will wait until the out-of-band activity quiesces on that CPU, this in turn ensures that all other operations we have to carry out from the out-of-band stage for preparing the migration are done before the task is woken up eventually.\n  the autonomous core blocks/suspends the migrating task, rescheduling immediately afterwards. For instance, the EVL core adds the T_INBAND block bit to the task\u0026rsquo;s state for this purpose.\n at some point later, the out-of-band context is exited by the current CPU when no more out-of-band work is left, causing the in-band kernel code to resume execution at the latest preemption point. The handler scheduled at step #1 eventually runs, waking up the migrating task from the standpoint of the main kernel. The TASK_RUNNING state is set for the task.\n the migrating task resumes from the tail scheduling code of the alternate scheduler, where it suspended in step #2. Noticing the migration, the core calls dovetail_resume_inband() eventually, for finalizing the transition of the incoming task to the in-band stage.\n  In the following flowchart, core_suspend_task() and core_schedule() stand for the operations described at step #2, with each dotted link representing a context switch. The out-of-band idle state represents the CPU transitioning from out-of-band to in-band execution stage, as the core has no more out-of-band task to schedule:\ngraph LR; S(\"start transition\") -- A style S fill:#ff950e; A[\"irq_work(wakeup_req)\"] -- B[\"core_suspend_task()\"] style A fill:#ff950e; style B fill:#ff950e; B -- C[\"core_schedule()\"] style C fill:#ff950e; C -.- Y((OOB idle)) Y -.- D[\"wake_up_process()\"] style D fill:#99ccff; D -- E[\"schedule()\"] style E fill:#99ccff; E -.- X[\"out-of-band switch tail\"] style X fill:#99ccff; X -- G[\"dovetail_resume_inband()\"] style G fill:#99ccff; G -- I(\"transition complete\") style I fill:#99ccff;  At the end of this process, the task has transitioned from a running state to a blocked state in the autonomous core, and conversely from TASK_INTERRUPTIBLE to TASK_RUNNING in the main scheduler.\nevl_switch_inband() implements the switch to in-band context in the EVL core, with support from evl_suspend_thread() and evl_schedule() for suspending and rescheduling threads respectively.\n Dovetail context switching Dovetail allows an autonomous core embedded into the main kernel to schedule a set of Linux tasks out-of-band compared to the regular in-band kernel work, so that:\n the worse-case response time of such tasks only depends on the performance of a software core which has a limited complexity.\n the main kernel\u0026rsquo;s logic does not have to cope with stringent bounded response time requirements, which otherwise tends to lower the throughput while making the design, implementation and maintenance significantly more complex.\n  The flowchart below represents the typical context switch sequence an autonomous core should implement, from the context of the outgoing PREV task to the incoming NEXT task:\ngraph LR; S(PREV) -- A[\"core_schedule()\"] A -- B{in-band IRQ stage?} B --|Yes| D[\"jump out-of-band\"] D -- A B --|No| C[\"pick NEXT\"] style C fill:#ff950e; C -- P{PREV == NEXT?} style P fill:#ff950e; P --|Yes| Q(no change) style Q fill:#ff950e; P --|No| E{PREV == idle?} style E fill:#ff950e; E --|Yes| F[\"dovetail_resume_oob()\"] style F fill:#ff950e; E --|No| H[\"dovetail_context_switch()\"] style H fill:#ff950e; F -- H H -.- I(NEXT)  Those steps are:\n core_schedule() is called over the PREV context to check for the NEXT task to schedule, by priority order. If PREV still has the highest priority among runnable tasks, the sequence stops there. CAVEAT: the core must make sure to perform context switches from the out-of-band interrupt stage, otherwise weird things may happen down the road. run_oob_call() is a routine provided by the interrupt pipeline which may help there. See evl_schedule() in the EVL core for a typical usage.\n If NEXT is not the low priority placeholder task but PREV is, we will be preempting the in-band kernel: in this case, we must tell the kernel about such preemption by a call to dovetail_resume_oob(). This routine saves all context deemed necessary to restore it later on when the core switches back to the in-band execution stage. Otherwise, if NEXT is the placeholder task, there is no out-of-band task waiting for the CPU, and we will branch back to the last preemption point that led to calling dovetail_resume_oob() during the converse transition.\n dovetail_context_switch() is called, switching the memory context as/if required, and the CPU register file to NEXT\u0026rsquo;s, saving PREV\u0026rsquo;s in the same move.\n NEXT resumes from its latest switching point, which may be:\n the switch tail code in core_schedule(), if NEXT was running out-of-band prior to sleeping.\n the switch tail code of schedule() if NEXT is completing an out-of-band switch.\n   The event notifier Once dovetail_start_altsched() has been called for a Linux task, it may receive events of interest with respect to running under the supervision of an autonomous core. Those events are delivered by invoking __weak handlers which should by overriden by this core.\nOut-of-band exception handling If a processor exception is raised while the CPU is busy running a task on the out-of-band stage (e.g. due to some invalid memory access, bad instruction, FPU or alignment error etc.), the task has to leave such context before it may run the in-band fault handler. Dovetail notifies the core about incoming exceptions early from the low-level fault trampolines, but only when some out-of-band code was running when the exception was taken. The core may then fix up the current context, such as switching to the in-band execution stage.\nEnabling debuggers to trace tasks running on the out-of-band stage involves dealing with debug traps ptrace() may poke into the debuggee\u0026rsquo;s code for breakpointing.\n The notification is delivered to the handle_oob_trap() handler the core should override for receiving those events (__weak binding). handle_oob_trap() is passed the exception code as defined in arch/*/include/asm/dovetail.h, and a pointer to the register frame of the faulting context (struct pt_regs).\nInterrupts are disabled in the CPU when this handler is called.\nSystem calls The autonomous core is likely to introduce its own set of system calls application tasks may invoke. From the standpoint of the main kernel, this is a foreign set of calls, which can be distinguished unambiguously from regular ones.\nIf a task attached to the core issues any system call, regardless of which of the kernel or the core should handle it, the latter must be given the opportunity to:\n handle the request directly, possibly switching the caller to out-of-band context first if required.\n pass the request downward to the normal system call path on the in-band stage, possibly switching the caller to in-band context if needed.\n  Dovetail intercepts system calls early in the kernel entry code, delivering them to one of these handlers the core should override:\n the call is delivered to the handle_oob_syscall() handler if the system call number is not in the valid range for the in-band kernel - i.e. it has to belong to the core instead -, and the caller issued the request from the out-of-band context. This is the fast path, when a task running out-of-band is requesting a service the core provides.\n handle_pipelined_syscall() is a slower path, in which this handler is probed for the appropriate execution stage in order to handle the request. In this case, the calling logic is as follows:\n  graph LR; S(\"from out-of-band IRQ stage\") -- A style S fill:#ff950e; A[\"handle_pipelined_syscall()\"] -- B{returned zero?} B --|Yes| C{on in-band stage?} B --|No| R(done) C --|Yes| Q(handle as regular kernel syscall) style Q fill:#99ccff; C --|No| P[switch to in-band IRQ stage] style P fill:#ff950e; P -- A  In the flowchart above, handle_pipelined_syscall() should return zero if it wants Dovetail to propagate the unhandled system call down the pipeline, non-zero if the request was handled. The propagation is carried out:\n first by re-running handle_pipelined_syscall() in the in-band context coming from the out-of-band one.\n then by passing the syscall to the regular in-band system call handler in the assembly entry code eventually, if it remained unhandled by handle_pipelined_syscall().\n  Interrupts are enabled in the CPU when any of these handlers is called.\nThe core may need to switch the calling task to the converse execution stage (i.e. in-band \u0026lt;-\u0026gt; out-of-band) either from the handle_oob_syscall() or handle_pipelined_syscall() handlers, this is fine. Dovetail would notice and reconcile its logic according to the current stage on return of these handlers.\n In-band events The last set of notifications involves pure in-band events which the autonomous core may need to know about, as they may affect its own task management. Except for INBAND_PROCESS_CLEANUP which is called for any exiting user-space task, all other notifications are only issued for tasks bound to the core (which may involve kthreads).\nThe notification is delivered to the handle_inband_event() handler. Interrupts are enabled in the CPU when this handler is called.\nThe notification handler is given the event type code, and a single pointer argument which relates to the event type.\nThe following events are defined (see include/linux/dovetail.h):\n INBAND_TASK_SCHEDULE(struct task_struct *next)  sent in preparation of a context switch, right before the memory context is switched to next.\n INBAND_TASK_SIGNAL(struct task_struct *target)  sent when target is about to receive a signal. The core may decide to schedule a transition of the recipient to the in-band stage in order to have it handle that signal asap, which is required for keeping the kernel sane. This notification is always sent from the context of the issuer.\n INBAND_TASK_MIGRATION(struct dovetail_migration_data *p)  sent when p-\u0026gt;task is about to move to CPU p-\u0026gt;dest_cpu.\n INBAND_TASK_EXIT(struct task_struct *current)  sent from do_exit() before the current task has dropped the files and mappings it owns.\n INBAND_PROCESS_CLEANUP(struct mm_struct *mm)  sent before mm is entirely dropped, before the mappings are exited. Per-process resources which might be maintained by the core could be released there, as all tasks have exited.\nAlternate task context Your autonomous core will need to keep its own set of per-task data, starting with the alternate scheduling context block. To help in maintaining such information on a per-task, per-architecture basis, Dovetail adds the struct oob_thread_state member to the (stack-based) thread information block (aka struct thread_info) each kernel architecture port defines.\nYou may want to fill that structure reserved to out-of-band support with any information your core may need for maintaining a task context. The core may then retrieve the address of the structure by calling dovetail_current_state(). For instance, this is the definition the EVL core has for the oob_thread_state structure, storing a backpointer to its own task control block, along with a core-specific preemption count for fast stack-based access:\nstruct evl_thread; struct oob_thread_state { struct evl_thread *thread; int preempt_count; };  Which gives the following chain:\ngraph LR; T(\"struct thread_info\") -- |includes| t t(\"struct oob_thread_state\") -.- |refers to| c c(\"struct evl_thread\")  By having this information defined in a file accessible from the architecture-specific code by including \u0026lt;dovetail/thread_info.h\u0026gt;, the core-specific structure is automatically added to struct thread_info as required. For instance,\n arch/arm/include/dovetail/thread_info.h\nstruct oob_thread_state { /* Define your core-specific data here. */ };    struct oob_thread_state *dovetail_current_state(void)  This call retrieves the address of the out-of-band data structure for the current task, which is always a valid pointer. The content of this structure is zeroed when the task is created, and stays so until your autonomous core initializes it.\n"
},
{
	"uri": "https://evenless.org/dovetail/pipeline/porting/atomic/",
	"title": "Atomic operations",
	"tags": [],
	"description": "",
	"content": "The effect of virtualizing interrupt protection must be reversed for atomic helpers everywhere interrupt disabling is needed to serialize callers, regardless of the stage they live on. Typically, the following files are concerned:\n include/asm-generic/atomic.h include/asm-generic/cmpxchg-local.h include/asm-generic/cmpxchg.h  Likewise in the architecture-specific code:\narch/arm/include/asm/atomic.h arch/arm/include/asm/bitops.h arch/arm/include/asm/cmpxchg.h\nThis is required to keep those helpers usable on data which might be accessed from both stages. A common way to revert such virtualization involves substituting calls to the - virtualized - local_irq_save(), local_irq_restore() API with their hard, non-virtualized counterparts.\n Restoring strict serialization for operations on generic atomic counters\n --- a/include/asm-generic/atomic.h +++ b/include/asm-generic/atomic.h @@ -80,9 +80,9 @@ static inline void atomic_##op(int i, atomic_t *v)\t\\ {\t\\ unsigned long flags;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\ v-\u0026gt;counter = v-\u0026gt;counter c_op i;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\ } #define ATOMIC_OP_RETURN(op, c_op)\t\\ @@ -91,9 +91,9 @@ static inline int atomic_##op##_return(int i, atomic_t *v)\t\\ unsigned long flags;\t\\ int ret;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\ ret = (v-\u0026gt;counter = v-\u0026gt;counter c_op i);\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\ \\ return ret;\t\\ } @@ -104,10 +104,10 @@ static inline int atomic_fetch_##op(int i, atomic_t *v)\t\\ unsigned long flags;\t\\ int ret;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\ ret = v-\u0026gt;counter;\t\\ v-\u0026gt;counter = v-\u0026gt;counter c_op i;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\ \\ return ret;\t\\ }  Likewise, such operations may exist in architecture-specific code, overriding their generic definitions. For instance, the ARM port defines its own version of atomic operations for which real interrupt protection has to be reinstated:\n Restoring strict serialization for operations on atomic counters for ARM\n --- a/arch/arm/include/asm/atomic.h +++ b/arch/arm/include/asm/atomic.h @@ -168,9 +168,9 @@ static inline void atomic_##op(int i, atomic_t *v)\t\\ {\t\\ unsigned long flags;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\ v-\u0026gt;counter c_op i;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\ }\t\\ #define ATOMIC_OP_RETURN(op, c_op, asm_op)\t\\ @@ -179,10 +179,10 @@ static inline int atomic_##op##_return(int i, atomic_t *v)\t\\ unsigned long flags;\t\\ int val;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\ v-\u0026gt;counter c_op i;\t\\ val = v-\u0026gt;counter;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\ \\ return val;\t\\ } @@ -193,10 +193,10 @@ static inline int atomic_fetch_##op(int i, atomic_t *v)\t\\ unsigned long flags;\t\\ int val;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\ val = v-\u0026gt;counter;\t\\ v-\u0026gt;counter c_op i;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\ \\ return val;\t\\ } @@ -206,11 +206,11 @@ static inline int atomic_cmpxchg(atomic_t *v, int old, int new) int ret; unsigned long flags; -\traw_local_irq_save(flags); +\tflags = hard_local_irq_save(); ret = v-\u0026gt;counter; if (likely(ret == old)) v-\u0026gt;counter = new; -\traw_local_irq_restore(flags); +\thard_local_irq_restore(flags); return ret; }  "
},
{
	"uri": "https://evenless.org/core/user-api/condvar/",
	"title": "Condition variable",
	"tags": [],
	"description": "",
	"content": " EVL\u0026rsquo;s condition variable behave like the POSIX one for the most part. It can be used for synchronizing EVL threads on the state of some shared information in a raceless way. Proper serialization is enforced by pairing a condition variable with a mutex.\nint evl_new_condvar(struct evl_condvar *cv, int clockfd, const char *fmt, \u0026hellip;) int evl_open_condvar(struct evl_condvar *cv, const char *fmt, \u0026hellip;) int evl_wait(struct evl_condvar *cv, struct evl_mutex *mutex) int evl_timedwait(struct evl_condvar *cv, struct evl_mutex *mutex, const struct timespec *timeout) int evl_signal(struct evl_condvar *cv) int evl_signal_thread(struct evl_condvar *cv, int thrfd) int evl_broadcast(struct evl_condvar *cv) int evl_close_condvar(struct evl_condvar *cv) EVL_CONDVAR_INITIALIZER(__name, __clock) "
},
{
	"uri": "https://evenless.org/core/kernel-api/",
	"title": "Driver interface",
	"tags": [],
	"description": "",
	"content": "EVL heavily relies on Dovetail for its seamless integration into the main kernel. Among other things, the latter extends the set of file operations which can be supported by file descriptions, so that they can handle I/O operations from the out-of-band stage upon request from an autonomous core such as EVL, while retaining the common structure of a character device driver. Applications running in user-space can start these I/O operations, by issuing specific system calls to EVL.\n Excerpt from include/linux/fs.h, as modified by Dovetail\nstruct file_operations { ... ssize_t (*oob_read) (struct file *, char __user *, size_t); ssize_t (*oob_write) (struct file *, const char __user *, size_t); long (*oob_ioctl) (struct file *, unsigned int, unsigned long); __poll_t (*oob_poll) (struct file *, struct oob_poll_wait *); ... } __randomize_layout;   Therefore, EVL drivers are common character device drivers, which only need to implement some out-of-band handlers for performing real-time I/O operations upon request from the EVL core. In other words, there is no such thing as an EVL driver model, because EVL fits into the regular Linux driver model.\nDovetail is in charge of routing the system calls received from applications to the proper recipient kernel, either EVL or the main one. As mentioned earlier when describing the everything-is-a-file mantra EVL implements, only I/O transfer and control requests have to run from the out-of-band context (i.e. EVL\u0026rsquo;s real-time mode), establishing and managing the underlying file channel from the regular in-band context is just fine. Therefore, the execution flow upon I/O system calls looks like this:\nWhich translates as follows:\n When an application issues the open(2) or close(2) system call to an EVL driver, the request goes through the virtual filesystem switch (VFS) as usual, ending up into the .open() and .release() handlers (when the last reference is dropped) defined by the driver in its struct file_operations descriptor. The same goes for mmap(2), ioctl(2), read(2), write(2), and all other common file operations for character device drivers.\n When an applications issues a oob_read(), oob_write() or oob_ioctl() system call via the EVL library, Dovetail routes the call to the EVL core (instead of the VFS), which in turn fires the corresponding handlers defined by the driver\u0026rsquo;s struct file_operations descriptor: i.e. .oob_read(), .oob_write(), and .oob_ioctl(). Those handlers should use the EVL kernel API, or the main kernel services which are known to be usable from the out-of-band-context, exclusively. Failing to abide by this golden rule may lead to funky outcomes ranging from plain kernel crashes (lucky) to rampant memory corruption (unlucky).\n  Now, you may wonder: \u0026ldquo;what if an out-of-band operation is ongoing in the driver on a particular file, while I\u0026rsquo;m closing the last reference to that file?\u0026rdquo; Well, the close(2) call will block until the out-of-band operation finishes, at which point the .release() handler may proceed to dismantling the file. A simple rule for writing EVL drivers ensures this.\n "
},
{
	"uri": "https://evenless.org/todo/",
	"title": "TODO list",
	"tags": [],
	"description": "",
	"content": " EVL\u0026rsquo;s /sysfs interface needs work A choice was made for the EVL core to rely exclusively on /sysfs for exporting the core state information, excluding /procfs entirely. After all, we have only a handful of element types there, each instance is backed by a regular kernel device, so using /sysfs attributes to export information about them to user-space is just the obvious thing to do.\nThis said, some work remains to review the attributes already defined by the EVL core, improve and possibly add more of them. This work is most likely to be paired with the following task about writing evl status commands.\nMissing attributes which come to mind:\n thread/\u0026lt;pid\u0026gt;/wchan, which would dump the stack backtrace of the corresponding thread if it currently sleeps on an EVL wait channel (struct evl_wait_channel).  Also, the sysfs element nodes EVL creates appear directly at the top of the /sys/class hierarchy (e.g. /sys/class/thread), which is clumsy. sysfs does not formally support sub-classes, so I see no obvious way for having all element classes rooted at /sys/class/evl instead. Any idea would be welcome.\nWhat the heck is the core running? As a matter of fact, we have absolutely no handy tool for inspecting the current runtime status of the EVL core. A ps-like command is direly needed for instance, that would read the /sysfs attributes exported by the currently running threads to send out some useful information.\nThe code for an umbrella command named evl is already there in libevl/commands/evl.c, which like its git counterpart, serves as a centralized command router for executing ancillary scripts or binaries. Such a script available from the same directory called evl-trace illustrates the interface logic with the evl command.\n poll is naive with deep nesting The file descriptor polling feature is represented by a file descriptor in userland too, so a polling object may be used to poll other polling objects and so on. The core should make sure not to accept too deeply nested monitoring requests. The case of cyclic graphs is detected ok, but non-cyclic deep nesting is definitely not as the algorithm there is pretty naive. linux-evl/kernel/evl/poll.c is where to look for this.\nlibevl needs more tests There are some unit tests exercising the EVL core in the library, but we need many more:\n error injection is mostly absent from those tests.\n there is no test verifying the sanity of the condition variable implementation.\n the file descriptor polling feature is only lightly tested, so is the cross-buffer element.\n  THE big issue: phase out the ugly big lock This lock was inherited from the Xenomai code base (aka nklock there). This is a massive bottleneck on the road to efficient SMP scalability with more than 6 CPU cores running real-time load.\nSome significant preparatory work has taken place in the EVL core for taking this lock out:\n we don\u0026rsquo;t synchronize the timer events on it anymore, every CPU now has its own time base lock, which improves serialization.\n the scheduler core has been simplified significantly, reducing the scope of the code which still requires to be covered by the ugly big lock.\n  However, we are not there yet. Some complex stuff remains to be sorted out for eliminating this lock entirely, specifically in the thread-to-scheduler interface points.\nImproving the tracepoints The EVL core defines a number of FTRACE-based tracepoints, which for the most part have been inherited from Xenomai\u0026rsquo;s Cobalt core, there is root for improvement in order to better fit the EVL context. In addition, the new code represents 2/3rd of the code base, which is not instrumented at all.\nDisconnect Dovetail\u0026rsquo;s hard and mutable locks from CONFIG_LOCKDEP If we could exclude Dovetail\u0026rsquo;s spinlocks from CONFIG_LOCKDEP using a debug option, we could run the lock validator for detecting glitches with in-band locking exclusively, without causing massive latency spots.\nEnable a variant of Xenomai\u0026rsquo;s slackspot utility for EVL Xenomai 3 introduced a utility called slackspot for tracing back code which caused the application to switch to the in-band stage.\nWe need an equivalent, passing the trace data back to userland via debugfs.\n"
},
{
	"uri": "https://evenless.org/dovetail/rulesofthumb/",
	"title": "Rules Of Thumb",
	"tags": [],
	"description": "",
	"content": " Turn on debug options in the kernel configuration! During the development phase, do yourself a favour: turn on CONFIG_DEBUG_IRQ_PIPELINE and CONFIG_DEBUG_DOVETAIL.\nThe first one will catch many nasty issues, such as calling unsafe in-band code from out-of-band context. The second one checks the integrity of the alternate scheduling support, detecting issues in the architecture port.\nThe runtime overhead induced by enabling these options is marginal. Just don\u0026rsquo;t port Dovetail or implement out-of-band client code without them enabled in your target kernel, seriously.\nSerialize stages when accessing shared data If some writable data is shared between in-band and out-of-band code, you have to guard against out-of-band code preempting or racing with the in-band code which accesses the same data. This is required to prevent dirty reads and dirty writes:\n one the same CPU, by disabling interrupts in the CPU.\n from different CPUs, by using hard or mutable spinlocks.\n  Check that the pipeline torture tests pass Before any consideration is made to implement out-of-band code on a platform, check that interrupt pipelining is sane there, by enabling CONFIG_IRQ_PIPELINE_TORTURE_TEST in the configuration. As its name suggests, this option enables test code which exercizes the interrupt pipeline core, and related features such as the proxy tick device.\nSince the torture tests need to enable the out-of-band stage for their own purpose, you may have to disable any Dovetail-based autonomous core in the kernel configuration for running those tests, like switching off _CONFIGEVL if the EVL core is enabled.\n When those tests pass, the following output should appear in the kernel log:\nStarting IRQ pipeline tests... IRQ pipeline: high-priority torture stage added. irq_pipeline-torture: CPU0 initiates stop_machine() irq_pipeline-torture: CPU3 responds to stop_machine() irq_pipeline-torture: CPU1 responds to stop_machine() irq_pipeline-torture: CPU2 responds to stop_machine() irq_pipeline-torture: CPU1: proxy tick registered (62.50MHz) irq_pipeline-torture: CPU2: proxy tick registered (62.50MHz) irq_pipeline-torture: CPU3: proxy tick registered (62.50MHz) irq_pipeline-torture: CPU0: proxy tick registered (62.50MHz) irq_pipeline-torture: CPU0: irq_work handled irq_pipeline-torture: CPU0: in-band-\u0026gt;in-band irq_work trigger works irq_pipeline-torture: CPU0: stage escalation request works irq_pipeline-torture: CPU0: irq_work handled irq_pipeline-torture: CPU0: oob-\u0026gt;in-band irq_work trigger works IRQ pipeline: torture stage removed. IRQ pipeline tests OK.  Otherwise, if you observe any issue when running any of those tests, then the IRQ pipeline definitely needs fixing.\nKnow how to differentiate safe from unsafe in-band code Not all in-band kernel code is safe to be called from out-of-band context, actually most of it is unsafe for doing so.\nA code is deemed safe in this respect when you are 101% sure that it never does, directly or indirectly, any of the following:\n attempts to reschedule in-band wise, meaning that schedule() would end up being called. The rule of thumb is that any section of code traversing the might_sleep() check cannot be called from out-of-band context.\n takes a spinlock from any regular type like raw_spinlock_t or spinlock_t. The former would affect the virtual interrupt disable flag which is invalid outside of the in-band context, the latter might reschedule if CONFIG_PREEMPT is enabled.\n  In the early days of dual kernel support in Linux, some people would mistakenly invoke the do_gettimeofday() routine from an out-of-band context in order to get a wallclock timestamp for their real-time code. Doing so would create a deadlock situation if some in-band code running do_gettimeofday() is preempted by the out-of-band code re-entering the same routine on the same CPU. The out-of-band code would then wait spinning indefinitely for the in-band context to leave the routine - which won\u0026rsquo;t happen by design - leading to a lockup. Nowadays, enabling CONFIG_DEBUG_IRQ_PIPELINE would be enough to detect such mistake early enough to preserve your mental health.\n Careful with disabling interrupts in the CPU When pipelining is enabled, use hard interrupt protection with caution, especially from in-band code. Not only this might send latency figures over the top, but this might even cause random lockups would a rescheduling happen while interrupts are hard disabled.\nDealing with spinlocks Converting regular kernel spinlocks (e.g. spinlock_t, raw_spin_lock_t) to hard spinlocks should involve a careful review of every section covered by such lock. Any such section would then inherit the following requirements:\n no unsafe in-band kernel service should be called within the section.\n the section covered by the lock should be short enough to keep interrupt latency low.\n  Enable RAW_PRINTK support for printk-like debugging Unless you are lucky enough to have an ICE for debugging hard issues involving out-of-band contexts, you might have to resort to basic printk-style debugging over a serial line. Although the printk() machinery can be used from out-of-band context when Dovetail is enabled, you should rather use the raw_printk() interface for this.\n"
},
{
	"uri": "https://evenless.org/dovetail/pipeline/porting/arch/",
	"title": "Architecture-specific bits",
	"tags": [],
	"description": "",
	"content": " Interrupt mask virtualization The architecture-specific code which manipulates the interrupt flag in the CPU\u0026rsquo;s state register in arch//include/asm/irqflags.h should be split between real and virtual interrupt control. The real interrupt control operations are inherited from the in-band kernel implementation. The virtual ones should be built upon services provided by the interrupt pipeline core.\n firstly, the original arch_local_* helpers should be renamed as native_* helpers, affecting the hardware interrupt state in the CPU. This naming convention is imposed on the architecture code by the generic helpers in include/asm-generic/irq_pipeline.h.   Example: introducing the native interrupt state accessors for the ARM architecture\n --- a/arch/arm/include/asm/irqflags.h +++ b/arch/arm/include/asm/irqflags.h #if __LINUX_ARM_ARCH__ \u0026gt;= 6 #define arch_local_irq_save arch_local_irq_save -static inline unsigned long arch_local_irq_save(void) +static inline unsigned long native_irq_save(void) { unsigned long flags; asm volatile( -\t\u0026quot;\tmrs\t%0, \u0026quot; IRQMASK_REG_NAME_R \u0026quot;\t@ arch_local_irq_save\\n\u0026quot; +\t\u0026quot;\tmrs\t%0, \u0026quot; IRQMASK_REG_NAME_R \u0026quot;\t@ native_irq_save\\n\u0026quot; \u0026quot;\tcpsid\ti\u0026quot; : \u0026quot;=r\u0026quot; (flags) : : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;); return flags; } #define arch_local_irq_enable arch_local_irq_enable -static inline void arch_local_irq_enable(void) +static inline void native_irq_enable(void) { asm volatile( -\t\u0026quot;\tcpsie i\t@ arch_local_irq_enable\u0026quot; +\t\u0026quot;\tcpsie i\t@ native_irq_enable\u0026quot; : : : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;); } #define arch_local_irq_disable arch_local_irq_disable -static inline void arch_local_irq_disable(void) +static inline void native_irq_disable(void) { asm volatile( -\t\u0026quot;\tcpsid i\t@ arch_local_irq_disable\u0026quot; +\t\u0026quot;\tcpsid i\t@ native_irq_disable\u0026quot; : : : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;); @@ -69,12 +76,12 @@ static inline void arch_local_irq_disable(void) * Save the current interrupt enable state \u0026amp; disable IRQs */ #define arch_local_irq_save arch_local_irq_save -static inline unsigned long arch_local_irq_save(void) +static inline unsigned long native_irq_save(void) { unsigned long flags, temp; asm volatile( -\t\u0026quot;\tmrs\t%0, cpsr\t@ arch_local_irq_save\\n\u0026quot; +\t\u0026quot;\tmrs\t%0, cpsr\t@ native_irq_save\\n\u0026quot; \u0026quot;\torr\t%1, %0, #128\\n\u0026quot; \u0026quot;\tmsr\tcpsr_c, %1\u0026quot; : \u0026quot;=r\u0026quot; (flags), \u0026quot;=r\u0026quot; (temp) @@ -87,11 +94,11 @@ static inline unsigned long arch_local_irq_save(void) * Enable IRQs */ #define arch_local_irq_enable arch_local_irq_enable -static inline void arch_local_irq_enable(void) +static inline void native_irq_enable(void) { unsigned long temp; asm volatile( -\t\u0026quot;\tmrs\t%0, cpsr\t@ arch_local_irq_enable\\n\u0026quot; +\t\u0026quot;\tmrs\t%0, cpsr\t@ native_irq_enable\\n\u0026quot; \u0026quot;\tbic\t%0, %0, #128\\n\u0026quot; \u0026quot;\tmsr\tcpsr_c, %0\u0026quot; : \u0026quot;=r\u0026quot; (temp) @@ -103,11 +110,11 @@ static inline void arch_local_irq_enable(void) * Disable IRQs */ #define arch_local_irq_disable arch_local_irq_disable -static inline void arch_local_irq_disable(void) +static inline void native_irq_disable(void) { unsigned long temp; asm volatile( -\t\u0026quot;\tmrs\t%0, cpsr\t@ arch_local_irq_disable\\n\u0026quot; +\t\u0026quot;\tmrs\t%0, cpsr\t@ native_irq_disable\\n\u0026quot; \u0026quot;\torr\t%0, %0, #128\\n\u0026quot; \u0026quot;\tmsr\tcpsr_c, %0\u0026quot; : \u0026quot;=r\u0026quot; (temp) @@ -153,11 +160,11 @@ static inline void arch_local_irq_disable(void) * Save the current interrupt enable state. */ #define arch_local_save_flags arch_local_save_flags -static inline unsigned long arch_local_save_flags(void) +static inline unsigned long native_save_flags(void) { unsigned long flags; asm volatile( -\t\u0026quot;\tmrs\t%0, \u0026quot; IRQMASK_REG_NAME_R \u0026quot;\t@ local_save_flags\u0026quot; +\t\u0026quot;\tmrs\t%0, \u0026quot; IRQMASK_REG_NAME_R \u0026quot;\t@ native_save_flags\u0026quot; : \u0026quot;=r\u0026quot; (flags) : : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;); return flags; } @@ -166,21 +173,28 @@ static inline unsigned long arch_local_save_flags(void) * restore saved IRQ \u0026amp; FIQ state */ #define arch_local_irq_restore arch_local_irq_restore -static inline void arch_local_irq_restore(unsigned long flags) +static inline void native_irq_restore(unsigned long flags) { asm volatile( -\t\u0026quot;\tmsr\t\u0026quot; IRQMASK_REG_NAME_W \u0026quot;, %0\t@ local_irq_restore\u0026quot; +\t\u0026quot;\tmsr\t\u0026quot; IRQMASK_REG_NAME_W \u0026quot;, %0\t@ native_irq_restore\u0026quot; : : \u0026quot;r\u0026quot; (flags) : \u0026quot;memory\u0026quot;, \u0026quot;cc\u0026quot;); } #define arch_irqs_disabled_flags arch_irqs_disabled_flags -static inline int arch_irqs_disabled_flags(unsigned long flags) +static inline int native_irqs_disabled_flags(unsigned long flags) { return flags \u0026amp; IRQMASK_I_BIT; } +static inline bool native_irqs_disabled(void) +{ +\tunsigned long flags = native_save_flags(); +\treturn native_irqs_disabled_flags(flags); +} + +#include \u0026lt;asm/irq_pipeline.h\u0026gt; #include \u0026lt;asm-generic/irqflags.h\u0026gt; #endif /* ifdef __KERNEL__ */   finally, a new set of arch_local_* helpers should be provided, affecting the virtual interrupt disable flag implemented by the pipeline core for controlling the in-band stage protection against interrupts. It is good practice to implement this set in a separate file available for inclusion from \u0026lt;asm/irq_pipeline.h\u0026gt;.   Example: providing the virtual interrupt state accessors for the ARM architecture\n--- /dev/null +++ b/arch/arm/include/asm/irq_pipeline.h @@ -0,0 +1,138 @@ +#ifndef _ASM_ARM_IRQ_PIPELINE_H +#define _ASM_ARM_IRQ_PIPELINE_H + +#include \u0026lt;asm-generic/irq_pipeline.h\u0026gt; + +#ifdef CONFIG_IRQ_PIPELINE + +static inline notrace unsigned long arch_local_irq_save(void) +{ +\tint stalled = inband_irq_save(); +\tbarrier(); +\treturn arch_irqs_virtual_to_native_flags(stalled); +} + +static inline notrace void arch_local_irq_enable(void) +{ +\tbarrier(); +\tinband_irq_enable(); +} + +static inline notrace void arch_local_irq_disable(void) +{ +\tinband_irq_disable(); +\tbarrier(); +} + +static inline notrace unsigned long arch_local_save_flags(void) +{ +\tint stalled = inband_irqs_disabled(); +\tbarrier(); +\treturn arch_irqs_virtual_to_native_flags(stalled); +} + +static inline int arch_irqs_disabled_flags(unsigned long flags) +{ +\treturn native_irqs_disabled_flags(flags); +} + +static inline notrace void arch_local_irq_restore(unsigned long flags) +{ +\tif (!arch_irqs_disabled_flags(flags)) +\t__inband_irq_enable(); +\tbarrier(); +} + +#else /* !CONFIG_IRQ_PIPELINE */ + +static inline unsigned long arch_local_irq_save(void) +{ +\treturn native_irq_save(); +} + +static inline void arch_local_irq_enable(void) +{ +\tnative_irq_enable(); +} + +static inline void arch_local_irq_disable(void) +{ +\tnative_irq_disable(); +} + +static inline unsigned long arch_local_save_flags(void) +{ +\treturn native_save_flags(); +} + +static inline void arch_local_irq_restore(unsigned long flags) +{ +\tnative_irq_restore(flags); +} + +static inline int arch_irqs_disabled_flags(unsigned long flags) +{ +\treturn native_irqs_disabled_flags(flags); +} + +#endif /* !CONFIG_IRQ_PIPELINE */ + +#endif /* _ASM_ARM_IRQ_PIPELINE_H */   This new file should include \u0026lt;asm-generic/irq_pipeline.h\u0026gt; early to get access to the pipeline declarations it needs. This inclusion should be unconditional, even if the kernel is built with CONFIG_IRQ_PIPELINE disabled.\n Providing support for merged interrupt states The generic interrupt pipeline implementation requires the arch-level support code to provide for a pair of helpers aimed at translating the virtual interrupt disable flag to the interrupt bit in the CPU\u0026rsquo;s status register (e.g. PSR_I_BIT for ARM) and conversely. These helpers are used to create combined state words merging the virtual and real interrupt states.\n arch_irqs_virtual_to_native_flags(int stalled) must return a long word remapping the boolean value of @stalled to the CPU\u0026rsquo;s interrupt bit position in the status register. All other bits must be cleared.\n On ARM, this can be expressed as (stalled ? PSR_I_BIT : 0). on x86, that would rather be (stalled ? 0 : X86_EFLAGS_IF).  arch_irqs_native_to_virtual_flags(unsigned long flags) must return a long word remapping the CPU\u0026rsquo;s interrupt bit in @flags to an arbitrary bit position, choosen not to conflict with the former. In other words, the CPU\u0026rsquo;s interrupt state bit received in @flags should be shifted to a free position picked arbitrarily in the return value. All other bits must be cleared.\n On ARM, using bit position 31 to reflect the virtual state, this is expressed as (hard_irqs_disabled_flags(flags) ? (1 \u0026lt;\u0026lt; 31) : 0).\n On any other architecture, the implementation would be similar, using whatever bit position is available which would not conflict with the CPU\u0026rsquo;s interrupt bit position.\n   --- a/arch/arm/include/asm/irqflags.h +++ b/arch/arm/include/asm/irqflags.h /* * CPU interrupt mask handling. */ #ifdef CONFIG_CPU_V7M #define IRQMASK_REG_NAME_R \u0026quot;primask\u0026quot; #define IRQMASK_REG_NAME_W \u0026quot;primask\u0026quot; #define IRQMASK_I_BIT\t1 +#define IRQMASK_I_POS\t0 #else #define IRQMASK_REG_NAME_R \u0026quot;cpsr\u0026quot; #define IRQMASK_REG_NAME_W \u0026quot;cpsr_c\u0026quot; #define IRQMASK_I_BIT\tPSR_I_BIT +#define IRQMASK_I_POS\t7 #endif +#define IRQMASK_i_POS\t31  IRQMASK_i_POS (note the minus \u0026lsquo;i\u0026rsquo;) is the free bit position in the combo word where the ARM port stores the original CPU\u0026rsquo;s interrupt state in the combo word. This position can\u0026rsquo;t conflict with IRQMASK_I_POS, which is an alias to PSR_I_BIT (bit position 0 or 7).\n --- /dev/null +++ b/arch/arm/include/asm/irq_pipeline.h + +static inline notrace +unsigned long arch_irqs_virtual_to_native_flags(int stalled) +{ +\treturn (!!stalled) \u0026lt;\u0026lt; IRQMASK_I_POS; +} +static inline notrace +unsigned long arch_irqs_native_to_virtual_flags(unsigned long flags) +{ +\treturn (!!hard_irqs_disabled_flags(flags)) \u0026lt;\u0026lt; IRQMASK_i_POS; +}  Once all of these changes are in, the generic helpers from  such as local_irq_disable() and local_irq_enable() actually refer to the virtual protection scheme when interrupts are pipelined, which eventually allows to implement interrupt deferral for the protected in-band code running over the in-band stage.\n Adapting the assembly code to IRQ pipelining Interrupt entry As generic IRQ handling is a requirement for supporting Dovetail, the low-level interrupt handler living in the assembly portion of the architecture code can still deliver all interrupt events to the original C handler provided by the irqchip driver. That handler should in turn invoke:\n handle_domain_irq() for parent device IRQs\n generic_handle_irq() for cascaded device IRQs (decoded from the parent handler)\n  For those routines, the initial task of inserting an interrupt at the head of the pipeline is directly handled from the genirq layer they belong to. This means that there is usually not much to do other than making a quick check in the implementation of the parent IRQ handler, in the relevant irqchip driver, applying the rules of thumb carefully.\nOn some ARM platform equipped with a fairly common GIC controller, that would mean inspecting the function gic_handle_irq() for instance.\n  the arch-specific handle_IPI() or equivalent for special inter-processor interrupts  IPIs must be dealt with by specific changes introduced by the port we will cover later.\nInterrupt exit When interrupt pipelining is disabled, the kernel normally runs an epilogue after each interrupt or exception event was handled. If the event happened while the CPU was running some kernel code, the epilogue would check for a potential rescheduling opportunity in case CONFIG_PREEMPT is enabled. If a user-space task was preempted by the event, additional conditions would be checked for such as a signal pending delivery for that task.\nBecause interrupts are only virtually masked for the in-band code when pipelining is enabled, IRQs can still be taken by the CPU and passed on to the low-level assembly handlers, so that they can enter the interrupt pipeline.\n Running the regular epilogue afer an IRQ is valid only if the kernel was actually accepting interrupts when the event happened (i.e. the virtual interrupt disable flag was clear), and running in-band code.\n In all other cases, except for the interrupt pipeline core, the rest of the kernel does not expect those IRQs to ever happen in the first place. Therefore, running the epilogue in such circumstances would be at odds with the kernel\u0026rsquo;s logic. In addition, low-level handlers must have been made aware that they might receive an event under such conditions.\nFor instance, the original ARM code for handling an IRQ which has preempted a kernel context would look like this:\n__irq_svc: svc_entry irq_handler #ifdef CONFIG_PREEMPT ldr\tr8, [tsk, #TI_PREEMPT]\t@ get preempt count ldr\tr0, [tsk, #TI_FLAGS]\t@ get flags teq\tr8, #0\t@ if preempt count != 0 movne\tr0, #0\t@ force flags to 0 tst\tr0, #_TIF_NEED_RESCHED blne\tsvc_preempt #endif  In order to properly handle interrupts in a pipelined delivery model, we have to detect whether the in-band kernel was ready to receive such event, acting upon it accordingly. To this end, the ARM port passes the event to a trampoline routine instead (handle_arch_irq_pipelined()), expecting on return a decision whether or not the epilogue code should run next. In the illustration below, this decision is returned as a boolean status to the caller, non-zero meaning that we may run the epilogue, zero otherwise.\n--- a/arch/arm/kernel/entry-armv.S +++ b/arch/arm/kernel/entry-armv.S .macro\tirq_handler #ifdef CONFIG_MULTI_IRQ_HANDLER -\tldr\tr1, =handle_arch_irq mov\tr0, sp badr\tlr, 9997f +#ifdef CONFIG_IRQ_PIPELINE +\tldr\tr1, =handle_arch_irq_pipelined +\tmov\tpc, r1 +#else\t+\tldr\tr1, =handle_arch_irq ldr\tpc, [r1] -#else +#endif +#elif CONFIG_IRQ_PIPELINE +#error \u0026quot;Legacy IRQ handling not pipelined\u0026quot; +#else\tarch_irq_handler_default #endif 9997: .endm  The trampoline routine added to the original code, first delivers the interrupt to the machine-defined handler, then tells the caller whether the regular epilogue may run for such event.\n--- a/arch/arm/kernel/irq.c +++ b/arch/arm/kernel/irq.c @@ -112,6 +112,15 @@ void __init set_handle_irq(void (*handle_irq)(struct pt_regs *)) } #endif +#ifdef CONFIG_IRQ_PIPELINE +asmlinkage int __exception_irq_entry +handle_arch_irq_pipelined(struct pt_regs *regs) +{ +\thandle_arch_irq(regs); +\treturn running_inband() \u0026amp;\u0026amp; !irqs_disabled(); +} +#endif +  Eventually, the low-level assembly handler receiving the interrupt event is adapted, in order to carry out the earlier decision by handle_arch_irq_pipelined(), skipping the epilogue code if required to.\n--- a/arch/arm/kernel/entry-armv.S +++ b/arch/arm/kernel/entry-armv.S __irq_svc: svc_entry irq_handler +#ifdef CONFIG_IRQ_PIPELINE +\ttst\tr0, r0 +\tbeq\t1f +#endif #ifdef CONFIG_PREEMPT ldr\tr8, [tsk, #TI_PREEMPT]\t@ get preempt count blne\tsvc_preempt #endif +1: svc_exit r5, irq = 1\t@ return from exception  Taking the fast exit path when applicable is critical to the stability of the target system to prevent invalid re-entry of the in-band kernel code.\n Fault exit Similarly to the interrupt exit case, the low-level fault handling code must skip the epilogue code when the fault was taken over an out-of-band context. Upon fault, the current interrupt state is not considered for determining whether we should run the epilogue, since a fault may occur independently of such state.\n Running the regular epilogue after a fault is valid only if that fault was triggered by some in-band code, excluding any fault raised by out-of-band code.\n For instance, the original ARM code for returning from an exception event would be modified as follows:\n--- a/arch/arm/kernel/entry-armv.S +++ b/arch/arm/kernel/entry-armv.S @@ -754,7 +772,7 @@ ENTRY(ret_from_exception) UNWIND(.cantunwind\t) get_thread_info tsk mov\twhy, #0 -\tb\tret_to_user +\tret_to_user_pipelined r1 UNWIND(.fnend\t)  With the implementation of ret_to_user_pipelined checking for the current stage, skipping the epilogue if the faulting code was running over an out-of-band context:\n--- a/arch/arm/kernel/entry-header.S +++ b/arch/arm/kernel/entry-header.S +/* + * Branch to the exception epilogue, skipping the in-band work + * if running over the oob interrupt stage. + */ +\t.macro ret_to_user_pipelined, tmp +#ifdef CONFIG_IRQ_PIPELINE +\tldr\t\\tmp, [tsk, #TI_LOCAL_FLAGS] +\ttst\t\\tmp, #_TLF_OOB +\tbne\tfast_ret_to_user +#endif +\tb\tret_to_user +\t.endm +  _TLF_OOB is a local thread_info flag denoting a current task running out-of-band code over the oob stage. If set, the epilogue must be skipped.\nReconciling the virtual interrupt state to the epilogue logic A tricky issue to address when pipelining interrupts is about making sure that the logic from the epilogue routine (e.g. do_work_pending(), do_notify_resume()) actually runs in the expected (virtual) interrupt state for the in-band stage.\nReconciling the virtual interrupt state to the in-band logic dealing with interrupts is required because in a pipelined interrupt model, the virtual interrupt state of the in-band stage does not necessarily reflect the CPU\u0026rsquo;s interrupt state on entry to the early assembly code handling the IRQ events. Typically, a CPU would always automatically disable interrupts hardware-wise when taking an IRQ, which may contradict the software-managed virtual state until both are eventually reconciled.\nThose rules of thumb should be kept in mind when adapting the epilogue routine to interrupt pipelining:\n most often, such routine is supposed to be entered with (hard) interrupts off when called from the assembly code which handles kernel entry/exit transitions (e.g. arch/arm/kernel/entry-common.S). Therefore, this routine may have to reconcile the virtual interrupt state with such expectation, since according to the interrupt exit rules we discussed earlier, such state has to be originally enabled (i.e. the in-band stall bit is clear) for the epilogue code to run in the first place.\n conversely, we must keep the hard interrupt state consistent upon return from the epilogue code with the one received on entry. Typically, hard interrupts must be disabled before leaving this code if we entered it that way.\n likewise, we must also keep the virtual interrupt state consistent upon return of the epilogue code with the one received on entry. In other words, the stall bit of the in-band stage must be restored to its original state on entry before leaving this code.\n schedule() must be called with interrupts virtually disabled for the in-band stage, but the CPU\u0026rsquo;s interrupt state should allow for IRQs to be taken in order to minimize latency for the oob stage.\n generally speaking, while we may need the in-band stage to be stalled when the in-band kernel code expects this, we still want most of the epilogue code to run with hard interrupts enabled to shorten the interrupt latency for the oob stage, where autonomous cores live.\n  Dealing with IPIs Support for inter-processor interrupts is architecture-specific code.\n"
},
{
	"uri": "https://evenless.org/core/build-steps/",
	"title": "Building EVL",
	"tags": [],
	"description": "",
	"content": " Building EVL is a two-step process, which may happen in any order:\n a Linux kernel image featuring the EVL core is built.\n the EVL library (aka libevl), basic utilities and test programs are generated.\n  Prerequisites We need:\n a GCC toolchain for the target CPU architecture.\n the UAPI headers from the target Linux kernel fit with the EVL core. In other words, we need access to the contents of include/uapi/asm/ and include/uapi/evl/ from a source kernel tree which contains the EVL core code.\n  libevl relies on thread-local storage support (TLS), which might be broken in some obsolete toolchains.\n Building the core The kernel source tree which includes the latest EVL core is maintained at git://git.evenless.org/linux-evl. The development branch is evl/master.\nOnce your favorite kernel configuration tool is brought up, you should see the EVL configuration block somewhere inside the General setup menu. This configuration block looks like this:\nEnabling CONFIG_EVL should be enough to get you started, the default values for other EVL settings are safe to use. You should make sure to have CONFIG_EVL_LATMUS and CONFIG_EVL_HECTIC enabled too; those are drivers required for running the latmus and hectic utilities available with libevl, which measure latency and validate the context switching sanity.\nIf you are unfamiliar with building kernels, this document may help.\n Building libevl Build command The generic command for building libevl is:\n$ make [-C $SRCDIR] [ARCH=$cpu_arch] [CROSS_COMPILE=$toolchain] UAPI=$uapi_dir [OTHER_BUILD_VARS] [goal...]   Main build variables\n    Variable Description     $SRCDIR Path to this source tree   $cpu_arch CPU architecture you build for (\u0026lsquo;arm\u0026rsquo;, \u0026lsquo;arm64\u0026rsquo;)   $toolchain Optional prefix of the binutils filename (e.g. \u0026lsquo;aarch64-linux-gnu-\u0026rsquo;, \u0026lsquo;arm-linux-gnueabihf-\u0026rsquo;)     Other build variables\n    Variable Description Default     D={0|1} Disable or enable debug build, i.e. -g -O0 vs -O2 0   O=$output_dir Generate binary output files into $output_dir .   V={0|1} Set build verbosity level, 0 is terse 0   DESTDIR=$install_dir Install library and binaries into $install_dir /usr/evl     Make goals\n    Goal Action     all generate all binaries (library, utilities and tests)   clean remove the build files   install do all, copying the generated binaries to $DESTDIR in the process    Examples Let\u0026rsquo;s say the library source code is located at ~/git/libevl, and the kernel sources featuring the EVL core is located at ~/git/linux-evl. Building and installing the EVL library and utilities directly to a staging directory at /nfsroot/\u0026lt;machine\u0026gt;/usr/evl would amount to:\n Building from a (temporary) build directory\n $ mkdir /tmp/build-imx6q \u0026amp;\u0026amp; cd /tmp/build-imx6q $ make -C ~/git/libevl O=$PWD ARCH=arm CROSS_COMPILE=arm-linux-gnueabihf- UAPI=~/git/linux-evl DESTDIR=/nfsroot/imx6q/usr/evl install  or,\n Building directly into the EVL library source tree\n $ mkdir /tmp/build-hikey $ cd ~/git/libevl $ make O=/tmp/build-hikey ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- UAPI=~/git/linux-evl DESTDIR=/nfsroot/hikey/usr/evl install  "
},
{
	"uri": "https://evenless.org/core/user-api/semaphore/",
	"title": "Counting semaphore",
	"tags": [],
	"description": "",
	"content": " int evl_new_sem(struct evl_sem *sem, int clockfd, int initval, const char *fmt, \u0026hellip;) int evl_open_sem(struct evl_sem *sem, const char *fmt, \u0026hellip;) int evl_close_sem(struct evl_sem *sem) int evl_get(struct evl_sem *sem) int evl_timedget(struct evl_sem *sem, const struct timespec *timeout) int evl_put(struct evl_sem *sem) int evl_tryget(struct evl_sem *sem) int evl_getval(struct evl_sem *sem) EVL_SEM_INITIALIZER(__name, __clockfd, __initval) "
},
{
	"uri": "https://evenless.org/core/kernel-api/semaphore/",
	"title": "Kernel semaphores",
	"tags": [],
	"description": "",
	"content": " void evl_init_ksem(struct evl_ksem *ksem, unsigned int value) void evl_destroy_ksem(struct evl_ksem *ksem) int evl_down_timeout(struct evl_ksem *ksem, ktime_t timeout) int evl_down(struct evl_ksem *ksem) int evl_trydown(struct evl_ksem *ksem) void evl_up(struct evl_ksem *ksem) "
},
{
	"uri": "https://evenless.org/ports/",
	"title": "Status of EVL ports",
	"tags": [],
	"description": "",
	"content": "To get EVL running on a platform, we need the following software to be ported in the following sequence:\n the Dovetail interface. This task is composed of two incremental milestones: first getting the interrupt pipeline to work, then enabling the alternate scheduling. Porting Dovetail is where most of the work takes place, the other porting tasks are comparatively quite simple.\n the EVL core. It is mostly composed of architecture-independent code, only a few bits need to be ported (a FPU test helper for the most part).\n the EVL library. Likewise, this code has very little dependencies on the underlying CPU architecture and platform. A port boils down to resolving the address of the clock_gettime() helper in the vDSO basically.\n  The table below summarizes the current status of the existing ports.\n Current target kernel release\n Linux 5.1-rc2\n ARM64 SoC\n  SoC (Board) IRQ pipeline 1 Alternate task control EVL base2 EVL stress3 Test kernel   DragonBoard 410c     5.0   BCM2837 (Raspberry 3 Model B)     5.0   Kirin 620 (HiKey LeMaker)     5.0   virt (QEMU)     5.1-rc2    ARM SoC\n  SoC (Board) IRQ pipeline 1 Alternate task control EVL base2 EVL stress3 Test kernel   i.MX6qp (SabreSD)     5.0   i.MX7D (SabreSD)     5.0   Cannes2-STiH410 (B2260)     5.0   Cyclone V SoC FPGA (DevKit)     5.0   \n1 Means that the pipeline torture tests pass (see CONFIG_IRQ_PIPELINE_TORTURE_TEST). This milestone guarantees that we can deliver high-priority interrupt events immediately to a guest core, regardless of the work ongoing for the main kernel.\n2 When this box is checked, EVL\u0026rsquo;s basic functional test suite runs properly on the platform, which is a good starting point. So far so good.\n3 When this box is checked, the EVL core passes a massive stress test involving the hectic and latmus applications running in parallel for 24 hrs, all glitchlessly. This denotes a reliable state, including flawless alternate scheduling of threads between the main kernel and EVL. On the contrary, a problem with sharing the FPU unit properly between the in-band and out-of-band execution contexts is most often the reason for keeping this box unchecked until the situation is fixed.\n"
},
{
	"uri": "https://evenless.org/dovetail/pipeline/porting/timer/",
	"title": "Timer management",
	"tags": [],
	"description": "",
	"content": " Proxy tick device The proxy tick device is a synthetic clock event device for handing over the control of the hardware tick device to a high-precision, out-of-band timing logic, which cannot be delayed by the in-band kernel code. With this proxy in place, any out-of-band code can gain control over the timer hardware for carrying out its own timing duties. In the same move, it is required to honor the timing requests received from the in-band timer layer (i.e. hrtimers) since the latter won\u0026rsquo;t be able to program timer events directly into the hardware while the proxy is active.\nIn other words, the proxy tick device shares the functionality of the actual device between the in-band and out-of-band contexts, with only the latter actually programming the hardware.\nAdapting clock chip devices for proxying The proxy tick device borrows a real clock chip device from the in-band kernel, controlling it under the hood while substituting for the current tick device. Clock chips which may be controlled by the proxy tick device need their drivers to be specifically adapted for such use, as follows:\n clockevents_handle_event() must be substituted to any open-coded invocation of the event handler in the interrupt handler.\n struct clock_event_device::irq must be properly set to the actual IRQ number signaling an event from this device.\n struct clock_event_device::features must include CLOCK_EVT_FEAT_PIPELINE.\n __IRQF_TIMER must be set for the action handler of the timer device interrupt.\n   Adapting the ARM architected timer driver to out-of-band timing\n --- a/drivers/clocksource/arm_arch_timer.c +++ b/drivers/clocksource/arm_arch_timer.c @@ -585,7 +585,7 @@ static __always_inline irqreturn_t timer_handler(const int access, if (ctrl \u0026amp; ARCH_TIMER_CTRL_IT_STAT) { ctrl |= ARCH_TIMER_CTRL_IT_MASK; arch_timer_reg_write(access, ARCH_TIMER_REG_CTRL, ctrl, evt); -\tevt-\u0026gt;event_handler(evt); +\tclockevents_handle_event(evt); return IRQ_HANDLED; } @@ -704,7 +704,7 @@ static int arch_timer_set_next_event_phys_mem(unsigned long evt, static void __arch_timer_setup(unsigned type, struct clock_event_device *clk) { -\tclk-\u0026gt;features = CLOCK_EVT_FEAT_ONESHOT; +\tclk-\u0026gt;features = CLOCK_EVT_FEAT_ONESHOT | CLOCK_EVT_FEAT_PIPELINE; if (type == ARCH_TIMER_TYPE_CP15) { if (arch_timer_c3stop)  Only oneshot-capable clock event devices can be shared via the proxy tick device.\n  Adapting the ARM Global timer driver to out-of-band timing\n --- a/drivers/clocksource/arm_global_timer.c +++ b/drivers/clocksource/arm_global_timer.c @@ -156,11 +156,11 @@ static irqreturn_t gt_clockevent_interrupt(int irq, void *dev_id) *\tthe Global Timer flag _after_ having incremented *\tthe Comparator register\tvalue to a higher value. */ -\tif (clockevent_state_oneshot(evt)) +\tif (clockevent_is_oob(evt) || clockevent_state_oneshot(evt)) gt_compare_set(ULONG_MAX, 0); writel_relaxed(GT_INT_STATUS_EVENT_FLAG, gt_base + GT_INT_STATUS); -\tevt-\u0026gt;event_handler(evt); +\tclockevents_handle_event(evt); return IRQ_HANDLED; } @@ -171,7 +171,7 @@ static int gt_starting_cpu(unsigned int cpu) clk-\u0026gt;name = \u0026quot;arm_global_timer\u0026quot;; clk-\u0026gt;features = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT | -\tCLOCK_EVT_FEAT_PERCPU; +\tCLOCK_EVT_FEAT_PERCPU | CLOCK_EVT_FEAT_PIPELINE; clk-\u0026gt;set_state_shutdown = gt_clockevent_shutdown; clk-\u0026gt;set_state_periodic = gt_clockevent_set_periodic; clk-\u0026gt;set_state_oneshot = gt_clockevent_shutdown; @@ -195,11 +195,6 @@ static int gt_dying_cpu(unsigned int cpu) return 0; } @@ -302,8 +307,8 @@ static int __init global_timer_of_register(struct device_node *np) goto out_clk; } -\terr = request_percpu_irq(gt_ppi, gt_clockevent_interrupt, -\t\u0026quot;gt\u0026quot;, gt_evt); +\terr = __request_percpu_irq(gt_ppi, gt_clockevent_interrupt, +\tIRQF_TIMER, \u0026quot;gt\u0026quot;, gt_evt); if (err) { pr_warn(\u0026quot;global-timer: can't register interrupt %d (%d)\\n\u0026quot;, gt_ppi, err);  This is another example of adapting an existing clock chip driver for serving out-of-band timing requests, with a subtle change in the way we should test for the current state of the clock device in the interrupt handler:\n a real/original device (such as the ARM global timer in this example) is switched to detached mode when it is controlled by the proxy tick driver. Therefore, testing the original device state for clockevent_state_oneshot() always leads to false.\n since a real device controlled by the proxy for receiving out-of-band events has to be driven in one-shot mode under the hood, one should always check for clockevent_state_oob() in addition to clockevent_state_oneshot(), so that we do apply the work-around as expected.\n  Failing to fix up the way we test for the clock device state would certainly lead to an interrupt storm with any ARM global timer suffering erratum 740657, quickly locking up the board.\n Theory of operations Calling tick_install_proxy() creates an instance of the proxy tick device on each CPU mentioned in the cpumask it receives. This routine is also passed a pointer to a struct proxy_tick_ops operation descriptor (ops), defining a few handlers the caller should provide for installing and managing the proxy device.\n The proxy operation descriptor\n struct proxy_tick_ops { void (*register_device)(struct clock_event_device *proxy_ced, struct clock_event_device *real_ced); void (*unregister_device)(struct clock_event_device *proxy_ced, struct clock_event_device *real_ced); void (*handle_event)(struct clock_event_device *real_ced); };  tick_install_proxy() first invokes ops-\u0026gt;register_device() for doing the prep work for the synthetic device, allowing the client code to chose its settings before registering it, typically by a call to clockevents_config_and_register(). This device registration handler should fill the clock event device structure pointed by proxy_ced, defining the proxy device characteristics from the standpoint of the in-band kernel, just like a clock chip driver would do. real_ced is the actual clock event device being substituted for.\nConversely, ops-\u0026gt;unregister_device() is an optional handler called by tick_uninstall_proxy() for dismantling a proxy device. NULL may be given if the autonomous core has no specific action to take upon such event. In any case, tick_uninstall_proxy() ensures that the proxy is fully detached and all the related resources are freed before returning.\nAlthough this is not strictly required, it is highly expected that register_device() gives a better rating to the proxy device than the original tick device\u0026rsquo;s, so that the in-band kernel would substitute the former for the latter.\n  A register_device() handler preparing a proxy device\n static void proxy_device_register(struct clock_event_device *proxy_ced, struct clock_event_device *real_ced) { /* * We know how to deal with delays expressed as counts of * nanosecs directly for programming events (i.e. no need * to translate into cycles). */ proxy_ced-\u0026gt;features |= CLOCK_EVT_FEAT_KTIME; /* * The handler which would receive in-band timing * requests. */ proxy_ced-\u0026gt;set_next_ktime = proxy_set_next_ktime; proxy_ced-\u0026gt;set_next_event = NULL; /* * Make sure we substitute for the real device: * advertise a better rating. */ proxy_ced-\u0026gt;rating = real_ced-\u0026gt;rating + 1; proxy_ced-\u0026gt;min_delta_ns = 1; proxy_ced-\u0026gt;max_delta_ns = KTIME_MAX; proxy_ced-\u0026gt;min_delta_ticks = 1; proxy_ced-\u0026gt;max_delta_ticks = ULONG_MAX; /* All set, register now. */ clockevents_register_device(proxy_ced); }  As illustrated above, the set_next_event() or set_next_ktime() member should be set in the structure pointed by proxy_ced with the address of a handler which receives timer requests from the in-band kernel. This handler is normally implemented by the autonomous core which takes control over the timer hardware via the proxy device. Whenever that core determines that a tick is due for an outstanding request received from such handler, it should call tick_notify_proxy() to signal the event to the main kernel. We add CLOCK_EVT_FEAT_KTIME to the proxy device flags because the autonomous core managing this device uses nanoseconds internally for expressing delays. For this reason, we want the main kernel to send timer requests to this core by passing delays as a count of nanoseconds to set_next_ktime() directly, without any conversion to hardware clock ticks.\n Once the user-supplied ops-\u0026gt;register_device() handler returns, the following events happen in sequence:\n with a rating most likely set to be higher than the current tick device\u0026rsquo;s (i.e. the real device), the proxy device substitutes for the former.\n the real device is detached from the clockevent layer. However, it is left in a functional state. The set_next_event() handler of this device is redirected to the user-supplied ops-\u0026gt;handle_event() handler. This way, every tick received from the real device would be passed on to the latter.\n the proxy device starts controlling the real device under the hood to carry out timing requests from the in-band kernel. When the hrtimer layer from the in-band kernel wants to program the next shot of the current tick device, it invokes the set_next_event() handler of the proxy device, which was defined by the caller. This handler should be scheduling in-band ticks at the requested time based on its own timer management.\n the timer interrupt triggered by the real device is switched to out-of-band handling. As a result, ops-\u0026gt;handle_event() receives tick events sent by the real device hardware directly from the oob stage of the interrupt pipeline, over the out-of-band context. This ensures high-precision timing. From that point, the out-of-band code can carry out its own timing duties, in addition to honoring the in-band kernel requests for timing.\n  Step 3. involves emulating ticks scheduled by the in-band kernel by a software logic controlled by some out-of-band timer management, paced by the real ticks received as described in step 4. When this logic decides than the next in-band tick is due, it should call tick_notify_proxy() to trigger the corresponding event for the in-band kernel, which would honor the pending (hr)timer request.\n Under the hood\n clockevents_program_event() [ROOT STAGE] proxy_dev-\u0026gt;set_next_event(proxy_dev) proxy_set_next_ktime(proxy_dev) (out-of-band timing logic) real_dev-\u0026gt;set_next_event(real_dev) ... \u0026lt;hardware tick event\u0026gt; oob_timer_handler() [HEAD STAGE] clockevents_handle_event(real_dev) ops-\u0026gt;handle_event(proxy_dev) (out-of-band timing logic) tick_notify_proxy() /* schedules hrtimer tick */ ... \u0026lt;synthetic tick event\u0026gt; proxy_irq_handler(proxy_dev) [ROOT stage] clockevents_handle_event(proxy_dev) hrtimer_interrupt(proxy_dev)  "
},
{
	"uri": "https://evenless.org/core/caveat/",
	"title": "Caveat",
	"tags": [],
	"description": "",
	"content": " Things you definitely want to know CONFIG_LOCKDEP is cool but ruins real-time guarantees The lock dependency engine which helps in tracking down deadlocks and other locking-related issues is available to Dovetail\u0026rsquo;s hard locks, which underpins most of the serialization mechanisms the EVL core uses.\nThis is nice as it has the regular lock validator monitor the hard spinlocks EVL uses too. However, this comes with a high price latency-wise: seeing hundreds of microseconds spent in the validator with hard interrupts off from time to time is not uncommon. Running the latency monitoring utility (aka latmus) which is part of libevl in this configuration should give you pretty ugly numbers.\nBy enabling any of the following switches in the kernel configuration, you would implicitly cause CONFIG_LOCKDEP to be enabled too:\n CONFIG_PROVE_LOCKING CONFIG_LOCK_STAT CONFIG_DEBUG_WW_MUTEX_SLOWPATH CONFIG_DEBUG_LOCK_ALLOC  In short, it is fine enabling them for debugging some locking pattern, but you won\u0026rsquo;t be able to meet real-time requirements at the same time in such configuration.\nisolcpus is our friend too Isolating some CPUs on the kernel command line using the isolcpus= option, in order to prevent the load balancer from offloading in-band work to them is not only a good idea with PREEMPT_RT, but for any dual kernel configuration too.\nBy doing so, having some random in-band work evicting cache lines on a CPU where real-time threads briefly sleep is less likely, increasing the odds of costly cache misses, which translates positively into the latency numbers you can get. Even if EVL\u0026rsquo;s small footprint core has a limited exposure to such kind of disturbance, saving a handful of microseconds is worth it when the worst case figure is already within tenths of microseconds.\n"
},
{
	"uri": "https://evenless.org/core/user-api/clock/",
	"title": "Clock element",
	"tags": [],
	"description": "",
	"content": " The target platform can provide particular clock chips and/or clock source drivers in addition to the architecture-specific ones. For instance, some device on a PCI bus could implement a timer chip which the application wants to use for timing its threads, in addition to the architected timer found on ARM64 and some ARM-based SoCs. In this case, we would need a specific clock driver, binding the timer hardware to the EVL core.\nEVL\u0026rsquo;s clock element ensures that all clock drivers present the same interface to applications in user-space. In addition, the clock element can export individual software timers to applications which comes in handy for running periodic loops or waiting for oneshot events on a specific time base.\nEVL abstracts clock event devices and clock sources (timekeeping hardware) into a single clock element.\n int evl_read_clock(int efd, struct timespec *tp) int evl_set_clock(int efd, const struct timespec *tp) int evl_get_clock_resolution(int efd, struct timespec *tp) int evl_adjust_clock(int efd, struct timex *tx) int evl_sleep(int efd, const struct timespec *timeout, struct timespec *remain) evl_udelay(unsigned int usecs) "
},
{
	"uri": "https://evenless.org/core/kernel-api/spinlock/",
	"title": "EVL Spinlocks",
	"tags": [],
	"description": "",
	"content": " evl_spin_lock_init(__lock) evl_spin_lock(__lock) evl_spin_lock_irqsave(__lock, __flags) evl_spin_unlock(__lock) evl_spin_unlock_irqrestore(__lock, __flags) DEFINE_EVL_SPINLOCK(__lock) "
},
{
	"uri": "https://evenless.org/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": " Who is behind the EVL project? A single developer at the moment, the guy who started this stuff nearly two decades ago, then that one too, which eventually became this. Hopefully, at some point, I will get the dual kernel thing right eventually.\nWhy the EVL project? EVL is a way to experiment freely with dual kernel technology going back to the drawing board for the most part, which can be pretty damn fun.\nIn the long run, I\u0026rsquo;m hoping that the work which takes place in EVL will help in improving Xenomai which I contribute to as well. For instance, substituting the venerable I-pipe with Dovetail as Xenomai\u0026rsquo;s dual kernel interface is a valuable goal.\nWhere should the discussion on EVL take place? You can e-mail me at rpm@xenomai.org to discuss EVL for now. If at some point more than a handful of folks find this work interesting, we will get a mailing list. To set high expectations, I\u0026rsquo;m not counting myself in this handful.\nHow do EVL and Xenomai differ? In several ways implementation-wise, starting from the interface which connects the Linux kernel to the autonomous core: EVL is developing Dovetail, Xenomai relies on its ancestor, the interrupt pipeline.\nAlthough the EVL core inherits some basic code from Xenomai\u0026rsquo;s Cobalt core, both implementations are already quite different, and poised to diverge even more over time:\n the EVL core only implements a handful of basic features in kernel space known as elements, which should be sufficient to provide high-level API services from user-space. At the opposite, Xenomai\u0026rsquo;s Cobalt core implements a POSIX interface directly from kernel space which amounts to 100+ system calls.\n another major difference is the lack of specific driver model in the EVL core, which fits in the regular Linux model. Xenomai relies on the RTDM layer instead, which is entirely separate.\n high scalability with large multi-core systems is a strong goal of the EVL core, and the current single-lock model Cobalt exhibits won\u0026rsquo;t fit that bill. For this reason, their respective scheduler infrastructures are already diverging.\n  EVL and Xenomai differ in purpose too. On the one hand, Xenomai aims at a comprehensive real-time framework offering multiple APIs, RTOS emulators and various protocol stacks, based on a POSIX-compliant core and the I-pipe dual kernel interface, tracking LTS kernel releases. Xenomai APIs can even run in single kernel configurations such as PREEMPT_RT.\nOn the other hand, EVL is about enabling any autonomous software core to run along the most recent mainline kernel release, according to clear interface rules defined by the latter. In other words, EVL is all about, and only about dual kernel technology. To showcase this, EVL comes with a compact real-time core delivering basic services via a small library implementing its ad hoc API, for anyone to play with and build on.\n"
},
{
	"uri": "https://evenless.org/core/kernel-api/flag/",
	"title": "Event flags",
	"tags": [],
	"description": "",
	"content": " void evl_init_flag(struct evl_flag *wf) void evl_destroy_flag(struct evl_flag *wf) int evl_wait_flag_timeout(struct evl_flag *wf, ktime_t timeout, enum evl_tmode timeout_mode) int evl_wait_flag(struct evl_flag *wf) struct evl_thread *evl_wait_flag_head(struct evl_flag *wf) void evl_raise_flag_nosched(struct evl_flag *wf) void evl_raise_flag(struct evl_flag *wf) void evl_pulse_flag_nosched(struct evl_flag *wf) void evl_pulse_flag(struct evl_flag *wf) void evl_flush_flag_nosched(struct evl_flag *wf, int reason) void evl_flush_flag(struct evl_flag *wf, int reason) DEFINE_EVL_FLAG(__name) "
},
{
	"uri": "https://evenless.org/dovetail/pipeline/porting/rawprintk/",
	"title": "Raw printk support",
	"tags": [],
	"description": "",
	"content": " Unless you are lucky enough to have an ICE for debugging hard issues involving out-of-band contexts, you might have to resort to basic printk-style debugging over a serial line. Although the printk() machinery can be used from out-of-band context when Dovetail is enabled, the output is deferred until the in-band stage gets back in control, which means that:\n you can\u0026rsquo;t reliably trace out-of-band code on the spot, deferred output issued from an out-of-band context, or from a section of code running with interrupts disabled in the CPU may appear after subsequent in-band messages under some circumstances, due to a buffering effect.\n if the debug traces are sent at high pace (e.g. from an out-of-band IRQ handler every few hundreds of microseconds), the machine is likely to come to a stall due to the massive output the heavy printk() machinery would have to handle, leading to an apparent lockup.\n  The only sane option for printk-like debugging in demanding out-of-band context is using the raw_printk() routine for issuing raw debug messages to a serial console, so that you may get some sensible feedback for understanding what is going on with the execution flow. This feature should be enabled by turning on CONFIG_RAW_PRINTK, otherwise all output sent to raw_printk() is discarded.\nBecause a stock serial console driver won\u0026rsquo;t be usable from out-of-band context, enabling raw printk support requires adapting the serial console driver your platform is using, by adding a raw write handler to the console description. Just like the write() handler, the write_raw() output handler receives a console pointer, the character string to output and its length as parameters. This handler should send the characters to the UART as quickly as possible, with little to no preparation.\nAll output formatted by the generic raw_printk() routine is passed to the raw write handler of the current serial console driver if present. Calls to the raw output handler are serialized in raw_printk() by holding a hard spinlock, which means that interrupts are disabled in the CPU when running the handler.\nA raw write handler is normally derived from the regular write handler for the same serial console device, skipping any in-band locking construct, only waiting for the bare minimum time for the output to drain in the UART since we want to keep interrupt latency low.\n  Adding RAW_PRINTK support to the AMBA PL011 serial driver\n --- a/drivers/tty/serial/amba-pl011.c +++ b/drivers/tty/serial/amba-pl011.c @@ -2206,6 +2206,40 @@ static void pl011_console_putchar(struct uart_port *port, int ch) pl011_write(ch, uap, REG_DR); } +#ifdef CONFIG_RAW_PRINTK + +/* + * The uart clk stays on all along in the current implementation, + * despite what pl011_console_write() suggests, so for the time being, + * just emit the characters assuming the chip is clocked. If the clock + * ends up being turned off after writing, we may need to clk_enable() + * it at console setup, relying on the non-zero enable_count for + * keeping pl011_console_write() from disabling it. + */ +static void +pl011_console_write_raw(struct console *co, const char *s, unsigned int count) +{ +\tstruct uart_amba_port *uap = amba_ports[co-\u0026gt;index]; +\tunsigned int old_cr, new_cr, status; + +\told_cr = readw(uap-\u0026gt;port.membase + UART011_CR); +\tnew_cr = old_cr \u0026amp; ~UART011_CR_CTSEN; +\tnew_cr |= UART01x_CR_UARTEN | UART011_CR_TXE; +\twritew(new_cr, uap-\u0026gt;port.membase + UART011_CR); + +\twhile (count-- \u0026gt; 0) { +\tif (*s == '\\n') +\tpl011_console_putchar(\u0026amp;uap-\u0026gt;port, '\\r'); +\tpl011_console_putchar(\u0026amp;uap-\u0026gt;port, *s++); +\t} +\tdo +\tstatus = readw(uap-\u0026gt;port.membase + UART01x_FR); +\twhile (status \u0026amp; UART01x_FR_BUSY); +\twritew(old_cr, uap-\u0026gt;port.membase + UART011_CR); +} + +#endif /* !CONFIG_RAW_PRINTK */ + static void pl011_console_write(struct console *co, const char *s, unsigned int count) { @@ -2406,6 +2440,9 @@ static struct console amba_console = { .device\t= uart_console_device, .setup\t= pl011_console_setup, .match\t= pl011_console_match, +#ifdef CONFIG_RAW_PRINTK +\t.write_raw\t= pl011_console_write_raw, +#endif .flags\t= CON_PRINTBUFFER | CON_ANYTIME, .index\t= -1, .data\t= \u0026amp;amba_reg,  ARM-specific raw console driver The vanilla ARM kernel port already provides an UART-based raw output routine called printascii() when CONFIG_DEBUG_LL is enabled, provided the right debug UART channel is defined too (CONFIG_DEBUG_UART_xx).\nWhen CONFIG_RAW_PRINTK and CONFIG_DEBUG_LL are both defined in the kernel configuration, the ARM implementation of Dovetail automatically registers a special console device for emitting debug output (see arch/arm/kernel/raw_printk.c), which redirects calls to its raw write handler by raw_printk() to printascii(). In other words, if CONFIG_DEBUG_LL already provides you with a functional debug output channel, you don\u0026rsquo;t need the active serial console driver to implement a raw write handler for enabling raw_printk(), the raw console device should handle raw_printk() requests just fine.\nEnabling CONFIG_DEBUG_LL with a wrong UART debug channel is a common cause of lockup at boot. You do want to make sure that the proper CONFIG_DEBUG_UART_xx symbol matching your hardware is selected along with CONFIG_DEBUG_LL.\n "
},
{
	"uri": "https://evenless.org/core/kernel-api/clock/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " The target platform can provide particular clock chips and/or clock source drivers in addition to the architecture-specific ones. For instance, some device on a PCI bus could implement a timer chip which the application wants to use for timing its threads, in addition to the architected timer found on ARM64 and some ARM-based SoCs. In this case, we would need a specific clock driver, binding the timer hardware to the EVL core.\nEVL\u0026rsquo;s clock device interface gives you a framework for integrating such hardware into the real-time core, which in turns enables applications to use it.\nEVL abstracts clock event devices and clock sources (timekeeping hardware) into a single clock element.\n int evl_init_clock(struct evl_clock *clock, const struct cpumask *affinity) evl_init_slave_clock(struct evl_clock *clock, struct evl_clock *master) int evl_register_clock(struct evl_clock *clock, const struct cpumask *affinity) void evl_unregister_clock(struct evl_clock *clock) void evl_announce_tick(struct evl_clock *clock) void evl_adjust_timers(struct evl_clock *clock, ktime_t delta) ktime_t evl_read_clock(struct evl_clock *clock) ktime_t evl_ktime_monotonic(void) u64 evl_read_clock_cycles(struct evl_clock *clock) int evl_set_clock_time(struct evl_clock *clock, const struct timespec *ts) void evl_set_clock_resolution(struct evl_clock *clock, ktime_t resolution) ktime_t evl_get_clock_resolution(struct evl_clock *clock) int evl_set_clock_gravity(struct evl_clock *clock, const struct evl_clock_gravity *gravity) struct evl_clock *evl_get_clock_by_fd(int efd) void evl_put_clock(struct evl_clock *clock) "
},
{
	"uri": "https://evenless.org/core/user-api/timer/",
	"title": "Timers",
	"tags": [],
	"description": "",
	"content": " int evl_new_timer(int clockfd) int evl_set_timer(int efd, struct itimerspec *value, struct itimerspec *ovalue) int evl_get_timer(int efd, struct itimerspec *value) "
},
{
	"uri": "https://evenless.org/dovetail/pipeline/porting/misc/",
	"title": "Misc",
	"tags": [],
	"description": "",
	"content": " printk() support printk() may be called by out-of-band code safely, without encurring extra latency. The output is conveyed like NMI-originated output, which involves some delay until the in-band code resumes, and the console driver(s) can handle it.\nTracing Tracepoints can be traversed by out-of-band code safely. Dynamic tracing is available to a kernel running the pipelined interrupt model too.\n"
},
{
	"uri": "https://evenless.org/core/user-api/xbuf/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " int evl_new_xbuf(size_t i_bufsz, size_t o_bufsz, const char *fmt, \u0026hellip;) "
},
{
	"uri": "https://evenless.org/core/kernel-api/timer/",
	"title": "Timers",
	"tags": [],
	"description": "",
	"content": " evl_init_timer(__timer, __clock, __handler, __rq, __flags) evl_init_core_timer(__timer, __handler) evl_init_timer_on_cpu(__timer, __cpu, __handler) void evl_destroy_timer(struct evl_timer *timer) void evl_start_timer(struct evl_timer *timer, ktime_t value, ktime_t interval) void evl_stop_timer(struct evl_timer *timer) ktime_t evl_get_timer_date(struct evl_timer *timer) ktime_t xntimer_get_interval(struct evl_timer *timer) ktime_t evl_get_timer_delta(struct evl_timer *timer) ktime_t evl_get_stopped_timer_delta(struct evl_timer *timer) "
},
{
	"uri": "https://evenless.org/core/user-api/proxy/",
	"title": "File proxy",
	"tags": [],
	"description": "",
	"content": " int evl_new_proxy(int fd, size_t bufsz, const char *fmt, \u0026hellip;) "
},
{
	"uri": "https://evenless.org/core/kernel-api/xbuf/",
	"title": "Using cross-buffers",
	"tags": [],
	"description": "",
	"content": " struct evl_xbuf *evl_get_xbuf(int efd, struct evl_file **efilpp) void evl_put_xbuf(struct evl_file *efilp) ssize_t evl_read_xbuf(struct evl_xbuf *xbuf, void *buf, size_t count, int f_flags) ssize_t evl_write_xbuf(struct evl_xbuf *xbuf, const void *buf, size_t count, int f_flags) "
},
{
	"uri": "https://evenless.org/core/user-api/poll/",
	"title": "Polling file descriptors",
	"tags": [],
	"description": "",
	"content": " int evl_new_poll(void) int evl_add_pollfd(int efd, int newfd, unsigned int events) int evl_del_pollfd(int efd, int delfd) int evl_mod_pollfd(int efd, int modfd, unsigned int events) int evl_timedpoll(int efd, struct evl_poll_event *pollset, int nrset, struct timespec *timeout) int evl_poll(int efd, struct evl_poll_event *pollset, int nrset) "
},
{
	"uri": "https://evenless.org/dovetail/pipeline/usage/",
	"title": "Kernel API",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://evenless.org/core/kernel-api/interrupts/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://evenless.org/core/user-api/io/",
	"title": "Out-of-band I/O services",
	"tags": [],
	"description": "",
	"content": " ssize_t oob_read(int efd, void *buf, size_t count) ssize_t oob_write(int efd, const void *buf, size_t count) int oob_ioctl(int efd, unsigned long request, \u0026hellip;) "
},
{
	"uri": "https://evenless.org/core/user-api/scheduling/",
	"title": "Scheduling policies",
	"tags": [],
	"description": "",
	"content": " EVL defines four scheduling policies, which are prioritized: this means that every time the core needs to pick the next eligible thread to run on the current CPU, it queries each policy module for a runnable thread in the following order:\n SCHED_FIFO, which is the common first-in, first-out real-time policy, also dealing with the SCHED_RR round-robin policy internally.\n SCHED_QUOTA, which enforces a limitation on the CPU consumption of threads over a fixed period of time.\n SCHED_WEAK, which is a non real-time policy allowing its members to run in-band most of the time, while retaining the ability to request EVL services.\n  The SCHED_QUOTA policy is optionally supported by the core, make sure to enable CONFIG_EVL_SCHED_QUOTA in the kernel configuration if you need it.\n SCHED_FIFO The first-in, first-out policy, fixed priority, preemptive scheduling policy. If you really need a refresher about this one, you can still have a look at this inspirational piece of post-modern poetry for background info. Or you can just go for the short version: with SCHED_FIFO, the scheduler always picks the runnable thread with the highest priority which spent the longest time waiting for the CPU to be available.\nEVL provides 99 fixed priority levels starting a 1, which maps 1:1 to the main kernel\u0026rsquo;s SCHED_FIFO implementation as well.\nSwitching a thread to FIFO scheduling is achieved by calling evl_set_schedattr(). The evl_sched_attrs attribute structure should be filled in as follows:\n\tstruct evl_sched_attrs attrs; attrs.sched_policy = SCHED_FIFO; attrs.sched_priority = \u0026lt;priority\u0026gt;; /* [1-99] */  SCHED_RR The round-robin policy is based on SCHED_FIFO internally. Additionally, it limits the execution time of its members to a given timeslice, moving a thread which fully consumed its current timeslice to the tail of the scheduling queue for its priority level. This is designed as a simple way to prevent threads from over-consuming the CPU within their own priority level.\nUnlike the main kernel which defines a global timeslice value for all members of the SCHED_RR class, EVL defines a per-thread quantum instead. Since EVL is tickless, this quantum may be any valid duration, and may differ among threads from the same priority group.\nSwitching a thread to round-robin scheduling is achieved by calling evl_set_schedattr(). The evl_sched_attrs attribute structure should be filled in as follows:\n\tstruct evl_sched_attrs attrs; attrs.sched_policy = SCHED_RR; attrs.sched_priority = \u0026lt;priority\u0026gt;; /* [1-99] */ attrs.sched_rr_quantum = (struct timespec){ .tv_sec = \u0026lt;seconds\u0026gt;, .tv_nsec = \u0026lt;nanoseconds\u0026gt;, };  SCHED_QUOTA The quota-based policy enforces a limitation on the CPU consumption of threads over a fixed period of time, known as the global quota period. Threads undergoing this policy are pooled in groups, with each group being given a share of the period (expressed as a percentage). The rules of the SCHED_FIFO policy apply among all threads regardless of their group.\nFor instance, say that we have five distinct thread groups, each of which is given a runtime credit which represents a portion of the global period: 35%, 25%, 15%, 10% and finally 5% of a global period set to one second. The first group would be allotted 350 milliseconds over a second, the second group would get 250 milliseconds from the same period and so on.\nEvery time a thread undergoing the SCHED_QUOTA policy is given the CPU, the time it consumes is charged to the group it belongs to. Whenever the group as a whole reaches the alloted time credit, all its members stall until the next period starts, at which point the runtime credit of every group is replenished for the next round of execution, resuming all its members in the process.\nYou may attach as many threads as you need to a single group, and the number of threads may vary between groups. The alloted runtime quota for a group is decreased by the execution time of every thread in that group. Therefore, a group with no thread does not consume its quota.\nSwitching a thread to quota-based scheduling is achieved by calling evl_set_schedattr(). The evl_sched_attrs attribute structure should be filled in as follows:\n\tstruct evl_sched_attrs attrs; attrs.sched_policy = SCHED_QUOTA; attrs.sched_priority = \u0026lt;priority\u0026gt;; /* [1-99] */ attrs.sched_quota_group = \u0026lt;grpid\u0026gt;; /* Quota group id. */  Creating, modifying and removing thread groups is achieved by calling evl_sched_control().\nRuntime credit and peak quota Each thread group is given its full quota every time the global period starts, according to the configuration set for this group. If the group did not consume its quota entirely by the end of the current period, the remaining credit is added to the group\u0026rsquo;s quota for the next period, up to a limit defined as the peak quota. If the accumulated credit would cause the quota to exceed the peak value, the extra time is spread over multiple subsequent periods until the credit is fully consumed.\nCreating a quota group EVL supports up to 1024 distinct thread groups for quota-based scheduling system-wide. Each thread group is assigned to a specific CPU by the application code which creates it, among the set of CPUs EVL runs out-of-band activity on (see the evl.oobcpus kernel parameter).\nA thread group is represented by a unique integer returned by the core upon creation, aka the group identifier.\nCreating a new thread group is achieved by calling evl_sched_contol(). Some information including the new group identifier is returned in the ancillary evl_sched_ctlinfo structure passed to the request. The evl_sched_ctlparams control structure should be filled in as follows:\n\tstruct evl_sched_ctlparam param; struct evl_sched_ctlinfo info; param.quota.op = evl_quota_add; ret = evl_sched_control(SCHED_QUOTA, \u0026amp;param, \u0026amp;info, \u0026lt;cpu-number\u0026gt;);  On success, the following information is received from the core regarding the new group:\n info.quota.tgid contains the new group identifier.\n info.quota.quota_percent is the current percentage of the global period allotted to the new group. At creation time, this value is set to 100%. You may want to change it to reflect the final value.\n info.quota.quota_peak_percent reflects the current peak percentage of the global period allotted to the new group, which is also set to 100% at creation time. This means that by default, a new group might double its quota value by accumulating runtime credit, consuming up to 100% of the CPU time during the next period. Likewise, you may want to change it to reflect the final value.\n info.quota.quota_sum is the sum of the quota values of all groups assigned to the CPU specified in the evl_sched_control() request. This gives the overall CPU business as far as SCHED_QUOTA is concerned. This sum should not exceed 100% for a CPU in a properly configured system.\n  Modifying a quota group Removing a quota group SCHED_WEAK "
},
{
	"uri": "https://evenless.org/core/user-api/utils/",
	"title": "Utilities",
	"tags": [],
	"description": "",
	"content": " ssize_t evl_log(int fd, const void *buf, size_t len) int evl_printf(int fd, const char *fmt, \u0026hellip;) int evl_sched_control(int policy, union evl_sched_ctlparam *param, union evl_sched_ctlinfo *info, int cpu) "
},
{
	"uri": "https://evenless.org/core/kernel-api/function_index/",
	"title": "Function index",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://evenless.org/core/user-api/function_index/",
	"title": "Function index",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://evenless.org/dovetail/pipeline/porting/devnotes/",
	"title": "Developer&#39;s Notes",
	"tags": [],
	"description": "",
	"content": " Generic Fundamentally preemption-safe contexts Over a few contexts, we may traverse code using unprotected, preemption-sensitive accessors such as percpu() without disabling preemption specifically, because either one condition is true;\n if preempt_count() bears either of the PIPELINE_MASK or STAGE_MASK bits, which turns preemption off, therefore CPU migration cannot happen (debug_smp_processor_id() and preempt checks in percpu accessors would detect such context properly too).\n if we are running over the context of the in-band stage\u0026rsquo;s event log syncer (sync_current_stage()) playing a deferred interrupt, in which case the virtual interrupt disable bit is set, so no CPU migration may occur either.\n  For instance, the following contexts qualify:\n clockevents_handle_event(), which should either be called from the oob stage - therefore STAGE_MASK is set - when the proxy tick device is active on the CPU, and/or from the in-band stage playing a timer interrupt event from the corresponding device.\n any IRQ flow handler from kernel/irq/chip.c. When called from generic_pipeline_irq() for pushing an external event to the pipeline, on_pipeline_entry() is true, which indicates that PIPELINE_MASK is set. When called for playing a deferred interrupt on the in-band stage, the virtual interrupt disable bit is set.\n  Checking for out-of-band interrupt property The IRQF_OOB action flag should NOT be used for testing whether an interrupt is out-of-band, because out-of-band handling may be turned on/off dynamically on an IRQ descriptor using irq_switch_oob(), which would not translate to IRQF_OOB being set/cleared for the attached action handlers.\nirq_is_oob() is the right way to check for out-of-band handling.\nstop_machine() hard disables interrupts The stop_machine() service guarantees that all online CPUs are spinning non-preemptible in a known code location before a subset of them may safely run a stop-context function. This service is typically useful for live patching the kernel code, or changing global memory mappings, so that no activity could run in parallel until the system has returned to a stable state after all stop-context operations have completed.\nWhen interrupt pipelining is enabled, Dovetail provides the same guarantee by restoring hard interrupt disabling where virtualizing the interrupt disable flag would defeat it.\nAs those lines are written, all stop_machine() use cases must also exclude any oob stage activity (e.g. ftrace live patching the kernel code for installing tracepoints), or happen before any such activity can ever take place (e.g. KPTI boot mappings). Dovetail makes a basic assumption that stop_machine() could not get in the way of latency-sensitive processes, simply because the latter could not keep running safely until a call to the former has completed anyway.\nHowever, one should keep an eye on stop_machine() usage upstream, identifying new callers which might cause unwanted latency spots under specific circumstances (maybe even abusing the interface).\nVirtual interrupt disable state breakage When some WARN_ON() triggers due to a wrong interrupt disable state (e.g. entering the softirqs/bh code with IRQs unexpectedly [virtually] disabled), this may be due to the CPU and virtual interrupt states being out-of-sync when traversing the epilogue code after a syscall, IRQ or trap has been handled during the latest kernel entry.\nTypically, do_work_pending() or do_notify_resume() should make sure to reconcile both states in the work loop, and also to restore the virtual state they received on entry before returning to their caller.\nThe routines just mentioned always enter from their assembly call site with interrupts hard disabled in the CPU. However, they may be entered with the virtual interrupt state enabled or disabled, depending on the kind of event which led to them eventually. Typically, a system call epilogue would always enter with the virtual state enabled, but a fault might also occur when the virtual state is disabled though. The epilogue routine called for finalizing some IRQ handling must enter with the virtual state enabled, since the latter is a pre-requisite for running such code.\n Losing the timer tick The symptom of a common issue in a Dovetail port is losing the timer interrupt when the out-of-band (co-)kernel takes control over the tick device, causing the in-band kernel to stall. After some time spent hanging, the in-band kernel may eventually complain about a RCU stall situation with a message like _INFO: rcupreempt detected stalls on CPUs/tasks followed by stack dump(s). In other cases, the machine may simply lock up due to an interrupt storm.\nThis is typical of timer interrupt events not flowing down normally to the in-band kernel anymore because something went wrong as soon as the proxy tick device replaced the regular device for serving in-band timing requests. When this happens, we should check the following code spots for bugs:\n the timer acknowledge code is wrong once called from the oob stage, which is going to be the case as soon as an autonomous core installs the proxy tick device for interposing on the timer. Being wrong here means performing actions which are not legit from such a context.\n the irqchip driver managing the interrupt event for the timer tick is wrong somehow, causing such interrupt to stay masked or stuck for some reason whenever it is switched to out-of-band mode. You need to double-check the implementation of the chip handlers, considering the effects and requirements of interrupt pipelining.\n power management (CONFIG_CPUIDLE) gets in the way, often due to the infamous C3STOP misfeature turning off the original timer hardware controlled by the proxy device. A detailed explanation is given in Documentation/irq_pipeline.rst when discussing the few changes to the scheduler core for supporting the Dovetail interface. If this is acceptable from a power saving perspective, having the autonomous core prevent the in-band kernel from entering a deeper C-state is enough to fix the issue, by overriding the irq_cpuidle_control() routine as follows:\n  bool irq_cpuidle_control(struct cpuidle_device *dev, struct cpuidle_state *state) { /* * Deny entering sleep state if this entails stopping the * timer (i.e. C3STOP misfeature). */ if (state \u0026amp;\u0026amp; (state-\u0026gt;flags \u0026amp; CPUIDLE_FLAG_TIMER_STOP)) return false; return true; }  Printk-debugging such timer iss ue requires enabling raw printk() support, you won\u0026rsquo;t get away with tracing the kernel behavior using the plain printk() routine for this, because most of the output would remain stuck into a buffer, never reaching the console driver before the board hangs eventually.\n ARM Context assumption with outer L2 cache There is no reason for the outer cache to be invalidated/flushed/cleaned from an out-of-band context, all cache maintenance operations must happen from in-band code. Therefore, we neither need nor want to convert the spinlock serializing access to the cache maintenance operations for L2 to a hard lock.\nThis above assumption is unfortunately only partially right, because at some point in the future we may want to run DMA transfers from the out-of-band context, which could entail cache maintenance operations.\n Conversion to hard lock may cause latency to skyrocket on some i.MX6 hardware, equipped with PL22x cache units, or PL31x with errata 588369 or 727915 for particular hardware revisions, as each background operation would be awaited for completion with hard irqs disabled, in order to work around some silicon bug.\n"
},
{
	"uri": "https://evenless.org/dovetail/pipeline/usage/interrupt_protection/",
	"title": "Interrupt Protection",
	"tags": [],
	"description": "",
	"content": " Disabling interrupts in the CPU The local_irq_save() and local_irq_disable() helpers are no more disabling interrupts in the CPU when interrupt pipelining is enabled, but only disable interrupt events virtually for the in-band stage.\nA set of helpers is provided for manipulating the interrupt disable flag in the CPU instead. When CONFIG_IRQ_PIPELINE is disabled, this set maps 1:1 over the regular local_irq_*() API.\n   Original/Virtual Non-virtualized call     local_save_flags(flags) flags = hard_local_save_flags()   local_irq_disable() hard_local_irq_disable()   local_irq_enable() hard_local_irq_enable()   local_irq_save(flags) flags = hard_local_irq_save()   local_irq_restore(flags) hard_local_irq_restore(flags)   irqs_disabled() hard_irqs_disabled()   irqs_disabled_flags(flags) hard_irqs_disabled_flags(flags)    Stalling the out-of-band stage Just like the in-band stage is affected by the state of the virtual interrupt disable flag, the interrupt state of the oob stage is controlled by a dedicated stall bit flag in the oob stage\u0026rsquo;s status. In combination with the interrupt disable bit in the CPU, this software bit controls interrupt delivery to the oob stage.\nWhen this stall bit is set, interrupts which might be pending in the oob stage\u0026rsquo;s event log of the current CPU are not played. Conversely, the out-of-band handlers attached to pending IRQs are fired when the stall bit is clear. The following table represents the equivalent calls affecting the stall bit for each stage:\n   In-band stage operation OOB stage operation     local_save_flags(flags) -none-   local_irq_disable() oob_irq_disable()   local_irq_enable() oob_irq_enable()   local_irq_save(flags) flags = oob_irq_save()   local_irq_restore(flags) oob_irq_restore(flags)   irqs_disabled() oob_irqs_disabled()   irqs_disabled_flags(flags) -none-    "
},
{
	"uri": "https://evenless.org/dovetail/pipeline/usage/stage_push/",
	"title": "Installing the out-of-band stage",
	"tags": [],
	"description": "",
	"content": " enable_oob_stage()\n disable_oob_stage()\n  "
},
{
	"uri": "https://evenless.org/dovetail/pipeline/usage/stage_escalation/",
	"title": "Stage escalation",
	"tags": [],
	"description": "",
	"content": "Sometimes you may need to escalate the current execution stage from in-band to out-of-band, only for running a particular routine. This can be done using run_oob_call(). For instance, the EVL core is using this service to escalate calls to its rescheduling procedure to the out-of-band stage, as described in the discussion about switching task contexts with Dovetail\u0026rsquo;s support for alternate scheduling.\n int run_oob_call(int (*fn)(void *arg), void *arg)  fnThe address of the routine to execute on the out-of-band stage.\n\nargThe routine argument.\n\nrun_oob_call() first switches the current execution stage to out-of-band - if need be - then calls the routine with hard interrupts disabled (i.e. disabled in the CPU). Upon return, the integer value returned by fn() is passed back to the caller.\nBecause the routine may switch the execution stage back to in-band for the calling context, run_oob_call() restores the original stage only if it did not change in the meantime. In addition, the interrupt log of the current CPU is synchronized before returning to the caller. The following matrix describes the logic for determining which epilogue should be performed before leaving run_oob_call(), depending on the active stage on entry to the latter and on return from fn():\n   On entry to run_oob_call() At exit from fn() Epilogue     out-of-band out-of-band sync current stage if not stalled   in-band out-of-band switch to in-band + sync both stages   out-of-band in-band sync both stages   in-band in-band sync both stages    run_oob_call() is a lightweight operation that switches the CPU to the out-of-band interrupt stage for the duration of the call, whatever the underlying context may be. This is different from switching a task context to the out-of-band mode indefinitely by offloading it to the autonomous core for scheduling. The latter operation would involve a more complex procedure.\n "
},
{
	"uri": "https://evenless.org/dovetail/pipeline/usage/pipeline_inject/",
	"title": "IRQ injection",
	"tags": [],
	"description": "",
	"content": " irq_inject_pipeline()\n irq_post_inband()\n irq_post_oob()\n irq_pipeline_send_remote()\n  "
},
{
	"uri": "https://evenless.org/dovetail/pipeline/usage/irq_handling/",
	"title": "IRQ handling",
	"tags": [],
	"description": "",
	"content": "The driver API to the IRQ subsystem exposes the new interrupt type flag IRQF_OOB, denoting an out-of-band handler to the generic interrupt API routines:\n setup_irq() for early registration of special interrupts request_irq() for device interrupts __request_percpu_irq() for per-CPU interrupts  An IRQ action handler bearing this flag will run from out-of-band context over the oob stage, regardless of the current interrupt state of the in-band stage. If no oob stage is present, the flag will be ignored, with the interrupt handler running on the in-band stage as usual.\nConversely, out-of-band handlers can be dismissed using the generic interrupt API, such as:\n free_irq() for device interrupts free_percpu_irq() for per-CPU interrupts  Out-of-band IRQ handling has the following constraints:\n If the IRQ is shared, with multiple action handlers registered for the same event, all other handlers on the same interrupt channel must bear the IRQF_OOB flag too, or the request will fail.  If meeting real-time requirements is your goal, sharing an IRQ line among multiple devices operating from different execution stages (in-band vs out-of-band) can only be a bad idea design-wise. You should resort to this in desparate hardware situations only.\n  Obviously, out-of-band handlers cannot be threaded (IRQF_NO_THREAD is implicit, IRQF_ONESHOT is ignored).   Installing an out-of-band handler for a device interrupt\n #include \u0026lt;linux/interrupt.h\u0026gt; static irqreturn_t oob_interrupt_handler(int irq, void *dev_id) { ... return IRQ_HANDLED; } init __init driver_init_routine(void) { int ret; ... ret = request_irq(DEVICE_IRQ, oob_interrupt_handler, IRQF_OOB, \u0026quot;Out-of-band device IRQ\u0026quot;, device_data); if (ret) goto fail; return 0; fail: /* Unwind upon error. */ ... }  "
},
{
	"uri": "https://evenless.org/dovetail/pipeline/usage/locking/",
	"title": "Locking",
	"tags": [],
	"description": "",
	"content": " Additional spinlock types The pipeline core introduces two spinlock types:\n hard spinlocks manipulate the CPU interrupt mask, and don\u0026rsquo;t affect the kernel preemption state in locking/unlocking operations.  This type of spinlock is useful for implementing a critical section to serialize concurrent accesses from both in-band and out-of-band contexts, i.e. from in-band and oob stages. Obviously, sleeping into a critical section protected by a hard spinlock would be a very bad idea. In other words, hard spinlocks are not subject to virtual interrupt masking, therefore can be used to serialize with out-of-band activities, including from the in-band kernel code. At any rate, those sections ought to be quite short, for keeping latency low.\n mutable spinlocks are used internally by the pipeline core to protect access to IRQ descriptors (struct irq_desc::lock), so that we can keep the original locking scheme of the generic IRQ core unmodified for handling out-of-band interrupts.  Mutable spinlocks behave like hard spinlocks when traversed by the low-level IRQ handling code on entry to the pipeline, or common raw spinlocks otherwise, preserving the kernel (virtualized) interrupt and preemption states as perceived by the in-band context. This type of lock is not meant to be used in any other situation.\nLockdep support The lock validator automatically reconciles the real and virtual interrupt states, so it can deliver proper diagnosis for locking constructs defined in both in-band and out-of-band contexts. This means that hard and mutable spinlocks are included in the validation set when LOCKDEP is enabled.\nThese two additional types are subject to LOCKDEP analysis. However, be aware that latency figures are likely to be really bad when LOCKDEP is enabled, due to the large amount of work the lock validator may have to do with interrupts disabled for the CPU (i.e. hard locking) for enforcing critical sections.\n "
},
{
	"uri": "https://evenless.org/dovetail/pipeline/usage/synthetic/",
	"title": "Synthetic IRQs",
	"tags": [],
	"description": "",
	"content": " The pipeline introduces an additional type of interrupts, which are purely software-originated, with no hardware involvement. These IRQs can be triggered by any kernel code. Synthetic IRQs are inherently per-CPU events. Because the common pipeline flow applies to synthetic interrupts, it is possible to attach them to out-of-band and/or in-band handlers, just like device interrupts.\nSynthetic interrupts abide by the normal rules with respect to interrupt masking: such IRQs may be deferred until the stage they should be handled from is unstalled.\nSynthetic interrupts and softirqs differ in essence: the latter only exist in the in-band context, and therefore cannot trigger out-of-band activities. Synthetic interrupts used to be called virtual IRQs (or virq for short) by the legacy I-pipe implementation, Dovetail\u0026rsquo;s ancestor; such rename clears the confusion with the way abstract interrupt numbers defined within interrupt domains may be called elsewhere in the kernel code base (i.e. virtual interrupts too).\n Allocating synthetic interrupts Synthetic interrupt vectors are allocated from the synthetic_irq_domain, using the irq_create_direct_mapping() routine.\nA synthetic interrupt handler can be installed for running on the in-band stage upon a scheduling request (i.e. being posted) from an out-of-band context as follows:\n#include \u0026lt;linux/irq_pipeline.h\u0026gt; static irqreturn_t sirq_handler(int sirq, void *dev_id) { do_in_band_work(); return IRQ_HANDLED; } static struct irqaction sirq_action = { .handler = sirq_handler, .name = \u0026quot;In-band synthetic interrupt\u0026quot;, .flags = IRQF_NO_THREAD, }; unsigned int alloc_sirq(void) { unsigned int sirq; sirq = irq_create_direct_mapping(synthetic_irq_domain); if (!sirq) return 0; setup_percpu_irq(sirq, \u0026amp;sirq_action); return sirq; }  A synthetic interrupt handler can be installed for running from the oob stage upon a trigger from an in-band context as follows:\nstatic irqreturn_t sirq_oob_handler(int sirq, void *dev_id) { do_out_of_band_work(); return IRQ_HANDLED; } unsigned int alloc_sirq(void) { unsigned int sirq; sirq = irq_create_direct_mapping(synthetic_irq_domain); if (!sirq) return 0; ret = __request_percpu_irq(sirq, sirq_oob_handler, IRQF_OOB, \u0026quot;Out-of-band synthetic interrupt\u0026quot;, dev_id); if (ret) { irq_dispose_mapping(sirq); return 0; } return sirq; }  Scheduling in-band execution of a synthetic interrupt handler The execution of sirq_handler() in the in-band context can be scheduled (or posted) from the out-of-band context in two different ways:\nUsing the common injection service irq_pipeline_inject(sirq);  Using the lightweight injection method (requires interrupts to be disabled in the CPU) unsigned long flags = hard_local_irqsave(); irq_post_inband(sirq); hard_local_irqrestore(flags);  Assuming that no interrupt may be pending in the event log for the oob stage at the time this code runs, the second method relies on the invariant that in a pipeline interrupt model, IRQs pending for the in-band stage will have to wait for the oob stage to quiesce before they can be handled. Therefore, it is pointless to check for synchronizing the interrupts pending for the in-band stage from the oob stage, which the irq_pipeline_inject() service would do systematically. irq_post_inband() simply marks the event as pending in the event log of the in-band stage for the current CPU, then returns. This event would be played as a result of synchronizing the log automatically when the current CPU switches back to the in-band stage.\n It is also valid to post a synthetic interrupt to be handled on the in-band stage from an in-band context, using irq_pipeline_inject(). In such a case, the normal rules of interrupt delivery apply, depending on the state of the virtual interrupt disable flag for the in-band stage: the IRQ is immediately delivered, with the call to irq_pipeline_inject() returning only after the handler has run.\nTriggering out-of-band execution of a synthetic interrupt handler Conversely, the execution of sirq_handler() on the oob stage can be triggered from the in-band context as follows:\nirq_pipeline_inject(sirq);  Since the oob stage has precedence over the in-band stage for execution of any pending event, this IRQ is immediately delivered, with the call to irq_pipeline_inject() returning only after the handler has run.\nIt is also valid to post a synthetic interrupt to be handled on the oob stage from an out-of-band context, using irq_pipeline_inject(). In such a case, the normal rules of interrupt delivery apply, depending on the state of the virtual interrupt disable flag for the oob stage.\nCalling irq_post_oob(sirq) from the in-band stage to trigger an out-of-band event is most often not the right way to do this, because this service would not synchronize the interrupt log before returning. In other words, the sirq event would still be pending for the oob stage despite the fact that it should have preempted the in-band stage before returning to the caller.\n "
},
{
	"uri": "https://evenless.org/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://evenless.org/",
	"title": "Evenless",
	"tags": [],
	"description": "",
	"content": " Evenless Dual Kernel Rebooted For certain types of applications, offloading a particular set of time-critical tasks to an autonomous core hosted by the Linux kernel may deliver the best possible performance when compared to forcing the entire Linux kernel to meet the most stringent scheduling latency requirements that only those tasks may have. Evenless (EVL for short) is a practical work on finding the best possible integration of such a dedicated software core into the mainline Linux kernel.\nEVL re-imagines the venerable dual kernel design by introducing the dual kernel support logic at the heart of the Linux kernel. Because Linux should define the set of rules to host a guest core, EVL has been designed specifically with this integration in mind.\nThe key technical issue is about defining a new execution stage in the mainline kernel logic which would represent high priority, out-of-band activities separated from the common work running in-band. This requires a few kernel subsystems to know intimately about the new execution context, which in turn has the following upsides:\n the integration is simpler and cleaner, because we don\u0026rsquo;t need sideways. The point is not about hiding the dual kernel interface from the main logic, but on the contrary to make it a customary interface of the main kernel.\n compared to the I-pipe which is the interface currently used by several dual kernel systems such as Xenomai, maintaining the new Dovetail dual kernel interface out-of-tree proved to be a much easier task already, without fundamental conflicts with upstream changes, only marginal adjustments so far.\n  Dual kernel made easy EVL as a project is about enabling engineers to implement their own flavour of software core running symbiotically with Linux, whatever the specific purpose of this core may be. This ongoing work is composed of:\n the Dovetail interface, which introduces a high-priority execution stage for the main kernel, enabling an independent software core to run on it.\n the EVL core, a compact autonomous core showcasing Dovetail. It aims at delivering reliable low-latency services to applications which have to meet real-time requirements. It is developed like any ordinary feature of the mainline kernel, making the best of the rich infrastructure we have there for improving the integration. This core is conceived as a learning tool for discovering the dual kernel technology as much as it aims at excellent performance and usability.\n an in-depth documentation which covers both Dovetail and the EVL core, with many cross-references between them, so that people can implement their software core of choice almost by example.\n  At the end of the day, the success criteria for EVL should be about exhibiting:\n low engineering and maintenance costs, so that common kernel knowledge with limited manpower should be enough to maintain Dovetail and the EVL core over the development tip of the mainline kernel.\n low runtime cost, with excellent real-time performances including on low-end and mid-range hardware, leaving plenty of cycles for running GPOS work concurrently.\n high scalability, from single core to dozens of them happily running the time-critical workload in parallel with reliably low latency footprint.\n  Getting the sources You need to clone two GIT repositories:\n Dovetail and the EVL core are maintained in separate branches of the same GIT repository, tracking the mainline kernel:\n git://git.evenless.org/linux-evl.git https://git.evenless.org/linux-evl.git  the second repository contains the source code of the EVL library, which is the user-space API to the EVL core:\n git://git.evenless.org/libevl.git https://git.evenless.org/libevl.git   The build recipe is available there.\nLicensing terms SPDX license identifiers are used throughout the code to state the licensing terms of each file clearly. This boils down to:\n GPL-2.0 for the EVL core in kernel space.\n GPL-2.0 WITH Linux-syscall-note for the UAPI bits exported to user-space, so that libevl knows at build time about the ABI details of the system call interface implemented by the EVL core.\n MIT for all code from libevl, which implements the EVL system call wrappers, a few utilities and test programs.\n  "
},
{
	"uri": "https://evenless.org/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]