[
  {
    "content": "A cornerstone of many real-time capable drivers which can process requests from the out-of-band stage is DMA support. Being able to offload data transfers to a DMA unit goes a long way toward implementing efficient acquisition loops, especially if they have to run at high frequency, sparing precious CPU cycles.\nIn order to support out-of-band transactions, a DMA controller (e.g. bcm2835-dma, or imx-sdma) must include Dovetail-specific changes. Since DMA drivers are commonly based on the virtual channel layer (aka virt-dma), Dovetail adds the required changes to this layer in order to cope with execution from the out-of-band stage. The current list of DMA controllers which support out-of-band transactions is available at this location.\nOut-of-band DMA channel Dovetail introduces a simple interface for switching a DMA channel to out-of-band mode, based on special flags passed to any of the supported prep calls, such as dmaengine_prep_slave_sg(), dmaengine_prep_slave_single() or dmaengine_prep_dma_cyclic(). This mode remains active until dmaengine_terminate_async() (or the deprecated dmaengine_terminate_all()) routine is called for the channel, which is therefore reserved for out-of-band operations until terminated.\nTwo modes of operation are available for running a DMA channel out-of-band:\n  the common cyclic mode, where an I/O peripheral is driving the data transaction, triggering transfers repeatedly on a periodic basis. If the DMA_OOB_INTERRUPT flag is passed to dmaengine_prep_dma_cyclic(), the DMA completion callback set for the TX descriptor (see struct dma_async_tx_descriptor) is called at the end of each cycle from the out-of-band stage, so that such callback can wake up real-time capable tasks without involving any non-deterministic in-band activity. A typical use case is that an application on the Linux side with strict low latency timing requirement runs the data exchanged with an audio codec through some processing pipeline.\n  the new pulsed mode introduced by Dovetail, in which the kernel software wants to trigger every data transfer manually and repeatedly, acting as the master side of the transaction. This mode should be available from any type of DMA transaction the out-of-band capable driver supports, but the cyclic one. It is turned on by passing the DMA_OOB_PULSE flag to the corresponding prep call, such as dmaengine_prep_slave_sg() or dmaengine_prep_slave_single(). Once the transfer is completed, any callback set for the TX descriptor is fired from the out-of-band stage. The key aspect is the ability to trigger multiple transfers and receive the corresponding completion events directly from the out-of-band stage using a single TX descriptor, which is different from the usual case in which every TX descriptor describes a single transfer which might be delayed by other pending operations. A typical use case is with implementing closed-control loops driven by the Linux system with stringent timing requirements, for instance over a SPI bus.\n  Programming a DMA channel for out-of-band operation Cyclic transaction The way to set up a cyclic transaction with out-of-band completion events is straightforward:\n  create a cyclic DMA transaction via dmaengine_prep_dma_cyclic(), OR’ing the DMA_OOB_INTERRUPT flag into the flags parameter. This flag ensures that the completion callback set into the DMA TX descriptor is called from the out-of-band stage.\n  submit the transaction by a call to dmaengine_submit(),\n  finally bring this transaction to active state by issuing the pending DMA requests the way you would do for common transactions using dma_async_issue_pending(). Once your out-of-band transaction is picked by the DMA engine for execution, it has a lock on the channel until terminated.\n   Setting up a DMA transaction for cyclic out-of-band transfers\n Pulsed mode transaction A DMA transaction of pulsed transfers is a common slave transaction for the most part, except that each transfer must be started manually by a call to dma_pulse_oob() for the related channel. To set it up, you need to:\n  create a slave DMA transaction via dmaengine_prep_slave_sg() or dmaengine_prep_slave_single() for instance, OR’ing the DMA_OOB_PULSE flag into the flags parameter. This flag ensures that:\n  each transfer in either direction can only be started at your command by a call to dma_pulse_oob() instead of leaving this decision to the DMA engine,\n  the completion callback set into the DMA TX descriptor is called from the out-of-band stage each time such a transfer has completed.\n    submit the transaction by a call to dmaengine_submit(),\n  finally bring this transaction to active state by issuing the pending DMA requests the way you would do for common transactions using dma_async_issue_pending(). Once your out-of-band transaction is picked by the DMA engine for execution, it has a lock on the channel until terminated.\n   Setting up a DMA transaction for pulsed out-of-band transfers\n   int dma_pulse_oob(struct dma_chan *chan)  This routine triggers the next transfer on a DMA channel set up for a pulsed mode transaction, either for sending or receiving data depending on the prep call.\nchanThe DMA channel descriptor for which to start the next transfer.\n\nZero is returned on success, otherwise:\n -EIO. No transfer from an out-of-band transaction is ready to be started manually. Typically, chan was not previously switched to out-of-band operation mode by submitting a TX descriptor bearing DMA_OOB_PULSE, followed by a call to dmaengine_submit(). You may also need to tell the DMA engine to start processing the pending transactions by calling dma_async_issue_pending() right after submitting the TX descriptor.   ИнформацияAn out-of-band completion handler executes in out-of-band IRQ context, so you may only run code which is legit there.\n Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "DMA",
    "uri": "/ru/core/oob-drivers/dma/"
  },
  {
    "content": "Thread services   #usvctable { width: 30%; } #usvctable td { text-align: center; }   Function name EVL Switch1 non-EVL2   evl_attach_thread() N/A ✅   evl_attach_self() N/A ✅   evl_detach_thread() ▽ ✕   evl_detach_self() ▽ ✕   evl_demote_thread() ⚊ ✅   evl_get_self() ⚊ ✕   evl_get_state() ⚊ ✅   evl_get_thread_mode() ⚊ ✅   evl_is_inband() ⚊ ✅   evl_set_thread_mode() ⚊ ✅   evl_subscribe() ▽ ✕   evl_switch_inband() ▽ ✕   evl_switch_oob() △ ✕   evl_unblock_thread() ⚊ ✅   evl_unsubscribe() ▽ ✕    Scheduler services   Function name EVL Switch1 non-EVL2   evl_get_schedattr() ⚊ ✅   evl_set_schedattr() ⚊ ✅  evl_control_sched() ⚊ ✅   evl_get_cpustate() ⚊ ✅   evl_yield() △ ✕    Clock services   Function name EVL Switch1 non-EVL2   evl_new_clock() ▽ ✅   evl_read_clock() ⚊ ✅   evl_set_clock() ⚊ ✅   evl_get_clock_resolution() ⚊ ✅   evl_sleep_until() △ ✕   evl_udelay() △ ✕    Timer services   Function name EVL Switch1 non-EVL2   evl_new_timer() ▽ ✅   evl_set_timer() ⚊ ✅   evl_get_timer() ⚊ ✅    Mutex services   Function name EVL Switch1 non-EVL2   evl_create_mutex() ▽ ✅   evl_new_mutex() ▽ ✅   evl_open_mutex() ▽ ✅   evl_lock_mutex() △4 ✕   evl_trylock_mutex() △4 ✕   evl_timedlock_mutex() △4 ✕   evl_unlock_mutex() ⚊3 ✕   evl_get_mutex_ceiling() ⚊ ✅   evl_set_mutex_ceiling() ⚊ ✅   evl_close_mutex() ▽ ✅    Event services   Function name EVL Switch1 non-EVL2   evl_create_event() ▽ ✅   evl_new_event() ▽ ✅   evl_open_event() ▽ ✅   evl_wait_event() ⚊4 ✕   evl_timedwait_event() ⚊4 ✕   evl_signal_event() ⚊4 ✕   evl_signal_thread() ⚊4 ✕   evl_broadcast_event() ⚊4 ✕   evl_close_event() ▽ ✅    Flags services   Function name EVL Switch1 non-EVL2   evl_create_flags() ▽ ✅   evl_new_flags() ▽ ✅   evl_open_flags() ▽ ✅   evl_wait_flags() △4 ✕   evl_timedwait_flags() △4 ✕   evl_trywait_flags() ⚊4 ✅   evl_peek_flags() ⚊ ✅   evl_post_flags() ⚊4 ✅   evl_close_flags() ▽ ✅    Semaphore services   Function name EVL Switch1 non-EVL2   evl_create_sem() ▽ ✅   evl_new_sem() ▽ ✅   evl_open_sem() ▽ ✅   evl_get_sem() △4 ✕   evl_timedget_sem() △4 ✕   evl_tryget_sem() ⚊4 ✅   evl_peek_sem() ⚊ ✅   evl_put_sem() ⚊4 ✅   evl_close_sem() ▽ ✅    Observable services   Function name EVL Switch1 non-EVL2   evl_create_observable() ▽ ✅   evl_new_observable() ▽ ✅   evl_read_observable() △5 ✅   evl_update_observable() △5 ✅    Polling services   Function name EVL Switch1 non-EVL2   evl_new_poll() ▽ ✅   evl_add_pollfd() △ ✕   evl_mod_pollfd() △ ✕   evl_del_pollfd() △ ✕   evl_poll_sem() △ ✕   evl_timedpoll_sem() △ ✕    Memory heap services   Function name EVL Switch1 non-EVL2   evl_init_heap() ▽ ✅   evl_extend_heap() ⚊ ✕   evl_alloc_block() △3 ✕   evl_free_block() △3 ✕   evl_check_block() △3 ✕   evl_destroy_heap() ▽ ✅   evl_heap_raw_size() ⚊ ✅   evl_heap_size() ⚊ ✅   evl_heap_used() ⚊ ✅    Proxy services   Function name EVL Switch1 non-EVL2   evl_create_proxy() ▽ ✅   evl_new_proxy() ▽ ✅   evl_send_proxy() ⚊ ✅   evl_vprint_proxy() ⚊ ✅   evl_print_proxy() ⚊ ✅   evl_printf() ⚊ ✅    Cross-buffer services   Function name EVL Switch1 non-EVL2   evl_create_xbuf() ▽ ✅   evl_new_xbuf() ▽ ✅    I/O services   Function name EVL Switch1 non-EVL2   oob_read() △ ✕   oob_write() △ ✕   oob_ioctl() △ ✕    Tube services   Function name EVL Switch1 non-EVL2   evl_init_tube() ⚊ ✅   evl_send_tube() ⚊ ✅   evl_receive_tube() ⚊ ✅   evl_get_tube_size() ⚊ ✅   evl_init_tube_rel() ⚊ ✅   evl_send_tube_rel() ⚊ ✅   evl_receive_tube_rel() ⚊ ✅   evl_get_tube_size_rel() ⚊ ✅    Misc routines   Function name EVL Switch1 non-EVL2   evl_init() N/A ✅   evl_get_version() ⚊ ✅   evl_sigdebug_handler() N/A ✅    1 Defines the stage switching behavior for EVL threads:\n  △ the core may promote the caller to the out-of-band execution stage if running in-band at the time of the call.\n  ▽ the core may demote the caller to the in-band execution stage if running out-of-band at the time of the call.\n  ⚊ the call does not entail any stage switch.\n  2 Whether this call is also available to non-EVL threads, i.e. threads not attached to the EVL core.\n3 Except if the caller undergoes the SCHED_WEAK policy, in which case it is switched back to in-band mode if it has released the last EVL mutex it holds by the end of the call.\n4 As an exception, if this synchronization object was statically initialized (EVL_*_INITIALIZER()), this routine may switch the caller to the in-band stage in order to finalize the construction before carrying out the requested operation. This is required only once in the object’s lifetime.\n5 If the caller undergoes the SCHED_WEAK policy, or is not attached to the core, this system call is directly handled from the in-band stage. In all other cases, the caller may be switched to the out-of-band execution stage.\n Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "libevl function index",
    "uri": "/ru/core/user-api/function_index/"
  },
  {
    "content": " ИнформацияПроект EVL теперь называется Xenomai 4. Ядро EVL изначально было развилкой ядра Xenomai 3 Cobalt, которое прошло значительную модернизацию, чтобы сделать его SMP-масштабируемым, более простым в освоении и обслуживании. Однако оба они по-прежнему придерживаются одних и тех же принципов, когда речь заходит о технике двойного ядра. Проект Xenomai внедряет интерфейс конвейера EVL Dovetail для своего предстоящего выпуска 3.2. Таким же образом EVL становится проектом Xenomai 4.\n   Архитектура с двумя ядрами. Как и его предшественники в серии Xenomai core, Xenomai 4 с ядром EVL предоставляет Linux возможности в реальном времени, встраивая в ядро сопутствующее ядро, которое специально предназначено для задач, требующих сверхнизкого и ограниченного времени отклика на события. По этой причине этот подход известен как двойная архитектура ядра, обеспечивающая строгие гарантии в реальном времени для некоторых задач наряду с богатыми сервисами операционной системы для других. В этой модели ядро общего назначения (Linux) и ядро реального времени (EVL) работают почти асинхронно, оба выполняют свой собственный набор задач, всегда отдавая последнему приоритет над первым.\n  Для достижения этой цели проект Xenomai 4 работает над тремя компонентами:\n  фрагмент внутреннего кода ядра - он же “Ласточкин хвост”, который действует как интерфейс между сопутствующим ядром и ядром общего назначения. Этот слой вводит этап выполнения с высоким приоритетом, на котором должны выполняться задачи с требованиями реального времени. Другими словами, с помощью кода “Ласточкин хвост” ядро Linux может планировать выполнение внеполосных задач в отдельном контексте выполнения, который не подчиняется общим формам сериализации, которым должна соответствовать работа общего назначения (например, маскировка прерываний, блокировки).\n  компактное и масштабируемое ядро реального времени, которое предназначено для использования в качестве эталонной реализации для других систем с двумя ядрами, основанных на Dovetail.\n  библиотека, известная как libevl, которая позволяет вызывать основные службы в реальном времени из приложений.\n  Первоначально кодовая база “Ласточкин хвост” ответвилась от интерфейса I-pipe еще в 2015 году, в основном для решения фундаментальных проблем обслуживания, которые касаются отслеживания самых последних выпусков ядра. Параллельно упрощенный вариант Cobalt core когда-то известный как ‘Steely’, был реализован для того, чтобы свободно экспериментировать с ласточкиным хвостом, поскольку последний значительно отличался от интерфейса I-pipe. В какой-то момент ‘Steely’ превратился в еще более простое, базовое и масштабируемое ядро реального времени, которое теперь служит эталонной реализацией для “Ласточкин хвост”, известное как EVL core, которое сегодня лежит в основе усилий Xenomai 4.\nКогда архитектура с двумя ядрами хорошо подходит? Система с двумя ядрами должна обеспечивать минимальное функциональное перекрытие между ядром общего назначения и сопутствующим ядром только для того, чтобы передать критическую по времени рабочую нагрузку выделенному компоненту, который прост и достаточно отделен от остальной системы, чтобы вы могли доверять. Типичные приложения, которые лучше всего обслуживаются такой инфраструктурой, должны получать данные с внешних устройств с небольшим дрожанием в течение нескольких десятых микросекунд (абсолютный наихудший случай) как только они будут доступны, обработайте такой ввод через потоки POSIX, которые могут удовлетворять требованиям в реальном времени, выполняя этап с высоким приоритетом, разгружая любую некритичную (по времени) работу в общие потоки. Вообще говоря, такой подход, как реализация ядра EVL, может хорошо подойти для этой работы в следующих случаях:\n  если вашему приложению требуется сверхнизкое время отклика и/или строго ограниченное дрожание надежным способом. Надёжным - это значит «не подверженным влиянию какого-либо кода ядра или пользовательского кода, ядро общего назначения способно работать параллельно таким образом, который может не допустить соблюдения жестких сроков в реальном времени». Допустимый код в этом случае означает, что он не вызывает сбоев машины; сопутствующее ядро двухъядерной системы не чувствительно к замедлениям, которые могут быть вызваны плохо написанными драйверами общего назначения. Например, рабочая нагрузка с низким приоритетом способна создать нагрузку на подсистему кэширования ЦП, вызывая задержки для действий в реальном времени, когда она возобновляется для обработки какого-либо внешнего события:\n  если эта рабочая нагрузка непрерывно обрабатывает большой набор данных, что приводит к частым удалениям кэша. Поскольку внешний кэш в иерархии разделяется между процессорами, эффект пульсации существует для всех из них, включая изолированные.\n  если эта рабочая нагрузка включает в себя множество параллельных потоков, вызывающих высокую частоту переключения контекста, что может стать еще хуже, если эти потоки принадлежат разным процессам (т.е. Имеют разные адресные пространства).\nНебольшая площадь выделенного ядра помогает в этом случае, поскольку в управлении системой реального времени в целом задействовано меньше кода и данных, что снижает влияние неблагоприятных условий кэширования. Кроме того, маленькое ядро не обязано соблюдать правила блокировки, налагаемые на код ядра общего назначения при планировании своих потоков. Вместо этого он может в любое время опередить его, основываясь на методе конвейерная обработка прерываний, который реализует Ласточкин хвост. Эта часть информации описывает типичные ситуации во время выполнения, когда рабочая нагрузка общего назначения оказывает давление на всю систему, независимо от относительного приоритета ее задач.\n    если при проектировании вашей прикладной системы требуется, чтобы путь выполнения в реальном времени был логически изолирован от действий общего назначения путем построения, чтобы не использовать общие критические подсистемы, такие как общий планировщик.\n  если обращение к изоляции ЦПУ для смягчения неблагоприятных последствий, которые может оказать рабочая нагрузка, не связанная с реальным временем, на стороне реального времени, не является вариантом или после тестирования недостаточно подходит для вашего варианта использования. Очевидно, что использование такого трюка с недорогим оборудованием на одноядерном процессоре не сработает, поскольку по крайней мере один неизолированный процессор должен быть доступен ядру для выполнения обязанностей по обслуживанию системы.\n  если использование PREEMTRT не даёт желаемого результата.\n  Что требует от вашего приложения архитектура с двумя ядрами? Чтобы уложиться в сроки, архитектура с двумя ядрами требует, чтобы ваше приложение использовало исключительно выделенный интерфейс системных вызовов, реализуемый ядром реального времени. С ядром EVL этим API является libevl, или любой другой основанный на нем API высокого уровня, который соблюдает это правило. Любой обычный системный вызов Linux, выполняемый во время выполнения на этапе с высоким приоритетом , автоматически понизит вызывающего абонента до этапа с низким приоритетом, что приведет к потере гарантий в режиме реального времени в процессе.\n Это правило имеет последствия, например, при использовании C++, для чего требуется определить набор используемых классов и функций среды выполнения, которые могут быть доступны приложению.\n Тем не менее, ваше приложение может использовать любую службу из вашей любимой стандартной библиотеки C/C++ вне критического по времени контекста.\nВообще говоря, четкое разделение рабочей нагрузки в реальном времени от остальной части реализации в вашем приложении является ключевым. Наличие такого разделения должно быть эмпирическим правилом, независимо от типа подхода в реальном времени, в том числе с собственным упреждением, это крайне важно с двухъядерной архитектурой. Кроме того, это требует четкого определения (нескольких) интерфейсов, которые могут существовать между задачами реального времени и общего назначения, что, безусловно, правильно.\nОсновной единицей выполнения, которую ядро EVL распознает для предоставления своих услуг в пользовательском пространстве, является поток, что обычно переводится как Потоки POSIX в пространстве пользователя.\nКакой API следует использовать для реализации драйверов устройств реального времени? Ядро EVL экспортирует API ядра для написания драйверов, расширяя интерфейс драйвера устройства Linux, чтобы приложения могли извлекать выгоду из внеполосных служб, предоставляемых на этапе выполнения с высоким приоритетом.\nОсновной единицей выполнения, которую ядро EVL распознает для предоставления своих услуг в пространстве ядра, является kthread, который является обычным kthread Linux на стероидах EVL.\nКаков объем кода с двойным ядром? Все цифры, представленные на приведенных ниже диаграммах, были определены CLOC, поддерживая в сравнениях только исходные файлы языка C и ассемблера.\n Размер Dovetail в коде ядра. Размер кода интерфейса Dovetail добавленого в ядро составляет 7.8 Kloc по состоянию на v5.7-rc5. Большинство изменений происходит в универсальном ядре и коде драйвера, которые составляют 73% от общего числа. Остальное разделено на ARM, arm64 и x86-специфический код. Порт, зависящий от архитектуры, в среднем составляет менее 10% от общего количества портов.\n      Размер ядра EVL в коде ядра. Размер ядра EVL поверх Dovetail составляет 15,2 Kloc. 97% этого кода не зависит от архитектуры. Каждый порт архитектуры составляет 1% от всего остального, что в среднем составляет 163 строки кода. Это показывает, что “Ласточкин хвост” на самом деле отвечает за подавляющее большинство специфичной для архитектуры поддержки, в которой должно нуждаться сопутствующее ядро.\n      Общий объем кода с двумя ядрами. Общий объем системы с двумя ядрами EVL составляет 26 Kloc, что включает в себя “Ласточкин хвост”, ядро EVL и его драйверы, способные работать вне диапазона. Это составляет 0,13% от общей базы кода ядра по состоянию на v5.7-rc5.\n      Сравнение отпечатков I-pipe и Ласточкиного хвоста. Эти цифры сравнивают последнюю реализацию I-pipe доступную на сегодняшний день, на основе ядра v4.19.x с “Ласточкиным хвостом” для v5.7-rc5. Ласточкин хвост предоставляет дополнительные основные сервисы, такие как встроенная поддержка внеполосного планирования задач, которую, как следствие, сопутствующим ядрам не нужно реализовывать для каждой архитектуры ЦП. Код, специфичный для ARM, значительно меньше для Dovetail, благодаря лучшей интеграции логики конвейера прерываний в основное ядро.\n      Сравнение отпечатков ядра Xenomai 3 Cobalt и Xenomai 4 EVL. Эти цифры сравнивают Cobalt 3.1 с ядром EVL для ядра v5.7-rc5. Резкое сокращение объема кода, отображаемого в ядре EVL, в основном связано с тем, что основное внимание уделяется более простому, но гибкому набору функций и повторному использованию общей модели драйверов. Кроме того, большая часть кода, специфичного для архитектуры, обрабатывается Dovetail, в отличие от Cobalt, которому все еще приходится иметь дело с мелкими деталями переключения задач, такими как управление FPU.\n     Перенос Ласточкиного хвоста Если вы намерены перенести Ласточкин хвост на:\n  какой-то произвольный вкус ядра или выпуск.\n  неподдерживаемая аппаратная платформа.\n  другая архитектура процессора.\n  Тогда вы могли бы с пользой использовать следующую информацию:\n  прежде всего, процесс разработки EVL описан в этом документе. Вам понадобится эта информация, чтобы отслеживать разработку EVL, на которой основана ваша собственная работа.\n  подробная информация о переносе интерфейса “Ласточкин хвост” на другую архитектуру процессора для ядра Linux приведена здесь.\n  текущая коллекция « эмпирических правил » когда она доходит до разработки программного обеспечения поверх двухъядерной инфраструктуры EVL.\n  Реализация вашего собственного сопутствующего ядра Если вы планируете разработать собственное ядро для встраивания в ядро Linux для запуска потоков POSIX на этапе с высоким приоритетом, введённом Dovetail, вы можете использовать реализацию ядра EVL в качестве справочного кода в отношении взаимодействия вашей работы с ядром общего назначения. Чтобы помочь вам в дальнейшем в этой задаче, вы можете обратиться к следующим разделам этого сайта:\n  все разделы документации, упомянутые ранее о переносе Dovetail.\n  описание так называемой альтернативной схемы планирования схемы, по которой потоки ядра Linux и потоки POSIX могут получить доступ к этапу выполнения с высоким приоритетом, чтобы воспользоваться гарантиями планирования в реальном времени.\n  разрабатываемая серия технических документов, которая проведет вас через реализацию ядра EVL.\n  Запуск ядра EVL Рецепт для нетерпеливых   прочитайте этот документ о создании ядра EVL и libevl.\n  загрузите ядро с поддержкой EVL.\n  напишите свой первый код приложения, используя API libevl. Вы можете найти следующие биты полезными, особенно при обнаружении системы:\n  что влечет за собой инициализация приложения EVL.\n  как запустить потоки POSIX на этапе с высоким приоритетом.\n  какой контекст вызова подходит для каждой службы EVL из этого API.\n    калибровка и тестирование системы.\n  Для всех остальных из нас Процесс запуска ядра EVL в вашей целевой системе можно кратко описать следующим образом (нажмите на шаги, чтобы открыть соответствующую документацию).:\ngraph LR; S(\"Сборка libevl\") -- X[\"Установка libevl\"] style S fill:#99ccff; click S \"/core/build-steps#building-libevl\" X -- A[\"Сборка ядра\"] click A \"/core/build-steps#building-evl-core\" A -- B[\"Установка ядра\"] style A fill:#99ccff; B -- C[\"Загрузка цели\"] style C fill:#ffffcc; C -- U[\"Запуск проверки evl\"] style U fill:#ffffcc; click U \"/core/commands#evl-check-command\" U -- UU{OK?} style UU fill:#fff; UU --|Yes| D[\"Запуск тестов\"] UU --|No| R[Исправление Kconfig] click R \"/core/caveat\" style R fill:#99ccff; R -- A style D fill:#ffffcc; click D \"/core/testing#evl-unit-testing\" D -- L{OK?} style L fill:#fff; L --|Yes| E[\"Тест с 'hectic'\"] L --|No| Z[\"Отчёт в upstream\"] style Z fill:#ff420e; click E \"/core/testing#hectic-program\" click Z \"https://xenomai.org/mailman/listinfo/xenomai/\" E -- M{OK?} style M fill:#fff; M --|Yes| F[\"Калибровка таймера\"] M --|No| Z click F \"/core/runtime-settings#calibrate-core-timer\" style E fill:#ffffcc; F -- G[\"Тест с 'latmus'\"] style F fill:#ffffcc; style G fill:#ffffcc; click G \"/core/testing#latmus-program\" G -- N{OK?} style N fill:#fff; N --|Yes| O[\"Иди праздновать\"] N --|No| Z style O fill:#33cc66;  Как только ядро EVL запустится в вашей целевой системе,вы можете перейти непосредственно к приведённому здесь шагу быстрого рецепта.\n Последнее изменение: Fri, 22 Oct 2021 21:41:35 MSK ",
    "description": "",
    "tags": null,
    "title": "Обзор",
    "uri": "/ru/overview/"
  },
  {
    "content": "Построение EVL из исходного кода   Процесс сборки. Сборка EVL из исходного кода - это процесс в два этапа: мы должны собрать ядро Linux включив EVL ядро, и библиотеку реализующую API пользователя для этого ядра - т.е. libevl - с использованием соответствующего инструментария. Эти шаги могут выполняться в любом порядке. Результатом этого процесса является:\n  образ ядра Linux с изображением Dovetail и ядром EVL поверх него.\n  общая библиотека* libevl.so, которая позволяет приложениям запрашивать службы из ядра EVL, а также несколько основных утилит и тестовых программ.\n* Также создается статический архив libevl.a.\n    Получение источников Источники EVL хранятся в репозиториях GIT. В качестве предварительного шага вы можете взглянуть на процесс разработки EVL, чтобы определить, какие ветви GIT могут вас заинтересовать в этих репозиториях:\n  Дерево ядра с ядром EVL:\n git@git.xenomai.org:Xenomai/xenomai4/linux-evl.git https://git.xenomai.org/xenomai4/linux-evl.git    Дерево libevl, которое предоставляет пользовательский интерфейс для ядра:\n git@git.xenomai.org:Xenomai/xenomai4/libevl.git https://git.xenomai.org/xenomai4/libevl.git    Другие предварительные условия В дополнение к исходному коду нам понадобится:\n  набор инструментов GCC для целевой архитектуры процессора.\n  заголовки UAPI из целевого ядра Linux соответствуют ядру EVL. Каждый файл UAPI экспортирует набор определений и типов интерфейсов, которые совместно используются с libevl.so работающей в пространстве пользователя, чтобы последняя могла отправлять хорошо сформированные системные вызовы первому. Другими словами, при сборке libevl.so, нам нужен доступ к содержимому include/uapi/asm/ и include/uapi/evl/ из исходного дерева ядра, которое содержит ядро EVL, которое будет обрабатывать системные вызовы.\n  Вниманиеlibevl полагается на поддержку локального хранилища потоков (TLS), которая может быть нарушена в некоторых устаревших цепочках инструментов (ARM). Обязательно используйте текущий.\n Сборка ядра Как только появится ваш любимый инструмент настройки ядра, вы должны увидеть блок конфигурации EVL где-то внутри меню Общая настройка. Этот блок конфигурации выглядит следующим образом:\nВключения CONFIG_EVL должно быть достаточно для начала работы, значения по умолчанию для других параметров EVL безопасны в использовании. Вы должны убедиться, что также включены CONFIG_EVL_LATMUS и CONFIG_EVL_HECTIC; это драйверы, необходимые для запуска утилит latmus и hectic, доступных с libevl, которые измеряют задержку и проверяют правильность переключения контекста.\nСоветЕсли вы не знакомы с созданием ядер, этот документ может помочь. Если вы сталкиваетесь с препятствиями, возникающими непосредственно в дереве исходных текстов ядра, как показано в упомянутом документе, вы можете проверить, может ли работать построение вне дерева, поскольку именно так разработчики Dovetail/EVL обычно перестраивают ядра. Если что-то пойдет не так при построении внутри дерева или вне дерева, пожалуйста, отправьте заметку в список рассылки EVL с соответствующей информацией.\n Все основные параметры конфигурации   #kconfig { width: 100%; } #kconfig th { text-align: center; } #kconfig td { text-align: left; } #kconfig tr:nth-child(even) { background-color: #f2f2f2; } #kconfig td:nth-child(2) { text-align: center; }   Название символа По умолчанию Назначение   CONFIG_EVL N Включите ядро EVL   CONFIG_EVL_SCHED_QUOTA N Включите политику планирования на основе квот   CONFIG_EVL_SCHED_TP N Включите политику планирования с разделением по времени   CONFIG_EVL_SCHED_TP_NR_PART N Количество временных разделов для CONFIG_EVL_SCHED_TP   CONFIG_EVL_HIGH_PERCPU_CONCURRENCY N Оптимизирует реализацию для приложений с множеством потоков реального времени, работающих одновременно на любом заданном ядре процессора   CONFIG_EVL_RUNSTATS Y Сбор статистики времени выполнения о потоках   CONFIG_EVL_COREMEM_SIZE 2048 Размер кучи основной памяти (в килобайтах)   CONFIG_EVL_NR_THREADS 256 Максимальное количество потоков EVL   CONFIG_EVL_NR_MONITORS 512 Максимальное количество мониторов EVL (т.е. мьютексы + семафоры + флаги + события)   CONFIG_EVL_NR_CLOCKS 8 Максимальное количество часов EVL   CONFIG_EVL_NR_XBUFS 16 Максимальное количество кросс-буферов EVL   CONFIG_EVL_NR_PROXIES 64 Максимальное количество прокси-серверов EVL   CONFIG_EVL_NR_OBSERVABLES 64 Максимальное количество наблюдаемых EVL (не включает потоки)   CONFIG_EVL_LATENCY_USER 0 Предварительно установленное значение силы тяжести таймера ядра для пользовательских потоков (0 означает использование предварительно откалиброванного значения)   CONFIG_EVL_LATENCY_KERNEL 0 Предварительно установленное значение силы тяжести таймера ядра для потоков ядра (0 означает использование предварительно откалиброванного значения)   CONFIG_EVL_LATENCY_IRQ 0 Предварительно установленное значение силы тяжести основного таймера для обработчиков прерываний (0 означает использование предварительно откалиброванного значения)   CONFIG_EVL_DEBUG N Включить функции отладки   CONFIG_EVL_DEBUG_CORE N Включить утверждения отладки ядра   CONFIG_EVL_DEBUG_CORE N Включить утверждения отладки ядра   CONFIG_EVL_DEBUG_MEMORY N Включите проверки отладки в распределителе основной памяти. Этот параметр добавляет значительные накладные расходы, влияющие на показатели задержки   CONFIG_EVL_DEBUG_WOLI N Включить контрольные точки предупреждения о несогласованности при блокировке   CONFIG_EVL_WATCHDOG Y Включить сторожевой таймер   CONFIG_EVL_WATCHDOG_TIMEOUT 4 Значение тайм-аута сторожевого таймера (в секундах).   CONFIG_GPIOLIB_OOB n Включите поддержку внеполосных запросов на обработку линий GPIO.   CONFIG_SPI_OOB, CONFIG_SPIDEV_OOB n Включите поддержку внеполосных передач SPI.   Включение 32-разрядной поддержки в 64-разрядном ядре (CONFIG_COMPAT) Начиная с EVL ABI 20 серии v5.6, ядро EVL обычно позволяет 32-разрядным приложениям выполнять системные вызовы 64-разрядного ядра, когда поддерживаются как 32-, так и 64-разрядные архитектуры ЦП, такие как код ARM (он же Aarch32), работающий поверх ядра arm64 (Aarch64). Для arm64 вам необходимо включить CONFIG_COMPAT и CONFIG_COMPAT_VDSO в конфигурации ядра. Чтобы иметь возможность изменять последнее, переменная среды CROSS_COMPILE_COMPAT должна быть установлена в префикс 32-разрядной цепочки инструментов ARMv7, которая должна использоваться для компиляции vDSO (да, это довольно запутанно). Например:\n$ make \u003cyour-make-args\u003e ARCH=arm64 \\ \tCROSS_COMPILE=aarch64-linux-gnu- \\ \tCROSS_COMPILE_COMPAT=arm-linux-gnueabihf- (x|g|menu)config  СоветНапример, если вы планируете запускать EVL на любом из 64-разрядных компьютеров Raspberry PI, вам может оказаться полезным использовать 32-разрядные дистрибутивы Linux, ориентированные на PI, такие как Raspbian. Для этого обязательно включите CONFIG_COMPAT и CONFIG_COMPAT_VDSO для вашего ядра с поддержкой EVL, создав 32-разрядный vDSO, как упоминалось ранее.\n Создание libevl Общая команда для построения libevl - это:\n$ make [-C $SRCDIR] [ARCH=$cpu_arch] \\ \t[CROSS_COMPILE=$toolchain] UAPI=$uapi_dir \\ \t[OTHER_BUILD_VARS] [goal...]  Основные переменные построения\n    Переменная Описание     $SRCDIR Путь к этому исходному дереву   $cpu_arch Архитектура процессора, которую вы создаете для (‘arm’, ‘arm64’, ‘x86’)   $toolchain Необязательный префикс имени файла binutils (например, ‘arm-linux-gnueabihf-’, ‘aarch64-linux-gnu-')     Другие переменные построения\n    Переменная Описание По умолчанию     D={0|1} Disable or enable debug build, i.e. -g -O0 vs -O2 0   O=$output_dir Генерировать двоичные выходные файлы в $output_dir .   V={0|1} Установите уровень детализации сборки, 0 - краткость 0   DESTDIR=$install_dir Установите библиотеку и двоичные файлы в $install_dir /usr/evl     Установить цель\n    Цель Действие     all создайте все двоичные файлы (библиотеку, утилиты и тесты)   clean удалите файлы сборки   install сделайте всё, скопировав сгенерированные системные двоичные файлы в $DESTDIR в процессе   install_all установите, скопировав все сгенерированные двоичные файлы, включая tidbits    Перекрестная компиляция EVL Допустим, исходный код библиотеки находится в ~/git/libevl, а исходные тексты ядра с ядром EVL расположены в ~/git/linux-evl.\nПерекрестная компиляция EVL и установка полученной библиотеки и утилит в промежуточный каталог, расположенный по адресу /nfsroot/\u003cmachine\u003e/usr/evl , будет означать следующее:\n Перекрестная компиляция из отдельного каталога сборки\n # Сначала создайте каталог сборки, в который должны помещаться выходные файлы $ mkdir /tmp/build-imx6q \u0026\u0026 cd /tmp/build-imx6q # Затем запустите процесс сборки+установки $ make -C ~/git/libevl O=$PWD ARCH=arm \\ \tCROSS_COMPILE=arm-linux-gnueabihf- \\ \tUAPI=~/git/linux-evl \\ \tDESTDIR=/nfsroot/imx6q/usr/evl install или,\n Перекрестная компиляция из дерева исходных текстов библиотеки EVL\n $ mkdir /tmp/build-hikey $ cd ~/git/libevl $ make O=/tmp/build-hikey ARCH=arm64 \\ \tCROSS_COMPILE=aarch64-linux-gnu- \\ \tUAPI=~/git/linux-evl \\ \tDESTDIR=/nfsroot/hikey/usr/evl install  ЗаметкаРекомендуется всегда создавать выходные файлы сборки в отдельный каталог сборки с помощью директивы O= в командной строке make, чтобы не загромождать ими дерево исходных текстов. Создание вывода в отдельный каталог также создает удобные файлы создания на лету в дереве вывода, которые вы можете использовать для запуска последующих сборок без необходимости снова упоминать весь ряд переменных и параметров в командной строке make.\n Сборка EVL в родной среде И наоборот, вы можете захотеть создать EVL изначально в целевой системе. Установка полученной библиотеки и утилит непосредственно в их конечный каталог, расположенный, например в /usr/evl, может быть выполнена следующим образом:\n Построение изначально из каталога сборки\n $ mkdir /tmp/build-native \u0026\u0026 cd /tmp/build-native $ make -C ~/git/libevl O=$PWD UAPI=~/git/linux-evl DESTDIR=/usr/evl install или,\n Построение изначально из дерева исходных текстов библиотеки EVL\n $ mkdir /tmp/build-native $ cd ~/git/libevl $ make O=/tmp/build-native UAPI=~/git/linux-evl DESTDIR=/usr/evl install Тестирование установки На этом этапе вы действительно захотите протестировать установку EVL.\n Последнее изменение: Sat, 06 Nov 2021 18:40:17 MSK content/core/build-steps.md 1e166ad ",
    "description": "",
    "tags": null,
    "title": "Сборка EVL",
    "uri": "/ru/core/build-steps/"
  },
  {
    "content": "EVL provides support for running high frequency SPI transfers which are useful in implementing closed-loop control systems. Applications manage the out-of-band transfers from user space via requests sent to the SPIDEV driver, which exports a user-space API to reach the SPI devices overs a given bus. To this end, EVL makes a few of strong assumptions:\n  a DMA is available for transferring the data over the SPI bus between the controller and the device. Since configuring the DMA cannot be done from the out-of-band stage, selecting the SPI device to talk to over the bus involves stopping the real-time operations, in order to switch to the in-band execution stage. Fortunately, most use cases which require high frequency, ultra-low latency transfers involve talking to a single device over a dedicated SPI bus. In this case, the device selection and DMA configuration need to be done only once, when setting up the communication.\n  the data to be exchanged with the SPI device is stored into a fixed-size memory buffer, shared between the kernel and the application. The buffer is split in two parts: an input area containing the data received from the remote device during the last transfer cycle, and an output area containing the data to be sent to that device during the same cycle. The shared memory is coherent, CPU cache is disabled. The DMA is managed in pulsed out-of-band mode from the SPIDEV interface.\n  while operating in out-of-band mode, the SPI bus cannot be used for regular in-band traffic.\n  only the SPI master mode is supported for out-of-band operations.\n  Enabling the out-of-band SPI capabilities is threefold, this requires:\n  adding the out-of-band management logic to the generic SPI master framework. This is done once and readily available from EVL.\n  having the SPIDEV driver export the out-of-band I/O interface to applications, which is also done once and readily available from EVL.\n  adding the needed bits to the SPI controller driver which manages the particular controller chip to be used for out-of-band I/O. This is the part you may have to implement for your own SPI controller of choice. The list of SPI controllers EVL currently extends with out-of-band capabilities is visible at this location.\n  Adding out-of-band capabilities to a SPI controller As stated in the introduction, the out-of-band I/O logic slips into the regular Linux device driver model as much as possible, without imposing a separate driver stack. In the SPI case, this means extending the generic SPI framework with a small set of operations which control the bus from the out-of-band execution stage. The handlers for those operations must be added to the struct spi_controller descriptor as follows:\n .prepare_oob_transfer should finish the setup for preparing a particular SPI bus for out-of-band transfers. This is called once, after the generic SPI framework has created the shared memory area, and configured the DMA. This is the right place to provide for any setup which is specific to out-of-band I/O, or any additional sanity check which is specific to the controller in such a context. For instance, the controller could make sure the size of the data frame to send/receive at each transfer is within the bounds supported by the hardware. This is an in-band operation, the SPI bus is locked for out-of-band traffic which means that regular in-band request to the same bus will have to wait until it leaves the out-of-band mode. This handler runs upon request from the application to enable out-of-band mode (SPI_IOC_ENABLE_OOB_MODE) for a given SPI bus.   Example: the prepare_oob_transfer() handler for the BCM2835 chip\n static int bcm2835_spi_prepare_oob_transfer(struct spi_controller *ctlr, struct spi_oob_transfer *xfer) { /* * The size of a transfer is limited by DLEN which is 16-bit * wide, and we don't want to scatter transfers in out-of-band * mode, so cap the frame size accordingly. */ if (xfer-\u003esetup.frame_len \u003e 65532) return -EINVAL; return 0; }  .start_oob_transfer should select the SPI device to talk to, set the SPI communication settings in the controller (e.g. speed, word size), then turn on the DMA operations on the configured channels Since the DMA is set in pulsed mode, no transfer takes place yet, but once start_oob_transfer() has returned, everything should be ready to trigger them simply by pulsing the DMA. Like prepare_oob_transfer(), this is an in-band operation which is invoked by the SPIDEV driver, upon request from the application (SPI_IOC_ENABLE_OOB_MODE). The bus is first prepared for out-of-band operations, before these are effectively started.   Example: the start_oob_transfer() handler for the BCM2835 chip\n static void bcm2835_spi_start_oob_transfer(struct spi_controller *ctlr, struct spi_oob_transfer *xfer) { struct bcm2835_spi *bs = spi_controller_get_devdata(ctlr); struct spi_device *spi = xfer-\u003espi; u32 cs = bs-\u003eprepare_cs[spi-\u003echip_select], effective_speed_hz; unsigned long cdiv; /* See bcm2835_spi_prepare_message(). */ bcm2835_wr(bs, BCM2835_SPI_CS, cs); cdiv = bcm2835_get_clkdiv(bs, xfer-\u003esetup.speed_hz, \u0026effective_speed_hz); xfer-\u003eeffective_speed_hz = effective_speed_hz; bcm2835_wr(bs, BCM2835_SPI_CLK, cdiv); bcm2835_wr(bs, BCM2835_SPI_DLEN, xfer-\u003esetup.frame_len); if (spi-\u003emode \u0026 SPI_3WIRE) cs |= BCM2835_SPI_CS_REN; bcm2835_wr(bs, BCM2835_SPI_CS, cs | BCM2835_SPI_CS_TA | BCM2835_SPI_CS_DMAEN); }  .pulse_oob_transfer is called when the application asks for triggering the next transfer by a request to the SPIDEV driver (SPI_IOC_RUN_OOB_XFER). This handler is called to apply the controller-specific tweaks which might be needed before the generic SPI framework pulses the DMA, causing the I/O operation to take place. This is an out-of-band operation; what this handler may do is restricted to the set of calls available to the out-of-band execution stage (reading/writing a couple of I/O registers should be enough in most cases here).   Example: the pulse_oob_transfer() handler for the BCM2835 chip\n static void bcm2835_spi_pulse_oob_transfer(struct spi_controller *ctlr, struct spi_oob_transfer *xfer) { struct bcm2835_spi *bs = spi_controller_get_devdata(ctlr); /* Reload DLEN for the next pulse. */ bcm2835_wr(bs, BCM2835_SPI_DLEN, xfer-\u003esetup.frame_len); }  .terminate_oob_transfer should stop and disable out-of-band DMA operations for the controller. This handler is called when the generic SPI framework was told by the SPIDEV driver to leave the out-of-band management mode for the controller (SPI_IOC_DISABLE_OOB_MODE). This is an in-band operation. Once the out-of-band mode is left, the bus is available for regular in-band traffic anew.   Example: the terminate_oob_transfer() handler for the BCM2835 chip\n static void bcm2835_spi_reset_hw(struct bcm2835_spi *bs) { u32 cs = bcm2835_rd(bs, BCM2835_SPI_CS); /* Disable SPI interrupts and transfer */ cs \u0026= ~(BCM2835_SPI_CS_INTR | BCM2835_SPI_CS_INTD | BCM2835_SPI_CS_DMAEN | BCM2835_SPI_CS_TA); /* * Transmission sometimes breaks unless the DONE bit is written at the * end of every transfer. The spec says it's a RO bit. Either the * spec is wrong and the bit is actually of type RW1C, or it's a * hardware erratum. */ cs |= BCM2835_SPI_CS_DONE; /* and reset RX/TX FIFOS */ cs |= BCM2835_SPI_CS_CLEAR_RX | BCM2835_SPI_CS_CLEAR_TX; /* and reset the SPI_HW */ bcm2835_wr(bs, BCM2835_SPI_CS, cs); /* as well as DLEN */ bcm2835_wr(bs, BCM2835_SPI_DLEN, 0); } static void bcm2835_spi_terminate_oob_transfer(struct spi_controller *ctlr, struct spi_oob_transfer *xfer) { struct bcm2835_spi *bs = spi_controller_get_devdata(ctlr); bcm2835_spi_reset_hw(bs); } Eventually, the SPI controller should fill the corresponding function pointers into its descriptor with the address of the out-of-band handlers.\n Example: declaring the out-of-band handlers for the BCM2835 chip\n static int bcm2835_spi_probe(struct platform_device *pdev) { struct spi_controller *ctlr; struct bcm2835_spi *bs; int err; ctlr = devm_spi_alloc_master(\u0026pdev-\u003edev, ALIGN(sizeof(*bs), dma_get_cache_alignment())); if (!ctlr) return -ENOMEM; platform_set_drvdata(pdev, ctlr); ... ctlr-\u003eprepare_oob_transfer = bcm2835_spi_prepare_oob_transfer; ctlr-\u003estart_oob_transfer = bcm2835_spi_start_oob_transfer; ctlr-\u003epulse_oob_transfer = bcm2835_spi_pulse_oob_transfer; ctlr-\u003eterminate_oob_transfer = bcm2835_spi_terminate_oob_transfer; ... }  Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "SPI",
    "uri": "/ru/core/oob-drivers/spi/"
  },
  {
    "content": "Dealing with GPIOs from the out-of-band execution stage enables the application to always respond to external signals within a few microseconds regardless of the in-band workload running in parallel on the system. Enabling CONFIG_GPIOLIB_OOB in the kernel configuration turns on such such capability in the regular GPIOLIB driver, which depends on the EVL core. The out-of-band GPIO support is available to applications using a couple of additional I/O requests to the character device interface exported by this driver to application code running in user-space (i.e. based on ioctl(2) and read(2) calls for the most part). This inner interface is tersely documented, but you may find your way by having a look at this demo code available from the mainline kernel tree.\nСоветThis driver interface is used by the libgpiod API internally.\n Out-of-band GPIO operations in a nutshell As stated in the introduction, the out-of-band I/O logic slips into the regular Linux device driver model as much as possible, without imposing a separate driver stack. In the GPIO case, we have been able to add the out-of-band support to an existing driver such as the GPIOLIB core instead of providing a dedicated driver.\nThis translates as follows:\n  Common GPIO handling operations in this driver are extended with the specific GPIOHANDLE_REQUEST_OOB flag, which tells the GPIOLIB core about our intent to operate a GPIO pin directly from the out-of-band execution stage, for input and/or output. Line set up and configuration are still done using the regular ioctl(2) interface, since these are in-band operations by design.\n  Waiting for a GPIO event is done by calling the oob_read() I/O service, instead of read(2).\n  Toggling a GPIO state is done by calling the oob_ioctl() I/O service, instead of ioctl(2).\n  Out-of-band line event and line handle requests In order to use the out-of-band GPIO features, one simply needs to add the GPIOHANDLE_REQUEST_OOB flag defined by the EVL core to the common GPIOHANDLE_REQUEST_INPUT, GPIOHANDLE_REQUEST_OUTPUT operation flags, when issuing the GPIO_GET_LINEEVENT_IOCTL and GPIO_GET_LINEHANDLE_IOCTL ioctl(2) requests respectively. For instance, this is a fragment of code adapted from the latmus application which measures response time to GPIO events:\n#include \u003csys/ioctl.h\u003e #include \u003clinux/gpio.h\u003e #include \u003cuapi/evl/devices/gpio.h\u003e static void setup_gpio_pins(int *fds) { struct gpiohandle_request out; struct gpioevent_request in; int ret; in.handleflags = GPIOHANDLE_REQUEST_INPUT | GPIOHANDLE_REQUEST_OOB; in.eventflags = GPIOEVENT_REQUEST_RISING_EDGE; in.lineoffset = gpio_inpin; /* Input pin number */ strcpy(in.consumer_label, \"latmon-pulse\"); ret = ioctl(gpio_infd, GPIO_GET_LINEEVENT_IOCTL, \u0026in); if (ret) error(1, errno, \"ioctl(GPIO_GET_LINEEVENT_IOCTL)\"); /* in.fd now contains the oob-capable line event descriptor. */ out.lineoffsets[0] = gpio_outpin; /* Output pin number */ out.lines = 1; out.flags = GPIOHANDLE_REQUEST_OUTPUT | GPIOHANDLE_REQUEST_OOB; out.default_values[0] = 1; strcpy(out.consumer_label, \"latmon-ack\"); ret = ioctl(gpio_outfd, GPIO_GET_LINEHANDLE_IOCTL, \u0026out); if (ret) error(1, errno, \"ioctl(GPIO_GET_LINEHANDLE_IOCTL)\"); /* out.fd now contains the oob-capable line handle descriptor. */ fds[0] = in.fd; fds[1] = out.fd; } Once a file descriptor is obtained on the GPIO controller - like /dev/gpiochip0 - for input (gpio_infd) and output (gpio_outfd), the application may ask for:\n  a line event descriptor for receiving GPIO interrupts directly from the out-of-band stage, by waiting on the oob_read() I/O service.\n  a line handle descriptor for changing the state of GPIO pins directly from the out-of-band stage, by calling the oob_ioctl() I/O service, with the GPIOHANDLE_SET_LINE_VALUES_IOCTL request code.\n  A thread which responds to GPIO events on an input pin by flipping the state of an output pin, all from the out-of-band stage could look like this:\nstatic void *gpio_responder_thread(void *arg) { struct gpiohandle_data data = { 0 }; struct gpioevent_data event; const int ackval = 0;\t/* Remote observes falling edges. */ int fds[2], efd, ret; setup_gpio_pins(fds); /* * Attach to the EVL core so that we may issue out-of-band * requests. */ efd = evl_attach_self(\"/gpio-responder:%d\", getpid()); if (efd \u003c 0) error(1, -efd, \"evl_attach_self() failed\"); /* * Loop: wait for the next event, then trigger an edge by flipping * the pin state. */ for (;;) { data.values[0] = !ackval; ret = oob_ioctl(fds[1], GPIOHANDLE_SET_LINE_VALUES_IOCTL, \u0026data); if (ret) error(1, errno, \"ioctl(GPIOHANDLE_SET_LINE_VALUES_IOCTL) failed\"); /* Wait for the next interrupt on the input pin. */ ret = oob_read(fds[0], \u0026event, sizeof(event)); if (ret != sizeof(event)) break; /* Start flipping the output pin. */ data.values[0] = ackval; ret = oob_ioctl(fds[1], GPIOHANDLE_SET_LINE_VALUES_IOCTL, \u0026data); if (ret) error(1, errno, \"ioctl(GPIOHANDLE_SET_LINE_VALUES_IOCTL) failed\"); } return NULL; }  Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "GPIO",
    "uri": "/ru/core/oob-drivers/gpio/"
  },
  {
    "content": "Калибровка основного таймера При включении в ядре EVL прозрачно управляет аппаратным чипом таймера через прокси-устройство, обслуживающего все запросы синхронизации, включая запросы, исходящие из внутриполосной логики ядра. Чтобы максимально повысить точность синхронизации, EVL необходимо определить базовую задержку целевой платформы.\nПосле получения прерывания, время которое тратится на прохождение кода ядра от кода ввода низкого уровня до вызова обработчика прерываний, установленного каким-либо драйвером, меньше, чем время, которое потребовалось бы для возобновления потока ядра при таком событии. Для возобновления потока пользовательского пространства потребуется еще больше времени, поскольку это может повлечь за собой изменение текущего адресного пространства памяти, что может повлечь за собой трудоемкие операции, связанные с MMU, влияющие на кэш ЦПУ.\nЭта базовая задержка может быть увеличена несколькими факторами, такими как:\n задержка кэша шины или процессора, задержка, необходимая для программирования микросхемы таймера для следующего выстрела, код, выполняемый с отключенными прерываниями на процессоре для получения IRQ, межпроцессорная сериализация в ядре EVL (hard spinlocks).  Чтобы доставить события как можно ближе к идеальному времени, EVL определяет понятие clock gravity, которое представляет собой набор статических значений регулировки для учета базовой задержки целевой системы для реагирования на события таймера с любого данного тактового устройства, как это воспринимается клиентским кодом, ожидающим этих событий пробуждения. По этой причине гравитация часов (clock gravity) определяется как триплет значений, который указывает время, в течение которого следует ожидать каждого выстрела таймера, в зависимости от целевого контекста, который он должен активировать, обработчика IRQ, ядра или пользовательского потока.\nПри запуске с опцией -t команда latmus выполняет серию тестов для определения наилучших значений калибровки для таймера ядра EVL, а затем сообщает ядру, чтобы оно их использовало.\nТипичный вывод этой команды выглядит следующим образом:\n Полная калибровка основного таймера\n # latmus -t == latmus started for core tuning, period=1000 us (may take a while) irq gravity...2000 ns kernel gravity...5000 ns user gravity...6500 ns == tuning completed after 34s Возможно, вам захочется ограничить процесс калибровки определенным контекстом (контекстами), и в этом случае вам следует передать соответствующие модификаторы контекста команде latmus, такие как -u для пользовательского пространства и -i для задержки IRQ соответственно:\n Калибровка в зависимости от контекста\n # latmus -tui == latmus started for core tuning, period=1000 us (may take a while) irq gravity...1000 ns user gravity...6000 ns == tuning completed after 21s  ЗаметкаВы можете получить триплеты ‘гравитации’, отличающиеся от нескольких микросекунд между запусками одного и того же процесса калибровки: это нормально, и не о чем беспокоиться, если все эти значения выглядят достаточно близко к ожидаемому дрожанию в целевой системе. Причина таких расхождений заключается в том, что, хотя latmus выполняет одни и те же тесты снова и снова, условия в целевой системе могут отличаться между запусками, что приводит к незначительным изменениям результатов (например, изменения производительности кэша процессора для цикла калибровки).\n  Последнее изменение: Wed, 03 Nov 2021 19:00:36 MSK %!s(\u003cnil\u003e) 1e166ad ",
    "description": "",
    "tags": null,
    "title": "Конфигурация времени выполнения",
    "uri": "/ru/core/runtime-settings/"
  },
  {
    "content": "EVL comes with a series of tests you can run to make sure the core is performing correctly on your target system.\nEVL поставляется с серией тестов, которые вы можете выполнить, чтобы убедиться, что ядро работает правильно в вашей целевой системе.\nТекст Code в обратных одинарных кавычках\n```sh for val in $(var); do echo $val done h2 Модульное тестирование Кода h3 Модульное тестирование Code h4 Модульное тестирование Code h5 Модульное тестирование Code h6 Модульное тестирование Code  link to site Ссылка на сайт Пермалинк hectic relref latmus relurl latmus2 qrelref  relref: “/core/testing.md#latmus-program” /ru/core/testing/  relref: “/core/_index.md#latmus-program” /ru/core/#latmus-program  relref: “/core/testing.md#latmus-program77” /ru/core/testing/  relurl: “/core/testing.md#latmus-program” /ru/core/testing.md#latmus-program  relurl: “/core/caveat.md#caveat-cpufreq” It is bad url /ru/core/caveat.md#caveat-cpufreq  flowchart LR; S(\"Unit testing\") -- X[\"hectic\"] style S fill:#99ccff; click S \" /ru/core/testing.md#evl-unit-testing\" click X \" /ru/core/testing.md#hectic-program\" X -- A[\"latmus\"] click A href \"/ru/core/testing/#latmus-program\" A -- B[\"Benchmarking\"] click B href \"/ru/core/benchmarks/\" B -- BA([\"irq response time\"]) click BA href \"/ru/core/benchmarks/#measuring-irq-response-time\" B -- BB([\"timer response time\"]) click BB href \"\thttps://the-going.github.io/evlproject//ru/core/benchmarks/_index#latmus-timer-response-time\"   A series of unit testing programs is produced in $prefix/tests as part of building libevl. You should run each of them to make sure everything is fine. The simplest way to do this is as follows:\n Running the EVL unit tests\n python mk R patch Bash C  print(\"Hello World!\")   # evl test duplicate-element: OK monitor-pp-dynamic: OK monitor-pi: OK clone-fork-exec: OK clock-timer-periodic: OK poll-close: OK sem-wait: OK monitor-pp-raise: OK monitor-pp-tryenter: OK heap-torture: OK monitor-pp-lower: OK poll-read: OK monitor-deadlock: OK monitor-wait-multiple: OK   \u003e print(\"Hello World!\") monitor-event: OK proxy-eventfd: OK monitor-flags.eshi: OK monitor-wait-multiple.eshi: OK sem-wait.eshi: OK detach-self.eshi: OK sem-timedwait.eshi: OK proxy-pipe.eshi: OK clock-timer-periodic.eshi: OK proxy-eventfd.eshi: OK monitor-event.eshi: OK heap-torture.eshi: OK poll-sem.eshi: OK poll-nested.eshi: OK sem-close-unblock: OK   --- a/include/asm-generic/atomic.h +++ b/include/asm-generic/atomic.h @@ -80,9 +80,9 @@ static inline void atomic_##op(int i, atomic_t *v)\t\\  {\t\\ unsigned long flags;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\  v-\u003ecounter = v-\u003ecounter c_op i;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\  } #define ATOMIC_OP_RETURN(op, c_op)\t\\ @@ -91,9 +91,9 @@ static inline int atomic_##op##_return(int i, atomic_t *v)\t\\  unsigned long flags;\t\\ int ret;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\  ret = (v-\u003ecounter = v-\u003ecounter c_op i);\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\  \\ return ret;\t\\ } @@ -104,10 +104,10 @@ static inline int atomic_fetch_##op(int i, atomic_t *v)\t\\  unsigned long flags;\t\\ int ret;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\  ret = v-\u003ecounter;\t\\ v-\u003ecounter = v-\u003ecounter c_op i;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\  \\ return ret;\t\\ }   #!/bin/bash # ####################################### usage() { info_pr \" used: $0mg - make git in to /tmp as tmpfs $0ps - aplly patches series $0evl - for testing make libevl $0rpm - make rpmbuild insert *.spec file $0mk - make kernel config = TODO = $0cc - ccache parametrs $0br - branding_evl for rpmbuild \" exit } ######################################### get_up_libevl() { if [ -f $libevl_tree/.git/config ] ; then git -C $libevl_tree pull else git -C $SCRATCH_AREA clone $URL_libevl fi }   #include \u003cevl/mutex.h\u003e static struct evl_mutex mutex; void create_new_mutex(void) { int fd; /* Create a (private) non-recursive mutex with priority inheritance enabled. */ fd = evl_create_mutex(mutex, EVL_CLOCK_MONOTONIC, 0, EVL_MUTEX_NORMAL, \"name_of_mutex\"); /* skipping checks */ return fd; } void arch_do_IRQ_pipelined(struct irq_desc *desc) { struct pt_regs *regs = raw_cpu_ptr(\u0026irq_pipeline.tick_regs); unsigned int irq = irq_desc_get_irq(desc); #ifdef CONFIG_SMP \t/* * Check for IPIs, handing them over to the specific dispatch * code. */ if (irq \u003e= OOB_IPI_BASE \u0026\u0026 irq \u003c OOB_IPI_BASE + NR_IPI + OOB_NR_IPI) { __handle_IPI(irq - OOB_IPI_BASE, regs); return; } #endif  do_domain_irq(irq, regs); }     In the example above, sched-quota-accuracy failed because CONFIG_EVL_SCHED_QUOTA was not set in the kernel configuration. Likewise, sched-tp-accuracy requires CONFIG_EVL_SCHED_TP to be enabled in the kernel configuration.\nВниманиеThe test loop aborts immediately upon a test failure. You may disable this behavior by running evl test -k (i.e. keep going) instead.\n СоветThe test loop aborts immediately upon a test failure. You may disable this behavior by running evl test -k (i.e. keep going) instead.\n Информация в заголовке может быть введенаЦикл тестирования прерывается сразу же после сбоя теста. Вы можете отключить это поведение, запустив вместо этого evl test -k (т.е. keep going).\n ЗаметкаTo get this test running, you will need CONFIG_EVL_HECTIC to be enabled in the kernel configuration, and loaded into the kernel under test if you built it as a dynamic module.\n hectic: hammering the EVL context switching machinery By default, the hectic program runs a truckload of EVL threads both in user and kernel spaces, for exercising the scheduler of the autonomous core. In addition, this test can specifically stress the floating-point management code to make sure the FPU is shared flawlessly between out-of-band and in-band thread contexts.\nЗаметкаTo get this test running, you will need CONFIG_EVL_HECTIC to be enabled in the kernel configuration, and loaded into the kernel under test if you built it as a dynamic module.\n latmus: the litmus test for latency СоветIf you plan for measuring the worst case latency on your target system, you should run the evl check command on such system in order to detect any obvious misconfiguration of the kernel early on.\n With the sole -m option or without any argument, the latmus application runs a 1Khz sampling loop, collecting the min, max and average latency values obtained for an EVL thread running in user-space which responds to timer events. This is a basic latency benchmark which does not require any additional interrupt source beyond the on-chip hardware timer readily available to the kernel.\nIn addition, you can use this application to measure the response time of a thread running in user-space to external interrupts, specifically to GPIO events. This second call form is selected by the -Z and -z option switches.\nFinally, passing -t starts a calibration of the EVL core timer, finding the best configuration values.\nСоветUnless you only plan to measure in-band response time to GPIO events, you will need CONFIG_EVL_LATMUS to be enabled in the kernel configuration to run the timer calibration or the response to timer test. This driver must be loaded into the kernel under test if you built it as a dynamic module. For those familiar with Xenomai 3 Cobalt, this program combines and extends the features of the latency and autotune utilities.\n latmus accepts the following arguments, given as short or long option names:\n-i --irqCollect latency figures or tune the EVL core timer from the context of an in-kernel interrupt handler.\n\n-k --kernelCollect latency figures or tune the EVL core timer from the context of a kernel-based EVL thread.\n\n-u --userCollect latency figures or tune the EVL core timer from the context of an EVL thread running in user-space. This is the default mode, in absence of -i and -k.\n\nflowchart LR; S(\"Сборка libevl\") -- X[\"Установка libevl\"] style S fill:#99ccff; click S \"/ru/core/build-steps/#building-libevl\" X -- A[\"Сборка ядра\"] click A href \"\thttps://the-going.github.io/evlproject//ru/core/build-steps#building-evl-core%!s(\u003cnil\u003e)\" A -- B[\"Установка ядра\"] style A fill:#99ccff; B -- C[\"Загрузка цели\"] style C fill:#ffffcc; C -- U[\"Запуск проверки evl\"] style U fill:#ffffcc; click U href \"/ru/core/commands/#evl-check-command\" U -- UU{OK?} style UU fill:#fff; UU --|No| R[Исправление Kconfig] UU --|Yes| D[\"Запуск тестов\"] click R href \"/ru/core/caveat/\" style R fill:#99ccff; R -- A style D fill:#ffffcc; click D \"/ru/core/testing/#evl-unit-testing\" D -- Тестирование subgraph \"Тестирование\" L{OK?} style L fill:#fff; L --|Yes| E[\"Тест с 'hectic'\"] L --|No| Z[\"Отчёт в upstream\"] style Z fill:#ff420e; click E \"/core/testing#hectic-program\" click Z \"https://xenomai.org/mailman/listinfo/xenomai/\" E -- M{OK?} style M fill:#fff; M --|Yes| F[\"Калибровка таймера\"] M --|No| Z click F \"/core/runtime-settings#calibrate-core-timer\" style E fill:#ffffcc; F -- G[\"Тест с 'latmus'\"] style F fill:#ffffcc; style G fill:#ffffcc; click G \"/core/testing#latmus-program\" G -- N{OK?} style N fill:#fff; N --|Yes| O[\"Иди праздновать\"] N --|No| Z style O fill:#33cc66; end  flowchart LR subgraph TOP subgraph B1 i1 --f1 end subgraph B2 i2 --f2 end end A -- TOP -- B B1 o==o B2   Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Протестируем рендер страницы",
    "uri": "/ru/overview/testing-page/"
  },
  {
    "content": "EVL поставляется с серией тестов, которые вы можете выполнить, чтобы убедиться, что ядро работает правильно в вашей целевой системе.\nМодульное тестирование Серия программ модульного тестирования создается в $prefix/tests как часть построения libevl. Вы должны запустить каждый из них, чтобы убедиться, что все в порядке. Самый простой способ сделать это заключается в следующем:\n Запуск модульных тестов EVL\n # evl test duplicate-element: OK monitor-pp-dynamic: OK monitor-pi: OK clone-fork-exec: OK clock-timer-periodic: OK poll-close: OK sem-wait: OK monitor-pp-raise: OK monitor-pp-tryenter: OK heap-torture: OK monitor-pp-lower: OK poll-read: OK monitor-deadlock: OK monitor-wait-multiple: OK monitor-event: OK proxy-eventfd: OK monitor-flags.eshi: OK monitor-wait-multiple.eshi: OK sem-wait.eshi: OK detach-self.eshi: OK sem-timedwait.eshi: OK proxy-pipe.eshi: OK clock-timer-periodic.eshi: OK proxy-eventfd.eshi: OK monitor-event.eshi: OK heap-torture.eshi: OK poll-sem.eshi: OK poll-nested.eshi: OK sem-close-unblock: OK monitor-steal: OK basic-xbuf: OK simple-clone: OK monitor-flags: OK poll-sem: OK sem-timedwait: OK mapfd: OK proxy-pipe: OK poll-flags: OK poll-nested: OK monitor-pp-pi: OK fault: OK monitor-pi-deadlock: OK detach-self: OK monitor-pp-nested: OK monitor-pp-weak: OK stax-lock: OK fpu-preload: OK Несколько тестов из набора могут завершиться неудачей, если для их поддержки отсутствует какая-либо поддержка в ядре, например:\nsched-quota-accuracy.c:213: FAILED: evl_control_sched(44, \u0026p, \u0026q, test_cpu) (=Operation not supported) sched-quota-accuracy: no kernel support В приведенном выше примере ошибка с sched-quota-accuracy произошла из-за того, что параметр CONFIG_EVL_SCHED_QUOTA не была задан в конфигурации ядра. Аналогично, sched-tp-accuracy требует, чтобы CONFIG_EVL_SCHED_TP был включен в конфигурации ядра.\nСоветЦикл тестирования прерывается сразу же после сбоя теста. Вы можете отключить это поведение, запустив вместо этого evl test -k (т.е. keep going).\n hectic: биение механизма переключения контекста EVL По умолчанию программа hectic запускает множество потоков EVL как в пространстве пользователя, так и в пространстве ядра для выполнения планировщика автономного ядра. Кроме того, этот тест может специально подчеркнуть код управления с плавающей запятой, чтобы убедиться, что FPU безупречно распределяется между внешними и внутренними контекстами потоков.\nЗаметкаЧтобы запустить этот тест, вам потребуется включить “CONFIG_EVL_HECTIC” в конфигурации ядра и загрузить его в тестируемое ядро, если вы создали его как динамический модуль.\n # /usr/evl/bin/hectic -s 200 == Testing FPU check routines... == FPU check routines: OK. == Threads: switcher_ufps0-0 rtk0-1 rtk0-2 rtup0-3 rtup0-4 rtup_ufpp0-5 rtup_ufpp0-6 rtus0-7 rtus0-8 rtus_ufps0-9 rtus_ufps0-10 rtuo0-11 rtuo0-12 rtuo_ufpp0-13 rtuo_ufpp0-14 rtuo_ufps0-15 rtuo_ufps0-16 rtuo_ufpp_ufps0-17 rtuo_ufpp_ufps0-18 fpu_stress_ufps0-19 switcher_ufps1-0 rtk1-1 rtk1-2 rtup1-3 rtup1-4 rtup_ufpp1-5 rtup_ufpp1-6 rtus1-7 rtus1-8 rtus_ufps1-9 rtus_ufps1-10 rtuo1-11 rtuo1-12 rtuo_ufpp1-13 rtuo_ufpp1-14 rtuo_ufps1-15 rtuo_ufps1-16 rtuo_ufpp_ufps1-17 rtuo_ufpp_ufps1-18 fpu_stress_ufps1-19 switcher_ufps2-0 rtk2-1 rtk2-2 rtup2-3 rtup2-4 rtup_ufpp2-5 rtup_ufpp2-6 rtus2-7 rtus2-8 rtus_ufps2-9 rtus_ufps2-10 rtuo2-11 rtuo2-12 rtuo_ufpp2-13 rtuo_ufpp2-14 rtuo_ufps2-15 rtuo_ufps2-16 rtuo_ufpp_ufps2-17 rtuo_ufpp_ufps2-18 fpu_stress_ufps2-19 switcher_ufps3-0 rtk3-1 rtk3-2 rtup3-3 rtup3-4 rtup_ufpp3-5 rtup_ufpp3-6 rtus3-7 rtus3-8 rtus_ufps3-9 rtus_ufps3-10 rtuo3-11 rtuo3-12 rtuo_ufpp3-13 rtuo_ufpp3-14 rtuo_ufps3-15 rtuo_ufps3-16 rtuo_ufpp_ufps3-17 rtuo_ufpp_ufps3-18 fpu_stress_ufps3-19 RTT| 00:00:01 RTH|---------cpu|ctx switches|-------total RTD| 0| 568| 568 RTD| 3| 853| 853 RTD| 2| 739| 739 RTD| 1| 796| 796 RTD| 0| 627| 1195 RTD| 2| 1258| 1997 RTD| 3| 1197| 2050 RTD| 1| 1311| 2107 RTD| 0| 627| 1822 RTD| 2| 1250| 3247 RTD| 3| 1254| 3304 RTD| 1| 1254| 3361 RTD| 2| 1254| 4501 RTD| 1| 1254| 4615 RTD| 0| 684| 2506 RTD| 3| 1311| 4615 RTD| 3| 1256| 5871 RTD| 2| 1311| 5812 RTD| 0| 684| 3190 RTD| 1| 1311| 5926 ... latmus: лакмусовая бумажка для проверки латентности СоветЕсли вы планируете измерить задержку наихудшего случая в вашей целевой системе, вам следует выполнить команду evl check в этой системе, чтобы на ранней стадии обнаружить любую очевидную неправильную конфигурацию ядра.\n С единственной опцией -m или без каких-либо аргументов приложение latmus запускает цикл выборки 1 кГц, собирая минимальные, максимальные и средние значения задержки, полученные для потока EVL, работающего в пространстве пользователя, который реагирует на события таймера. Это базовый тест задержки, который не требует какого-либо дополнительного источника прерываний, кроме встроенного аппаратного таймера, легко доступного ядру.\nКроме того, вы можете использовать это приложение для измерения времени отклика потока, запущенного в пользовательском пространстве, на внешние прерывания, в частности на события GPIO. Эта вторая форма вызова выбирается переключателями опций -Z и -z.\nНаконец, прохождение с -t запускает калибровку таймера ядра EVL, находя наилучшие значения конфигурации.\nСоветЕсли вы не планируете измерять только внутриполосное время отклика на события GPIO, вам потребуется включить CONFIG_EVL_LATMUS в конфигурации ядра для запуска калибровки таймера или ответа на тест таймера. Этот драйвер должен быть загружен в тестируемое ядро, если вы создали его как динамический модуль. Для тех, кто знаком с Xenomai 3 Cobalt, эта программа сочетает в себе и расширяет возможности утилит latency и autotune.\n latmus принимает следующие аргументы, заданные в виде коротких или длинных имен параметров:\n-i --irqСоберите данные о задержке или настройте таймер ядра EVL из контекста обработчика прерываний в ядре.\n\n-k --kernelСоберите данные о задержке или настройте таймер ядра EVL из контекста потока EVL на основе ядра.\n\n-u --userСоберите данные о задержке или настройте таймер ядра EVL из контекста потока EVL, запущенного в пространстве пользователя. Это режим по умолчанию, при отсутствии -i и -k.\n\n-s --sirqИзмерьте задержку между моментом отправки синтетического прерывания с внеполосной стадии и моментом, когда оно в конечном итоге будет получено его внутриполосным обработчиком. При измерении под значительным давлением рабочей нагрузки это дает наихудшую задержку прерывания, с которой сталкивается внутриполосное ядро из-за отключения локального прерывания (т.е. stalling этапа внутриполосного конвейера). Следовательно, это не имеет ничего общего с гораздо более короткой и ограниченной задержкой прерывания, наблюдаемой на внеполосной стадии приложениями EVL.\n\n-r --resetСбросьте значения силы тяжести основного таймера EVL до заводских значений по умолчанию. Эти значения по умолчанию статически определяются кодом платформы EVL.\n\n-q --quietСведите детализацию теста к минимуму, когда вступит в силу только окончательный отчет о задержке. Для передачи этой опции требуется установить тайм-аут с помощью параметра -T.\n\n-b --backgroundЗапустите тест в фоновом режиме оболочки. Все выходные данные подавляются до окончательного отчета о задержке.\n\n-K --keep-goingПродолжайте выполнение при неожиданном переключении потока ответчика во внутриполосный режим. Обычно любое переключение в режим внутри полосы из потока, реагирующего на события таймера/GPIO, приведет к остановке выполнения с сообщением об ошибке, поскольку показатели задержки будут испорчены переходом в контекст, отличный от реального времени. Этот параметр указывает latmus продолжать работу независимо от этого; это имеет смысл только для целей отладки, при сборе показателей задержки из потока EVL, работающего в пользовательском пространстве (т.е. -u).\n\n-m --measureИзмерьте время отклика на события таймера. В дополнение к этой опции -i, -k и -u выбирают конкретный контекст измерения, -u применяется по умолчанию. Измерение времени отклика на события таймера является режимом по умолчанию при отсутствии параметров -t, -Z и -z в командной строке.\n\n-t --tuneЗапустите процедуру калибровки основного таймера. -i, -k и -u можно использовать для выбора определенного контекста настройки, все они применяются последовательно в противном случае. См. ниже. Этот вариант является взаимоисключающим с -m, -Z и -z.\n\n-p --period=\u003cмкс.\u003eУстановите период выборки в микросекундах \u003cмкс.\u003e. По умолчанию используется 1000 (один тик каждую миллисекунду или 1 кГц). Самый медленный период выборки составляет 1000000 (1 Гц).\n\n-T --timeout=\u003cduration\u003e[dhms]Продолжительность теста, исключая период прогрева в одну секунду. Этот параметр включает тайм-аут, который автоматически останавливает тест по истечении указанного времени выполнения. По умолчанию тест выполняется бесконечно или до тех пор, пока не будет нажата кнопка ^C. Продолжительность интерпретируется в соответствии с суффиксом модификатора как количество d(дней), m(минут), h(часов) или s(секунд). При отсутствии модификатора предполагаются секунды.\n\n-A --maxlat-abort=\u003cmaxlat\u003eАвтоматически прерывайте тест всякий раз, когда наблюдаемый показатель максимальной задержки превышает \u003cmaxlat\u003e.\n\n-v --verbose=\u003cуровень\u003eУстановите уровень детализации на \u003cуровень\u003e. Установка 0 идентична переходу в тихий режим с помощью -q. Любое ненулевое значение учитывается при настройке таймера ядра EVL (опция -t), чтобы контролировать объем отладочной информации, которую сопутствующий драйвер latmus отправляет в журнал ядра. По умолчанию 1, максимум 2.\n\n-l --lines=\u003cколичество\u003eУстановите количество строк результатов на странице. В режиме измерения (-m) новый заголовок результата выводится после каждых \u003cколичество\u003e строк результата.\n\n-g --plot=\u003cfile\u003eВыведите гистограмму собранных значений задержки в \u003cфайл\u003e в формате, который легко читается утилитой gnuplot.\n\n-H --histogram=\u003cячеек\u003eУстановите количество ячеек на гистограмме, каждая ячейка покрывает одну микросекунду дополнительной задержки от 1 до \u003cячеек\u003e микросекунд. Это значение используется только в том случае, если в командной строке указано -g. По умолчанию установлено значение 200, охватывающее до 200 микросекунд в худшем случае задержки, которая никогда не должна быть такой высокой на любой целевой платформе с EVL.\n\n-P --priority=\u003cprio\u003eУстановите приоритет планирования потока ответчика в классе SCHED_FIFO. Этот параметр имеет смысл только при сборе показателей задержки или настройке таймера ядра EVL из контекста потока EVL (т.е. -u или -k). По умолчанию установлено значение 90.\n\n-c --cpu=\u003cnr\u003eУстановите соответствие процессора потоку ответчика. Этот параметр имеет смысл только при сборе показателей задержки или настройке таймера ядра EVL из контекста потока EVL (т.е. -u или -k). Значение по умолчанию равно 0.\n\n-Z --oob-gpio=\u003chost\u003eЗапустите внеполосный тест, измеряющий время отклика на события GPIO с внеполосной стадии, т.е. полагаясь на возможности ядра EVL в режиме реального времени. Аргументом является имя хоста или IPv4-адрес удаленной платы, которая отслеживает время отклика от тестируемой платы, на которой запущено приложение latmus. Этот параметр должен быть связан с -I и -O, чтобы указать используемые названия GPIO чипа и номена PIN.\n\n-z --inband-gpio=\u003chost\u003eЗапустите внутриполосный тест, измеряющий время отклика на события GPIO в обычном внутриполосном режиме. Аргументом является имя хоста или IPv4-адрес удаленной платы, которая отслеживает время отклика от тестируемой платы, на которой запущено приложение latmus. Этот параметр должен быть связан с -I и -O, чтобы указать используемые названия GPIO чипа и номена PIN.\n\n-I --gpio-in=\u003cgpiochip-name\u003e,\u003cpin-number\u003e[,rising-edge|falling-edge]Укажите название GPIO чипа и номер pin, которые будут использоваться для приема импульсов GPIO с платы удаленного монитора. При необходимости вы можете выбрать, должны ли события GPIO запускаться по восходящему фронту (по умолчанию) или по падающему фронту сигналов GPIO. Этот параметр имеет смысл только тогда, когда -Z или -z также присутствуют в командной строке.\n\n-O --gpio-out=\u003cgpiochip-name\u003e,\u003cpin-number\u003eУкажите название GPIO чипа и номер pin, которые будут использоваться для отправки импульсов GPIO и подтверждения их на плате монитора. Этот параметр имеет смысл только тогда, когда -Z или -z также присутствуют в командной строке.\n\nСоветЕсли latmus при запуске завершается с ошибкой Invalid argument (недопустимый аргумент), дважды проверьте номер процессора, переданный в опции -c как аргумент, если он указан. Назначенный процессор должен быть частью внеполосного набора процессоров, известного ядру EVL. Проверьте этот файл /sys/devices/virtual/evl/control/cpus, чтобы узнать, какие процессоры входят в этот набор.\n  Последнее изменение: Sun, 07 Nov 2021 11:51:49 MSK content/core/testing.md c56874c ",
    "description": "",
    "tags": null,
    "title": "Тестирование установки",
    "uri": "/ru/core/testing/"
  },
  {
    "content": "Сила ядра EVL в реальном времени Для определенных типов приложений выгрузка определенного набора критически важных по времени задач в автономное программное ядро, встроенное в ядро Linux, может обеспечить наилучшую производительность при минимальных затратах на разработку и время выполнения по сравнению с наложением поведения в реальном времени на всю логику ядра, чтобы уложиться в сроки, установленные только для этих задач, как того требует модель native preemption.\nВ двух словах, проект Xenomai 4 посвящен внедрению простой, масштабируемой и надежной архитектуры с двумя ядрами для Linux, основанной на интерфейсе Dovetail для подключения высокоприоритетного программного ядра к основному ядру. Этот интерфейс демонстрируется ядром реального времени, предоставляющим базовые услуги приложениям с помощью простого API. Ядро EVL - это непрерывное развитие готовой к производству инфраструктуры реального времени, которая также может стать отправной точкой для других разновидностей выделенного программного ядра, встроенного в ядро Linux. Эта работа состоит из:\n  интерфейс Ласточкин хвост - Dovetail который вводит этап выполнения с высоким приоритетом в основную логику ядра, где выполняется функционально независимое программное ядро.\n  ядро EVL, которое обеспечивает надежные сервисы с низкой задержкой для приложений, которые должны соответствовать требованиям реального времени. Приложения разрабатываются с использованием общей модели программирования Linux.\n  подробная документация, которая охватывает как “Ласточкин хвост”, так и ядро EVL, с множеством перекрестных ссылок между ними, так что инженеры могут использовать ядро EVL для поддержки приложения в реальном времени, улучшать его или даже реализовывать собственное программное ядро по выбору поверх Dovetail на практическом примере.\n  Примечание переводчикаЗдесь и далее англоязычное написание слова Dovetail и его перевод “Ласточкин хвост” будут фигурировать в обоих вариантах случайным образом.\n То, что мы ищем:\n  Низкие затраты на проектирование и техническое обслуживание. Работа над EVL должна требовать только общих знаний о разработке ядра, а объем и сложность кода должны оставаться приемлемыми для небольших команд разработчиков (в настоящее время около 20 KLOC, что даже вдвое меньше размера ядра Xenomai 3 Cobalt.\n  Низкая стоимость выполнения. Надежное, сверхнизкое и ограниченное время отклика для рабочей нагрузки в реальном времени, в том числе на низкоуровневом одноядерном оборудовании с минимальными накладными расходами, оставляя много циклов процессора для одновременного выполнения рабочей нагрузки общего назначения.\n  Высокая масштабируемость. От одноядерных до высокопроизводительных многоядерных машин, выполняющих рабочие нагрузки в режиме реального времени параллельно с низкой и ограниченной задержкой. Выполнение этих рабочих нагрузок на изолированных процессорах значительно улучшает показатель наихудшей задержки в конфигурациях SMP, но если в вашем устройстве есть только один из них, ядро EVL все равно должно обеспечивать сверхнизкую и ограниченную задержку.\n  Небольшая конфигурация. Мы хотим, чтобы требовалось совсем немного настроек во время выполнения, для обеспечения того, чтобы рабочая нагрузка в режиме реального времени не зависела от обычной рабочей нагрузки общего назначения. После включения в конфигурации ядра Linux, ядро EVL должно быть готово к доставке.\n  Сделай это обычным, сделай это простым Ядро EVL - это специализированное программное ядро, встроенное в ядро, предоставляющее услуги в режиме реального времени приложениям с жесткими требованиями к времени выполнения. Это небольшое ядро построено как любая обычная функция ядра Linux, а не как внешнее расширение, наложенное поверх него. Dovetail играет здесь важную роль, поскольку он скрывает мелкие детали встраивания сопутствующего ядра в ядро Linux. Его довольно низкий объем кода и ограниченная сложность делают его хорошим выбором в качестве инфраструктуры реального времени “подключи и забудь”, которая также может быть использована в качестве отправной точки для пользовательских реализаций ядра. Следующие цифры были получены с помощью инструмента CLOC подсчитывающего строки исходного кода из RTAI, Xenomai 3 Cobalt и Xenomai 4 EVL реализации ядра соответственно:\nИнтерфейсом пользовательского пространства для этого ядра является библиотека EVL (libevl.so), которая реализует базовые оболочки системных вызовов, а также основные службы синхронизации потоков. Никаких наворотов и свистков, только основы. Цель состоит в том, чтобы обеспечить простые механизмы, а сложную семантику и политики, которые могут и должны быть реализованы на основе этой библиотеки, в API высокого уровня, работающего на стране пользователей.\nElements Как следует из названия, elements -это основные функции, которые могут потребоваться от ядра EVL для поддержки приложений реального времени в этой среде с двумя ядрами. Кроме того, только ядро EVL в состоянии эффективно предоставлять такие функции, чистый код пользовательского пространства не может их предоставить. Ядро EVL определяет шесть элементов:\n  Thread(поток или нить) как основная исполнительная единица, и мы хотим, чтобы она выполнялась либо в режиме реального времени, либо в обычном режиме операционной системы общего назначения, в качестве альтернативы, что точно соответствует внеполосному и внутриполосному контекстам Dovetail.\n  Монитор. Этот элемент имеет ту же цель, что и futex основного ядра, который заключается в предоставлении интегрированного, хотя и гораздо более простого набора основных функций синхронизации потоков. Мониторы используются внутренне библиотекой EVL для реализации мьютексов, переменных условий, групп флагов событий и семафоров в пространстве пользователя.\n  Clock(часы). Мы можем найти тактовые устройства, зависящие от платформы, в дополнение к основным, определенным архитектурой, для которых должны быть написаны специальные драйверы. Элемент ‘clock’ гарантирует, что все драйверы часов представляют один и тот же интерфейс для приложений в пространстве пользователя. Кроме того, этот элемент может экспортировать отдельные программные таймеры в приложения, что удобно для запуска периодических циклов или ожидания одноразовых событий в определенное время.\n  Observable(Наблюдаемый). Этот элемент является строительным блоком, который приложения, управляемые событиями, могут использовать для реализации шаблона проектирования наблюдателя, в котором любое количество потоков наблюдателей может быть уведомлено об обновлениях любого количества наблюдаемых объектов в слабо связанной форме.\n  Кросс-буфер. (Он же xbuf) -это двунаправленный канал связи для обмена данными между внеполосными и внутриполосными контекстами потоков без влияния на производительность в реальном времени на внеполосной стороне. Любой поток (EVL или обычный) может ждать/опрашивать ввод с другой стороны. Кросс-буферы служат той же цели, что и каналы сообщений Xenomai 3, реализованные с помощью протокола сокетов XDDP.\n  Файловый прокси. Двухъядерные системы на базе Linux неприятны по своей конструкции: огромный набор функций объектов групповой политики всегда виден приложениям, но они не должны использовать его, когда они выполняют работу в режиме реального времени с помощью автономного ядра, иначе они рискуют неограниченным временем отклика. Из-за такого исключения выдача запросов на ввод/вывод файлов, таких как вызов printf(3) не должна выполняться непосредственно из критических по времени циклов. Файловый прокси-сервер решает такую проблему, перегружая операции ввода-вывода для внутриполосных операций с файлами выделенным работникам, одновременно удерживая вызывающего абонента на этапе внеполосного выполнения.\n  Все это файл Каждый ресурс, экспортируемый EVL в приложения, представлен файлом. Кроме того, каждый элемент EVL связан с объектом устройства ядра:\n  Поскольку все ресурсы EVL поддерживаются внутренним файлом ядра, тяжелая работа по управлению их временем жизни, предотвращению устаревших ссылок путем отслеживания их пользователей, остается на VFS.\n  Приложения могут создавать общедоступные или частные элементы. Общедоступный элемент отображается в иерархии файлов устройств EVL, что позволяет приложениям с несколькими процессами совместно использовать элементы.\n  Элементы EVL извлекают выгоду из логики управления разрешениями, мониторинга и аудита, которые поставляются с семантикой файлов.\n  правила udev могут быть привязаны к представляющим интерес событиям, которые могут произойти для любого элемента. Кроме того, внутреннее состояние ядра элементов экспортируется в пространство пользователя через файловую систему /sys.\n  Драйверы устройств EVL являются (почти) распространенными драйверами EVL не вводит какую-либо конкретную модель драйвера. Он экспортирует выделенный API ядра для реализации операций ввода-вывода в реальном времени в общих драйверах символьных устройств. Фактически, ядро EVL состоит из набора таких драйверов, реализующих каждый класс элементов.\nEVL также предоставляет возможность расширить существующие семейства протоколов сокетов с помощью возможностей внеполосного ввода-вывода или добавить свои собственные протоколы с помощью нового семейства PF_OOB.\n Последнее изменение: Wed, 03 Nov 2021 16:32:36 MSK content/core/_index.md c56874c ",
    "description": "",
    "tags": null,
    "title": "Ядро EVL",
    "uri": "/ru/core/"
  },
  {
    "content": "Универсальная утилита “evl” может запускать набор базовых команд, доступных для управления, проверки и тестирования состояния ядра EVL, и любую команду, соответствующую шаблону “evl-*”, которая может быть доступна из переменной $PATH оболочки. Способ, которым утилита ‘evl’ централизует доступ к различным командам, связанным с EVL, намеренно очень похож на способ git. Каждая из команд EVL реализуется внешним плагином, который может быть простым исполняемым файлом или сценарием на любом языке. Единственное требование состоит в том, что вызывающий должен иметь разрешение на выполнение для такого файла, чтобы запустить его.\nThe general syntax is as follows:\nevl [-V] [-P \u003ccmddir\u003e] [-h] [\u003ccommand\u003e [command-args]]   \u003ccommand\u003e may be any command word listed by ‘evl -h’, such as: check which checks a kernel configuration for common issues ps which reports a snapshot of the current EVL threads test for running the EVL test suite trace which is a simple front-end to the ftrace interface for EVL\n  -P switches to a different installation path for base command plugins, which is located at $prefix/libexec by default.\n  -V displays the version information then exits. The information is extracted from the libevl library the EVL command depends on, displayed in the following format:\nevl.\u003cserial\u003e -- #\u003cgit-HEAD-commit\u003e (\u003cgit-HEAD-date\u003e) [ABI \u003crevision\u003e]\nwhere \u003cserial\u003e is the libevl serial release number, the \u003cgit-HEAD\u003e information refers to the topmost GIT commit which is present in the binary distribution the ‘evl’ command is part of, and \u003crevision\u003e refers to the kernel ABI this binary distribution is compatible with. For instance:\n  ~ # evl -V evl.0 -- #1c6115c (2020-03-06 16:24:00 +0100) [requires ABI 19]  ИнформацияThe information following the double dash may be omitted if the built sources were not version-controlled by GIT.\n  given only -h or without any argument, ‘evl’ displays this general help, along with a short help string for each of the supported commands found in \u003ccmddir\u003e, such as:  ~ # evl usage: evl [options] [\u003ccommand\u003e [\u003cargs\u003e]] -P --prefix=\u003cpath\u003e set command path prefix -V --version print library and required ABI versions -h --help this help available commands: check check kernel configuration gdb debug EVL command plugin with GDB ps report a snapshot of the current EVL threads test run EVL tests trace ftrace control front-end for EVL Checking a kernel configuration (check) evl check may be the very first evl command you should run from a newly installed target system which is going to run the EVL core. This command checks a kernel configuration for common issues which may increase latency. The general syntax is as follows:\n$ evl check [-f --file=\u003c.config\u003e] [-L --check-list=\u003cfile\u003e] [-a --arch=\u003ccpuarch\u003e] [-H --hash-size=\u003cN\u003e] [-q --quiet] [-h --help] The kernel configuration to verify is a regular .config file which contains all the settings for building a kernel image. If none is specified using the -f option, the command defaults to reading /proc/config.gz on the current machine. If this fails because any of CONFIG_IKCONFIG or CONFIG_IKCONFIG_PROC was disabled in the running kernel, the command fails.\nThe check list contains a series of single-line assertions which are tested against the contents of the kernel configuration. You can override the default check list stored at $prefix/libexec/kconf-checklist.evl with our own set of checks with the -L option. Each assertion follows the BNF-like syntax below:\nassertion : expr conditions | \"!\" expr conditions expr : symbol /* matches =y and =m */ | symbol \"=\" tristate tristate : \"y\" | \"m\" | \"n\" conditions : dependency | dependency arch dependency : \"if\" symbol /* true if set as y/m */ arch : \"on\" cputype cputype : $(uname -m) For instance:\n  CONFIG_FOO must be set whenever CONFIG_BAR is unset can be written as CONFIG_FOO if !CONFIG_BAR.\n  CONFIG_FOO must not be set can be written as !CONFIG_FOO, or conversely CONFIG_FOO=n.\n  CONFIG_FOO must be built as module on aarch32 or aarch64 can be written as CONFIG_FOO=m on aarch.\n  CONFIG_FOO must not be built-in on aarch64 if CONFIG_BAR is set can be written as !CONFIG_FOO=y if CONFIG_BAR on aarch.\n  Assertions in the check list may apply to a particular CPU architecture. Normally, the command should be able to figure out which architecture the kernel configuration file applies to by inspecting the first lines, looking for the “Linux/” pattern. However, you might have to specify this information manually to the command using the -a option if the file referred to by the -f option does not contain such information. The architecture name (cputype) should match the output of $(uname -m) or some abbreviated portion of it. However, arm64 and arm are automatically translated to aarch64 and aarch32 when found in an assertion or passed to the -a option.\nThe default check list translates the configuration-related information gathered in the caveat section as follows:\nCONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE=y if CONFIG_CPU_FREQ CONFIG_DEBUG_HARD_LOCKS=n CONFIG_ACPI_PROCESSOR_IDLE=n CONFIG_LOCKDEP=n CONFIG_DEBUG_LIST=n CONFIG_DEBUG_VM=n CONFIG_DEBUG_PER_CPU_MAPS=n CONFIG_KASAN=n The command returns the following information:\n  the wrong settings detected in the kernel configuration are written to stdout, unless the quiet -q option was given.\n  the number of failed assertions is returned via the shell exit code ($?).\n   Example: checking the current kernel configuration\n ~ # evl check CONFIG_ACPI_PROCESSOR_IDLE=y ~ # echo $? 1 Reporting a snapshot of the current EVL threads (ps) When you need to know which threads are currently present in your system, evl ps comes in handy. The command syntax - which supports short and long options formats - is as follows:\n$ evl ps [-c --cpu=\u003ccpu\u003e[,\u003ccpu\u003e...]] [-s --state] [-t --times] [-p --policy] [-l --long] [-n --numeric] [-S --sort=\u003ckey\u003e] [-h --help] This command fetches the information it needs from the /sysfs attributes the EVL core exports for every thread it manages. The output is organized in groups of values, representing either runtime parameters or statistics for each displayed thread:\n  NAME reports the thread name, as specified in the evl_attach_self() library call.\n  CPU is the processor id. the thread is currently pinned to.\n  PID is the process id. inband-wise, since any EVL thread is originally a regular Linux [k]thread. This value belongs to the global namespace (i.e. task_pid_nr()).\n  SCHED is the current scheduling policy for the thread.\n  PRIO is the priority level in the scheduling policy for the thread.\n  ISW counts the number of inband switches. Under normal circumstances, this count should remain stable over time once the thread has entered its work loop. As the only exception, a thread which undergoes the SCHED_WEAK policy may see this counter progress as a result of calling out-of-band services. For all other scheduling policies, observing any increase in this value after the time-critical loop was entered is a sure sign of a problem in the application code, which might be calling real-time unsafe services when it should not.\n  CTXSW counts the number of context switches performed by the EVL core for the thread. This value is incremented each time the thread resumes from preemption or suspension in the EVL core. CAUTION: this has nothing to do with the context switches performed by the main kernel logic.\n  SYS is the number of out-of-band system calls issued by the thread to the EVL core. Basically, this is the number of out-of-band I/O requests the thread has issued so far, since everything is a file in the EVL core.\n  RWA counts the number of Remote WAkeup signals the EVL core had to send so far for waking up the thread whenever it was sleeping on a remote CPU. For instance, this would happen if two threads running on different CPUs were to synchronize on an EVL event. Waking up a remote thread entails sending an inter-processor interrupt to the CPU that thread sleeps on for kicking the rescheduling procedure, which entails more overhead than a local wakeup. If this counter increases like crazy when your application runs, you might want to check the situation with respect to CPU affinity, to make sure the current distribution of threads over the available CPUs is actually what you want.\n  STAT gives an abbreviated runtime status of the thread as follows:\n ‘w’/‘W’ ⇾ Waits on a resource with/without timeout (TIMEOUT displays the time before timeout) ‘D’ ⇾ Delayed (TIMEOUT displays the remaining sleep time) ‘p’ ⇾ Periodic timeline (kthread only, TIMEOUT displays the remaining time until the next period starts) ‘R’ ⇾ Ready to run (i.e. neither blocked nor suspended, but waiting for the CPU to be available) ‘X’ ⇾ Running in-band ‘T’ ⇾ Ptraced and stopped (whenever traced by a debugger such as gdb[server]) ‘r’ ⇾ Undergoes round-robin (when SCHED_RR is in effect) ‘S’ ⇾ Forcibly suspended (cumulative with ‘W’ state, won’t resume until lifted)    TIMEOUT is the remaining time before some timer which was started specifically for the thread fires. Which timer was started depends on the undergoing operation for such thread, which may block until a resource is available, wait for the next period in a timeline and so on. See STAT for details.\n  %CPU is the current amount of CPU horsepower consumed by the thread over the last second. When an out-of-band interrupt preempts a thread, the time spent handling it is charged to that thread.\n  CPUTIME reports the cumulated CPU time already consumed by the thread, using a minutes:milliseconds.microseconds format.\n  The command options allow to select which threads and which data should be displayed:\n the -c option filters the output on the CPU the threads are pinned on. The argument is a comma-separated list of CPU numbers. Ranges are also supported via the usual dash separator. For instance, the following command would report threads pinned on CPU0, and all CPUs from CPU3 to CPU15.  $ evl ps -c 0,3-15   -s includes information about the thread state, which is ISW, CTXSW, SYS, RWA and STAT.\n  -t includes the thread times, which are TIMEOUT, CPU% and CPUTIME.\n  -p includes the scheduling policy information, which is SCHED and PRIO.\n  -l enables the long output format, which is a combination of all information groups.\n  -n selects a numeric output for the STAT field, instead of the one-letter flags. This actually dumps the 32-bit value representing all aspects of a thread status in the EVL core, which contains more information than reported by the abbreviated format. EVL hackers want that.\n  -S sorts the output according to a given sort key in increasing order. The following sort keys are understood:\n ‘c’ sorts by CPU values ‘i’ sorts by ISW values ’t' sorts by CPUTIME values ‘x’ sorts by CTXSW values ‘w’ sorts by RWA values ‘r’ sorts in reverse order (decreasing order)    For instance, the following command would list the times of all threads from CPU14 by decreasing CPU time consumption:\n$ evl ps -t -Srt -c14 CPU PID TIMEOUT %CPU CPUTIME NAME 14 2603 - 0.6 00:435.952 rtup_ufpp14-5:2069 14 2604 - 0.6 00:430.147 rtup_ufpp14-6:2069 14 2599 - 0.5 00:423.118 rtup14-3:2069 14 2600 - 0.5 00:420.293 rtup14-4:2069 14 2595 - 0.3 00:207.143 [rtk1@14:2069] 14 2597 - 0.3 00:204.301 [rtk2@14:2069] 14 2619 - 0.2 00:186.139 rtuo_ufpp14-14:2069 14 2617 - 0.2 00:185.497 rtuo_ufpp14-13:2069 14 2623 - 0.2 00:184.812 rtuo_ufpp_ufps14-18:2069 14 2622 - 0.2 00:184.772 rtuo_ufpp_ufps14-17:2069 14 2621 - 0.2 00:181.692 rtuo_ufps14-16:2069 14 2616 - 0.2 00:181.329 rtuo14-12:2069 14 2615 - 0.2 00:181.230 rtuo14-11:2069 14 2620 - 0.2 00:180.604 rtuo_ufps14-15:2069 14 2572 - 0.0 00:000.006 rtus_ufps13-9:2069 14 2125 - 0.0 00:000.006 rtus1-7:2069 14 2646 - 0.0 00:000.005 rtus15-8:2069 14 2650 - 0.0 00:000.005 rtus_ufps15-10:2069 14 2310 - 0.0 00:000.005 rtus6-7:2069  -h displays the command help string.  Controlling the kernel tracer (trace) The trace command provides a simple front-end for controlling the function tracer which is part of the FTRACE kernel framework, in a way which is Dovetail-aware. We typically use this tracer to analyze high latency spots during the course of the latmus program execution.\nИнформацияThere is no Dovetail (or EVL-specific) tracer. Latency spots can be analyzed using the common kernel function tracer, which reports additional information about the current execution stage and interrupt state. Trace snapshots are automatically taken at appropriate times by the latmus utility in order to help in such analysis.\n In order to use this tracer, make sure to enable the following features in your kernel build:\n CONFIG_TRACER_SNAPSHOT CONFIG_TRACER_SNAPSHOT_PER_CPU_SWAP CONFIG_FUNCTION_TRACER  The command syntax is as follows:\nevl trace [-e[\u003ctrace_group\u003e][-E\u003ctracepoint_file\u003e][-s\u003cbuffer_size\u003e][-t]] [-d] [-p] [-f] [-h] [-c \u003ccpu\u003e] [-h]  СоветArguments to options must immediately follow the option letter, without any spacing in between.\n The command options allow for a straightforward use of the function tracer:\n  -e enables the tracer in the kernel, optionally turning on a trace group, which is a set of pre-defined tracepoints. From this point, FTRACE starts logging information about a set of kernel tracepoints which may be traversed while the system executes.\nUp to libevl r24, this option-less switch enables tracing for out-of-band IRQ events, CPU idling events, and all (in-kernel) EVL core routines.\nSince libevl r25, the name of a trace group can be mentioned right after the option letter, which refers to a pre-defined set of tracepoints. Those tracepoints are listed in a separate file which should be stored at $EVL_CMDDIR/trace.$name. A tracepoint in such file is specified relative to FTRACE’s tracing/ hierarchy, such as irq/irq_pipeline_entry, which would refer to $EVL_TRACEDIR/tracing/irq/irq_pipeline_entry. Without argument, -e behaves as -e -f, which enables all kernel tracepoints (see -f).\n  ~# evl trace -eirq tracing enabled ~ # cat /usr/evl/libexec/trace.irq irq/irq_pipeline_entry irq/irq_pipeline_exit irq/irq_handler_entry irq/irq_handler_exit evl/evl_timer_shot evl/evl_trigger evl/evl_latspot You can either extend the set of pre-defined trace groups by adding your own sets to $EVL_CMDDIR, or use the -E option to specify an arbitray tracepoint file.\nIf a particular CPU is mentioned with -c along with -e, then per-CPU tracing is enabled for \u003ccpu\u003e.\n  -E is similar to -e, except that its argument refers to an arbitrary tracepoint file. This is handy for working with your own custom set of tracepoints.\nIf a tracepoint listed in the file is invalid, it is silently ignored.\n  ~# cat \u003e /tmp/custom_traces evl/evl_schedule evl/evl_pick_next evl/evl_switch_context evl/evl_switch_tail evl/evl_finish_wait ^D ~# evl trace -E/tmp/custom_traces tracing enabled   -t turns on the dry run mode for -e and -E, meaning that all commands enabling tracepoints are echoed to the output but not actually applied. This is a quick way to check the sanity of a (custom) tracepoint file.\n  if -f is mentioned, all kernel functions traversed in the course of execution are logged. CAUTION: enabling full tracing may cause a massive overhead.\n  -s changes the size of the FTRACE buffer on each tracing CPU to \u003cbuffer_size\u003e. If a particular CPU is mentioned with -c along with -s, then the change is applied to the snapshot buffer of \u003ccpu\u003e only.\n  -d fully disables the tracer which stops logging events on all CPUs.\n  -p prints out the contents of the trace buffer. If a particular CPU is mentioned with -c along with -p, then only the snapshot buffer of \u003ccpu\u003e is dumped.\n  -h displays the command help string.\n  For instance, the following command starts tracing all kernel routines:\n$ evl trace -ef Interpreting the Dovetail-specific trace information The Dovetail-specific information is about:\n  whether the in-band stage is stalled and/or irqs are disabled in the CPU. ’d' appears in the entry state flags if the in-band stage is stalled while hard irqs are enabled in the CPU, ‘D’ denotes an unstalled in-band stage with hard irqs off in the CPU, and ‘*’ denotes a combined stalled in-band stage and hard irqs off in the CPU.\n  whether we are running on the out-of-band stage, if ‘~’ appears in the entry flags.\n  СоветYou may want to read this document for details on the notion of interrupt stage Dovetail implements.\n For instance:\n/* hard irqs off, running in-band */ \u003c...\u003e-4164 [003] D... 122.047972: do_syscall_64 \u003c-entry_SYSCALL_64_after_hwframe /* in-band stalled and hard irqs off, running out-of-band */ \u003c...\u003e-4164 [003] *.~. 122.048021: __evl_schedule \u003c-run_oob_call /* in-band stalled, hard irqs on, running in-band */ \u003c...\u003e-4164 [003] d... 122.048082: rcu_lockdep_current_cpu_online \u003c-rcu_read_lock_sched_held In addition to this basic information, latmus emits a special tracepoint named evl_latspot in the trace event log before taking a trace snapshot, each time the observed maximum latency increases. The frozen trace is visible in the corresponding per-CPU snapshot buffer. From that point, you may be able to backtrack to the source(s) of the extra latency. A typical debug session would look like this:\n~ # evl trace -ef tracing enabled ~ # latmus warming up on CPU1... RTT| 00:00:01 (user, 1000 us period, priority 98, CPU1) RTH|----lat min|----lat avg|----lat max|-overrun|---msw|---lat best|--lat worst RTD| 26.675| 26.951| 27.826| 0| 0| 26.675| 27.826 RTD| 26.712| 27.067| 31.204| 0| 0| 26.675| 31.204 RTD| 26.653| 26.961| 29.160| 0| 0| 26.653| 31.204 RTD| 26.678| 27.067| 29.285| 0| 0| 26.653| 31.204 RTD| 26.759| 27.051| 29.542| 0| 0| 26.653| 31.204 RTD| 26.770| 27.079| 29.266| 0| 0| 26.653| 31.204 ^C---|-----------|-----------|-----------|--------|------|------------------------- RTS| 10.119| 27.029| 31.204| 0| 0| 00:00:06/00:00:06 ~ # evl trace -c 1 ... \u003cidle\u003e-0 [001] *.~. 135.363256: do_trace_write_msr \u003c-__switch_to \u003cidle\u003e-0 [001] *.~. 135.363256: write_msr: c0000100, value 7ff90973e700 timer-responder-234 [001] *.~. 135.363256: switch_fpu_return \u003c-dovetail_context_switch timer-responder-234 [001] *.~. 135.363257: do_raw_spin_unlock \u003c-__evl_schedule timer-responder-234 [001] *.~. 135.363257: do_raw_spin_lock \u003c-evl_wait_schedule timer-responder-234 [001] *.~. 135.363258: do_raw_spin_unlock \u003c-evl_wait_schedule timer-responder-234 [001] *.~. 135.363258: do_raw_spin_lock \u003c-latmus_oob_ioctl timer-responder-234 [001] *.~. 135.363258: do_raw_spin_unlock \u003c-latmus_oob_ioctl timer-responder-234 [001] d.~. 135.363259: evl_oob_sysexit: result=0 timer-responder-234 [001] d.~. 135.363262: pipeline_syscall \u003c-do_syscall_64 timer-responder-234 [001] d.~. 135.363262: handle_oob_syscall \u003c-pipeline_syscall timer-responder-234 [001] d.~. 135.363263: do_oob_syscall \u003c-handle_oob_syscall timer-responder-234 [001] d.~. 135.363263: evl_oob_sysentry: syscall=oob_ioctl timer-responder-234 [001] d.~. 135.363264: EVL_ioctl \u003c-do_oob_syscall timer-responder-234 [001] d.~. 135.363264: evl_get_file \u003c-EVL_ioctl timer-responder-234 [001] *.~. 135.363264: do_raw_spin_lock \u003c-evl_get_file timer-responder-234 [001] *.~. 135.363265: do_raw_spin_unlock \u003c-evl_get_file timer-responder-234 [001] d.~. 135.363265: latmus_oob_ioctl \u003c-EVL_ioctl timer-responder-234 [001] d.~. 135.363266: add_measurement_sample \u003c-latmus_oob_ioctl timer-responder-234 [001] d.~. 135.363266: evl_latspot: ** latency peak: 31.204 us ** In the latmus case, part of this analysis would include estimating the delay between the latest tick date programmed in the hardware and the actual receipt of the timer interrupt. When tracing is enabled, this information is automatically produced in the trace log:\n/* This is when the timer chip is programmed for the next tick. */ \u003cidle\u003e-0 [001] *.~. 135.362244: evl_timer_shot: latmus_pulse_handler at 135.363228 (delay: 984 us, 196195 cycles ... /* This is when the corresponding timer interrupt is received by Dovetail. */ \u003cidle\u003e-0 [001] *.~. 135.363233: irq_handler_entry: irq=4354 name=Out-of-band LAPIC timer interrupt Running the test suite (test) This command is a short-hand for running the EVL test suite. The usage is as follows:\n$ evl test [-l][-L][-k] [test-list] With no argument, this command runs all of the tests available from the default installation path at $prefix/tests:\n$ evl test duplicate-element: OK monitor-pp-dynamic: OK monitor-pi: OK clone-fork-exec: OK ... You can also chose to run a specific set of tests by mentioning them as arguments to the command, such as:\n$ evl test duplicate-element monitor-pi duplicate-element: OK monitor-pi: OK You may ask for listing the available tests instead of executing them, by using the -l switch:\n$ evl test -l duplicate-element monitor-pp-dynamic monitor-pi clone-fork-exec ... In a variant aimed at making scripting easier, you can ask for the absolute paths instead:\n$ evl test -L proxy-pipe mapfd /usr/evl/tests/proxy-pipe /usr/evl/tests/mapfd If some test goes wrong, the command normally stops immediately. Passing -k would allow it to keep going until the end of the series.\nImplementing your own EVL commands You can implement your own ‘evl’ command plugins, which may be located anywhere provided it is reachable from the shell PATH variable with the proper execute permission bit set. EVL comes with a set of base plugins available from $prefix/libexec (*). The latter directory is implicitly searched for the command after the PATH variable was considered, which means that you may override any base command with your own implementation whenever you see fit.\n Crash course: adding the ‘foo’ command script to ~/tools\n $ mkdir ~/tools $ cat \u003e ~/tools/evl-foo #! /bin/sh echo \"this is your 'evl foo' command\" ^D $ chmod +x ~/tools/evl-foo $ export PATH=$PATH:~/tools $ evl foo this is your 'evl foo' command In addition, ‘evl’ sets a few environment variables before calling a plugin. Your plugin executable/script can retrieve them using getenv(3) from a C program, or directly dereference those variables from a shell:\n   Variable Description Default value     EVL_CMDDIR Where to find the base plugins $prefix/libexec   EVL_TESTDIR Where to find the tests $prefix/tests   EVL_SYSDIR root of the /sysfs hierarchy for EVL devices /sys/devices/virtual   EVL_TRACEDIR root of ftrace hierarchy /sys/kernel/debug/tracing    (*) may be overriden using the -P option.\n Последнее изменение: Wed, 03 Nov 2021 22:05:24 MSK %!s(\u003cnil\u003e) 09017f1 ",
    "description": "",
    "tags": null,
    "title": "Команда 'evl'",
    "uri": "/ru/core/commands/"
  },
  {
    "content": "Проблема надлежащего бенчмаркинга - это сизифов, бесконечный разговор, который перезагружается по мере изменения технологий и требований пользователей. У каждого теста есть какая-то программа; это нормально, если он не принимает читателя за дурака. По этим причинам данный раздел является незавершенным проектом, в котором обсуждаются только тестовые случаи, для которых полностью доступен исходный код, который не требует никакого черного ящика, чтобы вы могли проверить и воспроизвести их довольно легко.\nЛюбые комментарии и другие материалы, улучшающие этот раздел и/или EVL в целом, приветствуются.\nИзмерение времени отклика на прерывания Поскольку инфраструктура реального времени должна обеспечивать надежное время отклика на внешние события - быть строго ограничено - независимо от того, какая система может работать, когда они происходят, мы должны измерить задержку между идеальной датой доставки такого события и фактическим моментом, когда приложение начинает его обрабатывать. Затем мы можем оценить дрожание как изменение задержки. С Linux, работающим на аппаратном обеспечении, надежность означает, что можно определить верхнюю границу такой задержки, хотя мы используем неформальный вероятностный метод с помощью бесчисленных часов тестирования при значительной нагрузке. Хотя результат такого теста сам по себе не отражает общую способность системы поддерживать приложения в реальном времени, такой неправильный тест явно был бы демонстрационным. Без сомнения, каждая существующая инфраструктура реального времени хочет блистать на этом. Для измерения времени отклика на прерывания в различных контекстах EVL предоставляет утилиту latmus. На следующем рисунке показаны потенциальные задержки, которые могут существовать между моментом, когда устройство запрашивает прерывание, и временем, когда поток ответчика, запущенный в пространстве приложения, может воздействовать на него:\n(1) Может быть несколько причин задержки, вызванной аппаратным обеспечением, таких как (но и не только):\n  некоторые устройства могут временно блокировать выполнение процессором транзакций ввода-вывода, что может привести к остановке. Например, некоторые графические процессоры на короткое время запрещают процессорам получать доступ к своей памяти ввода-вывода или DMA в пакетном режиме, влияющем на скорость выполнения инструкций в ЦП, отключая его от шины памяти во время передачи.\n  процессору необходимо выполнить внутреннюю синхронизацию с запросом на прерывание, на конвейер команд могут повлиять операции, связанные с выполнением прерывания, что приведет к дополнительной задержке.\n  Временные прерывания маскируются в процессоре по запросу программного обеспечения, эффективно предотвращая его выполнение IRQ.\n  (2) Хотя обработка IRQ в некоторой служебной процедуре из ядра реального времени (надеюсь) является коротким процессом, который заканчивается подготовкой потока ответчика в планировщике, это может быть еще более отложено:\n  Кэш памяти ввода-вывода, возможно, был загрязнен действиями, не относящимися к реальному времени, в то время как система реального времени ожидала следующего события. Это увеличивает уровень пропусков кэша как для кода, так и для данных и, следовательно, замедляет выполнение инфраструктуры реального времени в целом, когда она начинает обрабатывать входящее событие.\n  время, необходимое для обработки пропуска записи, увеличивается, когда в контроллере кэша включена политика выделения записи, что замедляет выполнение, когда это происходит.\n  (3) В конце концов, планировщик вызывается для того, чтобы пересмотреть, какой поток должен выполняться на процессоре, на котором поток ответчика находился в спящем режиме при подготовке, что может привести к большим потенциальным задержкам:\n  процессор, принимающий IRQ, может быть не тем, на котором спит ответчик, и в этом случае первый должен отправить последнему запрос на изменение расписания, чтобы он возобновил работу ответчика. Обычно это делается с помощью межпроцессорного прерывания, известного как IPI. Время, необходимое для передачи IPI на удаленный процессор и обработки там, еще больше увеличивает задержку. Тривиальный обходной путь будет включать установку привязки IRQ к тому же процессору, на котором работает ответчик, но это может быть невозможно, если контроллер прерываний не разрешит это на вашем SoC.\n  как только код планировщика определит, что поток ответчика должен возобновить выполнение, выполняется код переключения контекста для восстановления контекста памяти и файла регистрации, чтобы этот поток возобновился с того места, где он остановился. Переключение контекста памяти может быть длительной операцией на некоторых архитектурах, так как это влияет на кэш и требует сильной синхронизации.\n  также применяются аппаратные замедления, упомянутые для шага (2).\n  ИнформацияВникание в эти проблемы - это не вопрос следования подходу с двойным ядром и собственным упреждением: все они кусаются одинаково, независимо от того.\n Измерение времени отклика на события таймера Система реального времени обычно поставляется с возможностью измерения задержки ее потоков в событиях таймера. Для Xenomai 3 это latency, для PREEMPT_RT cyclictest, а EVL поставляется с программой latmus. Неудивительно, почему выполнение точно рассчитанных рабочих циклов является основным требованием для приложений реального времени. Кроме того, такой тест требует лишь небольшой подготовки: для его проведения нам не нужен никакой внешний источник событий, не требуется никакого специального оборудования, встроенного высокоточного тактового таймера тестируемой системы должно быть достаточно, поэтому мы можем легко измерить задержку непосредственно оттуда.\nXenomai 4 реализует этот тест с помощью драйвера latmus который отправляет событие пробуждения потоку ответчика с поддержкой EVL, создаваемому приложением latmus, каждый раз, когда происходит новое прерывание таймера. Поток ответчика получает временную метку от монотонных часов EVL сразу после возобновления после пробуждения, затем отправляет эту информацию драйверу, который, в свою очередь, вычисляет значение задержки, т.е. задержку между идеальным временем, когда должно было быть получено прерывание, и фактическим временем пробуждения, сообщенным потоком ответчика. Таким образом, мы учитываем все задержки упомянутые ранее, которые могут повлиять на точность запроса на своевременное пробуждение.\nДрайвер накапливает эти результаты, каждую секунду отправляя промежуточную сводку в поток регистратора с минимальными, максимальными и средними значениями задержки, наблюдаемыми за этот период. Цикл отображения 1 Гц, который виден во время работы приложения latmus, синхронизируется при получении такой сводки. На следующем рисунке показан этот процесс выполнения:\nWhen the test completes, the latmus application determines the minimum, worst case and average latency values over the whole test duration. Upon request by passing the -g option, the latmus application dumps an histogram showing the frequency distribution of the worst case figures which have been observed over time. The output format can be parsed by gnuplot.\nRunning the timer-based test First, we need the latmus driver to be loaded into the kernel for the SUT. Therefore CONFIG_EVL_LATMUS should be enabled in the kernel configuration. From the command line, the entire test is controlled by the latmus application using the -m option, which can be omitted since measuring the response time to timer events is the default test.\n Measuring response time to timer events\n # latmus RTT| 00:00:01 (user, 1000 us period, priority 98, CPU1) RTH|----lat min|----lat avg|----lat max|-overrun|---msw|---lat best|--lat worst RTD| 1.211| 1.325| 2.476| 0| 0| 1.211| 2.476 RTD| 1.182| 1.302| 3.899| 0| 0| 1.182| 3.899 RTD| 1.189| 1.314| 2.486| 0| 0| 1.182| 3.899 RTD| 1.201| 1.315| 2.510| 0| 0| 1.182| 3.899 RTD| 1.192| 1.329| 2.457| 0| 0| 1.182| 3.899 RTD| 1.183| 1.307| 2.418| 0| 0| 1.182| 3.899 RTD| 1.206| 1.318| 2.375| 0| 0| 1.182| 3.899 RTD| 1.206| 1.316| 2.418| 0| 0| 1.182| 3.899 ^C ---|-----------|-----------|-----------|--------|------|------------------------- RTS| 1.182| 1.316| 3.899| 0| 0| 00:00:08/00:00:08  Collecting plottable histogram data (timer test)\n RTT| 00:00:01 (user, 1000 us period, priority 98, CPU1) RTH|----lat min|----lat avg|----lat max|-overrun|---msw|---lat best|--lat worst RTD| 1.156| 1.273| 1.786| 0| 0| 1.156| 1.786 RTD| 1.170| 1.288| 4.188| 0| 0| 1.156| 4.188 RTD| 1.135| 1.253| 3.175| 0| 0| 1.135| 4.188 RTD| 1.158| 1.275| 2.974| 0| 0| 1.135| 4.188 ... ^C # test started on: Fri Jan 24 15:36:34 2020 # Linux version 5.5.0-rc7+ (rpm@cobalt) (gcc version 9.2.1 20190827 (Red Hat 9.2.1-1) (GCC)) #45 SMP PREEMPT IRQPIPE Wed Jan 22 12:24:03 CET 2020 # BOOT_IMAGE=(tftp)/tqmxe39/switch/bzImage rw ip=dhcp root=/dev/nfs nfsroot=192.168.3.1:/var/lab/tftpboot/tqmxe39/switch/rootfs,tcp,nfsvers=3 nmi_watchdog=0 console=ttyS0,115200 isolcpus=1 evl.oob_cpus=1 # libevl version: evl.0 -- #a3ceb80 (2020-01-22 11:57:11 +0100) # sampling period: 1000 microseconds # clock gravity: 2000i 3500k 3500u # clocksource: tsc # vDSO access: architected # context: user # thread priority: 98 # thread affinity: CPU1 # C-state restricted # duration (hhmmss): 00:01:12 # peak (hhmmss): 00:00:47 # min latency: 0.205 # avg latency: 3.097 # max latency: 25.510 # sample count: 71598 0 95 1 25561 2 18747 3 2677 4 6592 5 17056 6 664 7 48 8 18 9 12 10 18 11 24 12 14 13 2 14 6 15 5 16 5 17 6 18 23 19 17 20 3 21 1 22 1 23 1 24 1 25 1 The output format starts with a comment section which gives specifics about the test environment and the overall results (all lines from this section begin with a hash sign). The comment section is followed by the frequency distribution forming the histogram, in the form of a series of value pairs: \u003clatency-µs\u003e . For instance, “1 25561” means that 25561 wake ups were delayed between 1 (inclusive) and 2 (exclusive) microseconds from the ideal time. A plus (+) sign appearing after the last count of occurrences means that there are outliers beyond the limit of the histogram size. In such event, raising the numnber of cells with the –histogram= option may be a good idea.\nInterpreting the comment section of a data distribution   # test started on: Fri Jan 24 15:36:34 2020\nDate the test was started (no kidding).\n  # Linux version 5.5.0-rc7+ (rpm@cobalt) (gcc version 9.2.1 20190827 (Red Hat 9.2.1-1) (GCC)) #45 SMP PREEMPT IRQPIPE Wed Jan 22 12:24:03 CET 2020\nThe output of uname -a on the system under test.\n  # BOOT_IMAGE=(tftp)/tqmxe39/switch/bzImage rw ip=dhcp root=/dev/nfs nfsroot=192.168.3.1:/var/lab/tftpboot/tqmxe39/switch/rootfs,tcp,nfsvers=3 nmi_watchdog=0 console=ttyS0,115200 isolcpus=1 evl.oob_cpus=1\nThe kernel command line as returned by /proc/cmdline.\n  # libevl version: evl.0 – #a3ceb80 (2020-01-22 11:57:11 +0100)\nThe version information extracted from libevl including its major version number, and optionally the GIT commit hash and date thereof libevl was built from.\n  # sampling period: 1000 microseconds\nThe frequency of the event to be responded to by the system under test, which can either be a timer tick or a GPIO signal.\n  # clock gravity: 2000i 3500k 3500u\nThe calibration settings of the EVL core clock which applied during the test.\n  # clocksource: tsc\nThe name of the kernel clock source used by the EVL core for reading timestamps. This value depends on the processor architecture, tsc commonly refers to x86.\n  # vDSO access: architected\nSince kernel v5.5, the core reports the type of access the EVL applications have to the clock source via the vDSO. The following values are defined:\n  architected denotes a fast (syscall-less) vDSO access to a built-in clock source defined by the architecture itself. This is the best case.\n  mmio denotes a fast (syscall-less) vDSO access to a [clock source] (/ru/dovetail/porting/clocksource/) exported via Dovetail’s generic access to MMIO-based devices. This is the second best case.\n  none denotes a not-so-fast access to the kernel clock source without vDSO support, which is one of the possible issues with legacy x86 hardware. You could also have such value due to an incomplete port of Dovetail to your target system, which may be missing the conversion of existing MMIO clock source to a user-mappable one visible from the generic vDSO mechanism.\n    # context: user\nWhich was the context of the responder, among:\n  user for EVL threads running in user-space waiting for timer events,\n  kernel for EVL kernel threads waiting for timer events,\n  irq for EVL interrupt handlers receiving timer events,\n  oob-gpio for EVL threads running in user-space waiting for GPIO events,\n  inband-gpio for regular (non-EVL) threads running in user-space waiting for GPIO events.\n    # thread priority: 98\nThe priority of the responder thread in the out-of-band SCHED_FIFO class. By definition, any legit value starting from 1 and on gives the responder thread higher priority than any in-band task running in the system, including kernel threads of any sort.\n  # thread affinity: CPU1\nThe processor affinity of the responder thread. If the CPU id. is suffixed by -noisol, then the responder was not running on an isolated processor during the test, which most likely entailed higher latency values compared to isolating this CPU - unless you did not stress the SUT enough or at all while measuring, which would make the figures obtained quite flawed and probably worthless anyway.\n  # C-state restricted\nWhether the processor C-State was restricted to the shallowest level, preventing it to enter deeper sleep states which are known to induce extra latency.\n  # duration (hhmmss): 00:01:12\nHow long the test ran (according to the wall clock).\n  # peak (hhmmss): 00:00:47\nWhen the worst case value was observed, relatively to the beginning of the test.\n  # min latency: 0.205\nThe best/shortest latency value observed throughout the test.\n  # avg latency: 3.097\nThe average latency value observed, accounting for all measurement samples collected during the test.\n  # max latency: 25.510\nThe maximum value observed throughout the test. This is the worst case value we should definitely care about, provided the stress load and test duration are meaningful.\n  # sample count: 71598\nThe number of samples collected. The larger, the better (71598 samples in this example is obviously way too small for drawing any meaningful conclusion). Running a test for a period of 24 hours under significant stress load is common practice to get reliable results.\n  Measuring response time to GPIO events In addition to timer events, you will likely need to get a sense of the worst case response time to common device interrupts you may expect from EVL. Since you want the common programming model to be available for developing real-time applications, the responder has to be a thread running in user space. This test mode is also available from the latmus program, when paired with a Zephyr-based application which monitors the response time from a remote system to the GPIO events it sends.\nВниманиеFrom the perspective of the monitor system, we will measure the time it takes the SUT to not only receive the incoming event, but also to respond to it by sending a converse GPIO acknowledge. Therefore we expect the worst case figures to be higher than those reported by a plain timer test which only gets a timestamp on wake up.\n For implementing this test, we need:\n  a small development board which supports the Zephyr RTOS, and offers external GPIO pins. This GPIO test was originally developed on a FRDM-K64F board, a low-cost development platform from NXP. For this reason, the Zephyr device tree and pinmux bits for enabling the GPIO lines are readily available from this patch.\n  the latency monitoring application, aka latmon, which runs on the Zephyr board. This application periodically sends a pulse on one GPIO (output) line to be received by the system under test, then waits for an acknowledge on another GPIO (input) line, measuring the time elapsed between the two events.\n  the EVL-based system under test, also offering external GPIO pins. This board runs latmus which sets up the test system, asking latmon to configure according to the settings requested by the user, then enters a responder loop which listens to then acknowledges GPIO events to the latency monitor using two separate GPIO lines. The period is chosen by calling latmus using the -p option (which defaults to 1 ms, i.e. 1 Khz sampling loop). This setting and a few others are passed by latmus to latmon during the setup phase via the TCP/IP connection they share.\n  a couple of wires between the GPIO pins of system under test and those of the Zephyr board running latmon, for transmitting pulses and receiving acknowledges to/from the system under test.\n  a network connection between both systems so that latmus can establish a TCP/IP stream with the remote latmon application.\n  a DHCP server reachable from the Zephyr board running on your LAN. The latency monitor asks for its IPv4 address by issuing a DHCP request at boot up.\n  The latmon application running on the monitor board periodically raises a GPIO pulse event on the TX wire, which causes the SUT to receive an interrupt. By calling into the [EVL-enabled gpiolib] (https://git.xenomai.org/xenomai4/linux-evl/-/blob/3451245cdc9846835f8a2786767b17037ee13dda/drivers/gpio/gpiolib-cdev.c) driver, the responder thread created by the latmus application waits for such interrupt by monitoring edges on the GPIO pulse signal. Upon receipt, it immediately acknowledges the event by raising an edge on the GPIO ack signal, which latmon monitors for calculating the latency value as the delay between the acknowledge and pulse events. The latter accumulates these results, sending an intermediate summary every second over a TCP/IP stream to a logger thread running on the remote latmus application. This summary includes the minimum, maximum and average latency values observed over the last 1Hz period. Here again, the 1Hz display loop you can observe while latmus is running is synchronized on the receipt of such summary. The following figure illustrates this execution flow:\nQuick recipe: monitoring a Raspberry PI board with the FRDM-K64F This recipe works with any Raspberry PI board featuring a 40-pin GPIO header. It has been successfully used for monitoring the response time to GPIO events from models 2B, 3B and 4B.\n  Build and install EVL on your Raspberry PI as described in this document.\n  Install the Zephyr SDK on your development system. Once the SDK is installed, if your are using Zephyr for the first time, you may want to get your feet wet with the blinky example. The rest of the description assumes that the Zephyr SDK is rooted at ~/zephyrproject, and the libevl source tree was cloned into ~/libevl.\n  Patch the device tree and pinmux changes to the FRDM-K64F board which enable the GPIO lines in the Zephyr source tree:\n  $ cd ~/zephyrproject/zephyr $ patch -p1 \u003c ~/libevl/benchmarks/zephyr/frdm_k64f-enable-EVL-latency-monitor.patch  Connect the GPIO lines between both boards:\n  GPIO24 on the FRDM-K64F (pulse signal) should be wired to GPIO23 on the RPI3 (ack signal, BCM numbering).\n  GPIO25 on the FRDM-K64F (ack signal) should be wired to GPIO24 on the RPI3 (pulse signal, BCM numbering).\n    An OpenSDA J-Link Onboard Debug Probe is present on the FRDM-K64F, which can be used to flash the board. On your development system, the OpenOCD suite provides GDB remote debugging and flash programming support compatible with this probe over USB. In most cases, a binary OpenOCD package should be readily available from your favorite Linux distribution. Once the OpenOCD suite is installed, you may need to add some udev rules in order for the USB device to appear on your development system, such as these ones.\n  Connect a USB cable from the Open SDA micro-USB connector of the FRDM-K64F to your development system. This will power on the FRDM-K64F board, enabling firmware upload.\n  Build the latmon application using the Zephyr SDK, then flash it to the FRDM-K64F.\n  $ cd ~/zephyrproject/zephyr $ source zephyr-env.sh $ west build -p auto -b frdm_k64f ~/libevl/benchmarks/zephyr/latmon -- west build: build configuration: source directory: ~/libevl/benchmarks/zephyr/latmon build directory: ~/zephyrproject/zephyr/build BOARD: frdm_k64f (origin: command line) ... Memory region Used Size Region Size %age Used FLASH: 100452 B 1 MB 9.58% SRAM: 45044 B 192 KB 22.91% IDT_LIST: 168 B 2 KB 8.20% [156/160] Generating linker_pass_final.cmd [157/160] Generating isr_tables.c [158/160] Building C object zephyr/CMakeFiles/zephyr_final.dir/misc/empty_file.c.obj [159/160] Building C object zephyr/CMakeFiles/zephyr_final.dir/isr_tables.c.obj [160/160] Linking C executable zephyr/zephyr.elf $ west flash -- west flash: rebuilding ninja: no work to do. -- west flash: using runner pyocd -- runners.pyocd: Flashing file: /home/rpm/git/zephyrproject/zephyr/build/zephyr/zephyr.bin [====================] 100% Once (re-)flashed, the FRDM-K64F is automatically reset, with the  latency monitor taking over. You should see the following output in the serial console of the FRDM-K64F:\n*** Booting Zephyr OS build zephyr-\u003csome version information\u003e *** [00:00:00.006,000] \u003cinf\u003e latency_monitor: DHCPv4 binding... [00:00:03.001,000] \u003cinf\u003e eth_mcux: Enabled 100M full-duplex mode. [00:00:03.003,000] \u003cinf\u003e net_dhcpv4: Received: \u003cIP address of the FRDM-K64F\u003e [00:00:03.003,000] \u003cinf\u003e latency_monitor: DHCPv4 ok, listening on \u003cIP\u003e:2306 [00:00:03.003,000] \u003cinf\u003e latency_monitor: waiting for connection... From that point, the latency monitor running on the FRDM-K64F is ready to accept incoming connections from the latmus application running on the Raspberry PI (SUT).\nNative preemption and IRQ threading The GPIO response time of the standard or PREEMPT_RT kernel may be delayed by threading the GPIO interrupt the latmus application monitors, since this behavior is built in the generic GPIOLIB driver, and forced by PREEMPT_RT for most interrupts anyway. The overhead involved in waiting for a context switch to be performed to the threaded handler increases the latency under stress load. Disabling IRQ threading entirely in a single kernel configuration (i.e. without EVL) would be the wrong option though, making the latency figures generally really bad. However, you can raise the priority of the IRQ thread serving the latency pulse above any activity which should not be in its way, so that it is not delayed even further.\nIn order to do this, you first need to locate the IRQ thread which handles the GPIO pulse interrupts. A simple way to achieve this is to check the output of /proc/interrupts once the test runs, looking for the GPIO consumer called latmon-pulse. For instance, the following output was obtained from a PREEMPT_RT kernel running on an i.MX8M SoM:\n~ # cat /proc/interrupts  CPU0 CPU1 CPU2 CPU3 3: 237357 90193 227077 224949 GICv3 30 Level arch_timer 6: 0 0 0 0 GICv3 79 Level timer@306a0000 7: 0 0 0 0 GICv3 23 Level arm-pmu 8: 0 0 0 0 GICv3 128 Level sai 9: 0 0 0 0 GICv3 82 Level sai 20: 0 0 0 0 GICv3 110 Level 30280000.watchdog 21: 0 0 0 0 GICv3 135 Level sdma 22: 0 0 0 0 GICv3 66 Level sdma ... 52: 1355491 0 0 0 gpio-mxc 12 Edge latmon-pulse \u003c\u003c\u003c The one we look for 55: 0 0 0 0 gpio-mxc 15 Edge ds1337 80: 0 0 0 0 gpio-mxc 8 Edge bd718xx-irq 84: 0 0 0 0 gpio-mxc 12 Edge 30b50000.mmc cd ... With this information, we can now figure out which IRQ thread is handling the pulse events monitored by latmus, raising its priority as needed. By default, all IRQ threads are normally set to priority 50 in the SCHED_FIFO class. Typically, you may want to raise the priority of this particular IRQ handler so that it does not have to compete with other handlers. For instance, continuing the previous example we would raise the priority of the kernel thread handling IRQ52 to 99:\nСоветYou can refine even futher the runtime configuration of a kernel threading its interrupts by locking the SMP affinity of the device IRQ threads on particular CPUs, in order to either optimize the wake up time of processes waiting for such events, or reduce the jitter in processing the real-time workload. Whether you should move an IRQ thread to the isolated CPU also running the real-time workload in order to favour locality, or keeping them spatially separate in order to reduce the disturbance of interrupt handling on the workload is something you may have to determine on a case-by-case basis.\n ~ # pgrep irq/52 345 ~ # chrt -f -p 99 345 pid 345's current scheduling policy: SCHED_FIFO pid 345's current scheduling priority: 50 pid 345's new scheduling policy: SCHED_FIFO pid 345's new scheduling priority: 99 Which can be shortened as:\n~ # chrt -f -p 99 $(pgrep irq/52) pid 345's current scheduling policy: SCHED_FIFO pid 345's current scheduling priority: 50 pid 345's new scheduling policy: SCHED_FIFO pid 345's new scheduling priority: 99  ВниманиеIn a single kernel configuration, the IRQ thread process varies between runs of the latmus application for the GPIO test, because the interrupt descriptor is released at the end of each execution by GPIOLIB. Make sure to apply the procedure explained above each time you spawn a new test.\n Running the GPIO-based test Once the Zephyr board is started with the latmon application flashed in, we can run the benchmark tests on the system under test.\nThis is done by running the latmus application on the SUT, passing either of the -Z or -z option switch to select the execution stage, depending on whether we look for out-of-band response time figures (i.e. using EVL) or plain in-band response time figures (i.e. without relying on EVL’s real-time capabilities) respectively. In the latter case, we would not even need EVL to be present in the kernel of the SUT; typically we would use a PREEMPT_RT kernel instead.\nRegardless of the execution stage they should run on, both tests are configured the same way. On the latmus application command line, we need to specify which GPIO chip and pin number should be used for receiving GPIO events (-I \u003cgpiochip-name\u003e,\u003cpin-number\u003e) and sending acknowledge signals (-O \u003cgpiochip-name\u003e,\u003cpin-number\u003e).\nWhen the test is started on the SUT, the latmon application should start monitoring the GPIO response times of the latmus application indefinitely until the latter stops, at which point the latency monitor goes back waiting for another connection.\n[00:04:15.651,000] \u003cinf\u003e latency_monitor: monitoring started /* ...remote latmus runs for some time... */ [00:04:24.877,000] \u003cinf\u003e latency_monitor: monitoring stopped [00:04:24.879,000] \u003cinf\u003e latency_monitor: waiting for connection...  Measuring out-of-band response time to GPIO events (on the SUT)\n /* * Caution: the following output was produced by running the test only * a few seconds on an idle EVL-enabled system: the results displayed do not * reflect the worst case latency (which is higher) on this platform when * the test runs long enough under proper stress load. */ # latmus -Z zephyr -I gpiochip0,23 -O gpiochip0,24 connecting to latmon at 192.168.3.60:2306... RTT| 00:00:02 (oob-gpio, 1000 us period, priority 98, CPU1) RTH|----lat min|----lat avg|----lat max|-overrun|---msw|---lat best|--lat worst RTD| 11.541| 15.627| 18.125| 0| 0| 11.541| 18.125 RTD| 10.916| 15.617| 30.950| 0| 0| 10.916| 30.950 RTD| 12.500| 15.598| 25.908| 0| 0| 10.916| 30.950 RTD| 1.791| 15.571| 25.908| 0| 0| 1.791| 30.950 RTD| 13.958| 15.647| 26.075| 0| 0| 1.791| 30.950 ^C ---|-----------|-----------|-----------|--------|------|------------------------- RTS| 1.791| 15.606| 30.950| 0| 0| 00:00:05/00:00:05  Measuring in-band response time to GPIO events (on the SUT)\n /* * Caution: the following output was produced by running the test only * a few seconds on an idle PREEMPT_RT-enabled system: the results displayed do * not reflect the worst case latency (which is higher) on this platform when * the test runs long enough under proper stress load. */ # latmus -z zephyr -I gpiochip0,23 -O gpiochip0,24 connecting to latmon at 192.168.3.60:2306... CAUTION: measuring in-band response time (no EVL there) RTT| 00:00:02 (inband-gpio, 1000 us period, priority 98, CPU1) RTH|----lat min|----lat avg|----lat max|-overrun|---msw|---lat best|--lat worst RTD| 38.075| 52.401| 93.733| 0| 0| 38.075| 93.733 RTD| 39.700| 53.289| 91.608| 0| 0| 38.075| 93.733 RTD| 41.283| 54.914| 93.900| 0| 0| 38.075| 93.900 RTD| 38.075| 54.615| 91.608| 0| 0| 38.075| 93.900 RTD| 38.075| 54.767| 96.108| 0| 0| 38.075| 96.108 RTD| 41.283| 54.563| 91.608| 0| 0| 38.075| 96.108 ^C ---|-----------|-----------|-----------|--------|------|------------------------- RTS| 38.075| 54.037| 96.108| 0| 0| 00:00:07/00:00:07 Configuring the test kernel for benchmarking Hints for configuring any test kernel   turn off all debug features and tracers in the kernel configuration.\n  ensure all CPUs keep running at maximum frequency by enabling the “performance” CPU_FREQ governor, or disabling CPU_FREQ entirely.\n  have a look at the caveats here.\n  make sure the GPU driver does not cause ugly latency peaks.\n  isolate a CPU for running the latency test. For instance, you could reserve CPU1 for this purpose, by passing isolcpus=1 on the kernel command line at boot.\n  verify your kernel configuration either offline or online using the evl check command.\n  Specific hints to configure a native preemption kernel   enable maximum preemption (CONFIG_PREEMPT_RT_FULL if available).\n  The GPIO response test over PREEMPT_RT involves a threaded interrupt handler for processing GPIO events. This interrupt thread must have higher scheduling priority than other interrupt threads. See this illustration for more.\n  check that no common thread can compete with the responder thread on the same priority level. Some kernel housekeeping threads might, but regular threads should not. For latency measurement, setting the scheduling parameters of the responder thread to [SCHED_FIFO, 98] is recommended. Setting the responder to priority 99 would work for this purpose as well (with no observable gain actually), however it is not recommended to allow real-world application threads to compete with threaded IRQ handlers and other critical housekeeping tasks running in kernel space, since the latter might assume that no user code may preempt in tricky corner cases.\n  switch to a non-serial terminal (ssh, telnet). Although this problem is being worked on upstream, significant output to a serial device might affect the worst case latency on some platforms with native preemption because of the implementation issues in console drivers, so the console should be kept quiet. You could also add the “quiet” option to the kernel boot arguments as an additional precaution.\n  Specific hints to configure an EVL-enabled kernel   turn on CONFIG_EVL in the configuration.\n  turn off all features from the CONFIG_EVL_DEBUG section. The cost of leaving the watchdog enabled should be marginal on the latency figures though.\n  if you plan to measure the response time to GPIO interrupts, you will need CONFIG_GPIOLIB and CONFIG_GPIOLIB_OOB to be enabled in the kernel configuration. You will also need to enable the GPIO pin control driver for your hardware platform, which must be GPIOLIB-compliant (most are these days), and out-of-band capable.\n  The issue of proper stress load Finding the worst case latency in measuring the response time to interrupts requires applying a significant stress load to the system in parallel to running the test itself. There have been many discussions about what significant should mean in this context. Some have argued that real-time applications should have reasonable requirements, defined by a set of restrictions on their behavior and environment, so that bounded response time can be guaranteed, which sounds like asking application developers to abide by the rules defined by kernel folks. When was the last time any of them did so anyway?\nObviously, we cannot ask the infrastructure to be resilient to any type of issue, including broken hardware or fatal kernel bugs. Likewise, it is ok to define and restrict which API should be used by applications to meet their real-time requirements. For instance, there would be no point in expecting low and bounded latency from all flavours of clone(2) calls, or whenever talking to some device involves a slow bus interface like i2c. Likewise, we may impose some restrictions on the kernel when it deals with these applications, like disabling ondemand loading and copy-on-write mechanisms with mlock(2).\n Per Murphy’s laws, we do know that there is no point in wishful thinking, like hoping for issues to never happen provided that we always do reasonable things which would meet some hypothetical standard of the industry. Application developers do not always do reasonable things, they just do what they think is best doing, which almost invariably differs from what kernel folks want or initially envisioned. After all, the whole point of using Linux in this field is the ability to combine real-time processing with the extremely rich GPOS feature set such system provides, so there is no shortage of options and varieties. Therefore, when it comes to testing a real-time infrastructure, let’s tickle the dragon’s tail.\n There are many ways to stress a system, often depending on which kind of issues we would like to trigger, and what follows does not pretend to exhaustiveness. This said, these few aspects have proved to be relevant over time when it comes to observing the worst case latency:\n  Each time the kernel needs to switch between tasks which belong to distinct user address spaces, some MMU operations have to be performed in order to change the active memory context, which might also include costly cache maintenance in some cases. Those operations tend to take longer when the cache and memory subsystems have been under pressure at the time of the switch. Because the context switching process runs with interrupts disabled in the CPU, the higher the task switch rate, the more likely such extended interrupt masking may delay the response time to an external event.\n  How the response time of the real-time infrastructure is affected by unfavourable cache situations is important. While no real-time work is pending, the real-time infrastructure just sleeps until the next event to be processed arrives. In the meantime, GPOS (as in non-realtime) activities may jump in, mobilizing all the available hardware resources for carrying out their work. As they do this, possibly treading on a lot of code and manipulating large volumes of data, the real-time program is gradually evicted from the CPU caches. When it resumes eventually in order to process an incoming event, it faces many cache misses, which induce delays. For this reason, and maybe counter-intuitively at first, the faster the timed loop the responder thread undergoes, the fewer the opportunities for the GPOS work to disturb the environment, the better the latency figures (up to a certain rate of course). On the contrary, a slower loop increases the likeliness of cache evictions when the kernel runs GPOS tasks while the real-time system is sleeping, waiting for the next event. If the CPU caches have been disturbed enough by the GPOS activities from the standpoint of the real-time work, then you may get closer to the actual worst case latency figures.\nIn this respect, the - apparently - dull dd(1) utility may become your worst nightmare as a real-time developer if you actually plan to assess the worst-case latency with your system. For instance, you may want to run this stupid workload in parallel to your favourite latency benchmark (hint: CPU isolation for the real-time workload won’t save the day on most platforms):\n$ dd if=/dev/zero of=/dev/null bs=128M \u0026  ВниманиеUsing a block factor of 128M is to make sure the loop will be disturbing the CPU caches enough. Too small a value here would only create a mild load, barely noticeable in the latency figures on many platforms.\n   A real-time application system is unlikely to be only composed of a single time-critical responder thread. We may have more real-time threads involved, likely at a lower priority though. So we need to assess the ability of the real-time infrastructure to schedule all of these threads efficiently. In this case, we want the responder thread to compete with other real-time threads for traversing the scheduler core across multiple CPUs in parallel. Efficient serialization of these threads within a CPU and between CPUs is key.\n  Since we have to follow a probabilistic approach for determining the worst case latency, we ought to run the test long enough in order to increase the likeliness of exercizing the code path(s) which might cause the worst latency. Practically, running the test under load for 24 hours uninterrupted may deliver a worst case value we can trust.\n  Defining the stress workloads  A WORD OF CAUTION: several stress workloads mentioned in this section are likely to ignite the CPUs of your test machine(s) quite badly. Before any attempt is made at running any stress workload, you do have to check for potential thermal issues with your hardware first. Typically, the Raspberry PI 4B was known to suffer from overheating until a firmware revision greatly improved the situation.\n Using Linux for running real-time workloads means that we have to meet contradictory requirements on a shared hardware, maximum throughtput and guaranteed response time at the same time, which on the face of it looks pretty insane, therefore interesting. Whichever real-time infrastructure we consider, we have to assess how badly non real-time applications might hurt the performances of real-time ones. With this information, we can decide which is best for supporting a particular real-time application on a particular hardware. To this end, the following stress workloads are applied when running benchmarks:\n  As its name suggests, the Idler workload is no workload at all. Under such conditions, the response time of the real-time system to some event (i.e. timer or GPIO interrupt) is measured while the GPOS is doing nothing in particular except waiting for something to do. The purpose is to get a sense of the best case we might achieve with a particular hardware and software configuration, unimpeded by GPOS activities.\n  The Scary Grinder workload combines hackbench loops to a continuous dd(1) copy from /dev/zero to /dev/null with a large block size (128Mb). This workload most often causes the worst latency spots on a platform for any type of real-time infrastructure, dual kernel and native preemption (PREEMPT_RT). As it pounds the CPU caches quite badly, it reveals the inertia of the real-time infrastructure when it has to ramp up quickly from an idle state in order to handle an external event. The more complex the code code paths involved in doing so, the longer the response time. Do not expect ugly figures on Big Irons and other high-end x86 though, however this workload is likely to thrash common embedded systems. The shell commands to start this workload are:\n~ # while :; do hackbench; done \u0026 ~ # dd if=/dev/zero of=/dev/null bs=128M \u0026   The Pesky Neighbour workload is based on the stress-ng test suite. Several “stressors” imposing specific loads on various kernel subsystems are run sequentially. The goal is to assess how sensitive the real-time infrastructure is to the pressure non real-time applications might put on the system by using common kernel interfaces. Given the ability some stress-ng stressors have to thrash and even break the system, we limit the amount of damage they can do with specific settings, so that the machine stays responsive throughout long-running tests. With this workload, we generally focus on stressors which affect the scheduler and the memory subsystem.\n  Common set up rules for all benchmarks   The kernel configuration is double-checked for features which may have an adverse impact on the latency figures. Typically, for benchmarking purpose only, all debug options are turned off for both types of real-time infrastructures, dual kernel like EVL or native preemption (PREEMPT_RT). evl check can be used to verify a kernel configuration.\n  On a multi-core system, we always reserve CPU1 for running the test code which is monitored for response time, since native preemption requires some CPU(s) to be dedicated to the real-time workload for best results. This implies that isolcpus=1 evl.oobcpus=1 are passed to the kernel as boot options. With stress-ng, we also restrict the CPUs usable for running the stress load using the --taskset option. Assuming CPU1 is the only isolated CPU running a test, this shell expression should produce the correct CPU set for that option: 0,2-$(nproc).\n  Some limits should be put on what the stress workload is allowed to do, in order to avoid bringing the machine down to a complete stall, or triggering OOM situations which may have unwanted outcomes such as wrecking the test (or even the machine) entirely. Since we are not in the business of crash testing, we need to set these limits depending on the compute power of the test hardware.\n  All threads running some stress workload should belong to the SCHED_OTHER scheduling class. They should NOT compete with the real-time threads whose response time is being monitored during the test. Although this would have actually no impact on the real-time performances of a dual kernel system, this would lead to an unfair comparison with a native preemption system such as PREEMPT_RT which is sensitive to this issue by design. For instance, we ensure this with stress-ng by excluding all stressors implicitly involving the SCHED_FIFO or SCHED_RR policies, setting the --sched parameter to other for good measure.\n  PREEMPT_RT specific tweaks for benchmarking   With native preemption, if a particular test involves responding to a device interrupt such as the GPIO response test served by a threaded handler, we make sure to raise the task priority of such handler above all others. This rule does not apply to a dual kernel infrastructure like EVL, which always processes IRQ events immediately from the interrupt context.\n  on x86, PREEMPT_RT v5.6.10-rt5 may trigger a BUG() assertion then crash as a result of stress-ng injecting a memory error via the madvise(2) interface if CONFIG_MEMORY_FAILURE is enabled in the kernel configuration. This is typically something the Pesky workload we apply while monitoring latency would do, so you need to disable CONFIG_MEMORY_FAILURE to prevent this.\n  The benchmark scenarios We want to characterize the following aspects of a real-time infrastructure:\n  its response time to external interrupts in the worst (observed) case when the GPOS side of the system is under stress. From this information we should be able to get a clear picture of the typical applications such infrastructure can best support, in terms of low latency and jitter requirements. We will be using the following test programs to do so, for measuring the response time to timer and GPIO events respectively:\n  for EVL, the latmus program in timer response mode and GPIO response mode.\n  for PREEMPT_RT, the cyclictest program, and the latmus program in GPIO response mode which can run without EVL support.\n    the overhead it adds to the GPOS side as a result of running the in-kernel real-time machinery. Because we aim at sharing a single hardware between GPOS and RTOS activities, we need hints about the actual cost of running the real-time infrastructure on the overall system. In many cases, especially in the embedded space, the ability to downsize the compute power required for running mixed applications is valuable. There is no test program ready for this part yet. Any thought about the way to best do this, contribution of any sort is welcome.\n   Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC content/core/benchmarks/_index.md 737bee5 ",
    "description": "",
    "tags": null,
    "title": "Запуск benchmarks",
    "uri": "/ru/core/benchmarks/"
  },
  {
    "content": "What is an “EVL application process”? An EVL application process is composed of one or more EVL threads, running along with any number of regular POSIX threads.\n  an EVL thread is initially a plain regular POSIX thread spawned by a call to pthread_create(3) or the main() context which has issued the evl_attach_self() system call. This service binds the caller to the EVL core, which enables it to invoke the real-time, ultra-low latency services the latter delivers.\n  once attached, such thread may call EVL core services, until it detaches from the core by a call to evl_detach_self(), or exits, whichever comes first.\n  whenever an EVL thread has to meet real-time requirements, it must rely on the services provided by libevl exclusively. If such a thread invokes a common C library service while in the middle of a time-critical code, the EVL core does keep the system safe by transparently demoting the caller to in-band context. However, the calling thread would loose any real-time guarantee in the process, meaning that unwanted jitter would happen.\n  To sum up, the lifetime of an EVL application usually looks like this:\n  When a new process initializes, a regular thread - often the main() one - invokes routines from the C library in order to get common resources, like opening files, allocating memory buffers and so on. At some point later on, this thread calls evl_attach_self() in order to bind itself to the EVL core, which in turn allows it to create other EVL objects (e.g. evl_create_mutex(), evl_create_event(), evl_create_xbuf()). If the application needs more EVL threads, it simply spawns additional POSIX threads using pthread_create(3), then ensures those threads bind themselves to the core with a call to evl_attach_self().\n  Each EVL thread runs its time-critical work loop, only calling EVL services which operate from the out-of-band context, therefore guaranteeing bounded, ultra-low latency. The pivotal EVL service from such loop has to be a blocking call, waiting for the next real-time event to process. For instance, such call could be evl_wait_flags(), evl_get_sem(), evl_poll(), oob_read() and so on.\n  Eventually, EVL threads may call common C library services in order to cleanup/unwind the application context when their time-critical loop is over and time has come to exit.\n  This page is an index of all EVL system calls available to applications, which should help you finding out which call is legit from which context. In order to use the ultra-low latency EVL services, you need to link your application code against the libevl library which provides the EVL system call wrappers.\nIs libevl a replacement for my favourite C library? (glibc, musl, uClibc etc.) NO, not even remotely. This is a drop-in complement to the common C library and NPTL support you may be using, which enables your thread(s) of choice to be scheduled with ultra-low latency guarantee by the EVL core. As it should be clear now from the above section, you may - and actually have to - use a combination of these libraries into a single application, but you must do this in a way that ensures your time-critical code only relies on either:\n  libevl’s well-defined set of low-latency services which operate from the out-of-band context.\n  a (very) small subset of the common C library which is known not to depend on regular in-band kernel services. In other words, only routines which do not issue common Linux system calls directly or indirectly are fine in a time-critical execution context. For instance, routines from the string(3) section may be fine in a time-critical code, like strcpy(3), memcpy(3) and friends. At the opposite, any routine which may directly or indirectly invoke malloc(3) must be banned from your time-critical code, which includes stdio(3) routines, or C++ default constructors which rely on the standard memory allocator.\n  Outside of those time-critical sections which require the EVL core to guarantee ultra-low latency scheduling for your application threads, your code may happily call whatever service from whatever C library.\nWhat about multi-process applications? libevl does not impose any policy regarding how you might want to organize your application over multiple processes. However, the design and implementation of the interface to the EVL core makes sharing EVL resources between processes a fairly simple task. EVL elements can be made visible as common devices, such as threads, mutexes, events, semaphores. Therefore, every element you may want to share can be exported to the device file system, based on a visibility attribute mentioned at creation time.\nIn addition, EVL provides a couple of additional features which come in handy for sharing data between processes:\n  a general memory-sharing mechanism based on the file proxy, which is used as an anchor point for memory-mappable devices.\n  the tube data structure, which is a lightweight FIFO working locklessly, you can also use for inter-process messaging.\n  the Observable element which gives your application a built-in support for implementing the observer design pattern among one or more processes transparently.\n  Visibility: public and private elements As hinted earlier, EVL elements created by the user API can be either publically visible to other processes, or private to the process which creates them. This is a choice you make at creation time, by passing the proper visibility attribute to any of the evl_create_*() system calls, either EVL_CLONE_PUBLIC or EVL_CLONE_PRIVATE.\nA public element is represented in the /dev/evl hierarchy by a device file, which is visible to any process. Once a file descriptor is available from opening such file, it can be used to send requests to the element it refers to.\nConversely, a private element has no presence in the /dev/evl hierarchy. Only the process which created such element receives a file descriptor referring to it, directly from the creation call.\nThe /dev/evl device file hierarchy Because of its everything-is-a-file mantra, EVL exports a number of device files in the /dev/evl hierarchy, which lives in the DEVTMPFS file system. Each device file either represents an active public element, or a special command device used internally by libevl.\nIn opening a public element device, an application receives a file descriptor which can be used to submit requests to the underlying element. For instance, the scheduling parameters of a thread running in process A could be changed by a thread running in process B by a call to evl_set_schedattr() using the proper file descriptor.\nElement device files are organized in directories, one for each element class: clock, monitor, proxy, thread, cross-buffer and observable; general command devices appear at the top level, such as control, poll and trace:\n~ # cd /dev/evl /dev/evl # ls -l total 0 drwxr-xr-x 2 root root 80 Jan 1 1970 clock crw-rw---- 1 root root 246, 0 Jan 1 1970 control drwxr-xr-x 2 root root 60 Apr 18 17:38 monitor drwxr-xr-x 2 root root 60 Apr 18 17:38 observable crw-rw---- 1 root root 246, 3 Jan 1 1970 poll drwxr-xr-x 2 root root 60 Apr 18 17:38 proxy drwxr-xr-x 2 root root 60 Apr 18 17:38 thread crw-rw---- 1 root root 246, 6 Jan 1 1970 trace drwxr-xr-x 2 root root 60 Apr 18 17:38 xbuf Inside each class directory, the live public elements of that class are visible, in addition to the special clone command device. For the curious, the role of this special device is documented in the under-the-hood section.\n/dev/evl/thread # ls -l total 0 crw-rw---- 1 root root 246, 1 Jan 1 1970 clone crw-rw---- 1 root root 244, 0 Apr 19 10:45 timer-responder:2562 Managing access permissions to EVL device files In some situations, you may want to restrict access to EVL devices files present in the /dev/evl file hierarchy to a particular user or group of users. Because a kernel device object is associated to each live EVL element in the system, you can attach rules to UDEV events generated for public EVL elements or special command devices appearing in the /dev/evl file hierarchy, in order to set up their ownership and access permissions at creation time.\nFor public elements which come and go dynamically, EVL enforces a simple rule internally to set the initial user and group ownership of any element device file, which is to inherit it from the clone device file of the class it belongs to. For instance, if you set the ownership of the /dev/evl/thread/clone device file via some UDEV rule to square.wheel, all public EVL threads will belong at creation time to user square, group wheel.\nElement names Every evl_create_*() call which creates a new element, along with evl_attach_thread() accepts a printf-like format string to generate the element name. A common way of generating unique names is to include the calling process’s pid somewhere into the format string, so that you may start multiple instances of the same application without running into naming conflicts. The requirement for a unique name does not depend on the visibility attribute: distinct elements must have distinct names, regardless of their visibility. For instance:\n#include \u003cunistd.h\u003e #include \u003cevl/thread.h\u003e ret = evl_attach_self(\"a-private-thread:%d\", getpid()); ~# ls -l /dev/evl/thread total 0 crw-rw---- 1 root root 248, 1 Apr 17 11:59 clone The generated name is used to create a /sysfs attribute directory exporting the state information about the element. For public elements, a device file is created with the same name in the /dev/evl hierarchy, for accessing the element via the open(2) system call. Therefore, a name must contain only valid characters in the context of a file name.\nAs a shorthand, libevl forces in the EVL_CLONE_PUBLIC visibility attribute whenever the element name passed to the system call starts with a slash ‘/’ character, in which case this leading character will be skipped to form the actual element name:\n#include \u003cunistd.h\u003e #include \u003cevl/thread.h\u003e ret = evl_attach_self(\"/a-publically-visible-thread:%d\", getpid()); ~# ls -l /dev/evl/thread total 0 crw-rw---- 1 root root 248, 1 Apr 17 11:59 clone crw-rw---- 1 root root 246, 0 Apr 17 11:59 a-publically-visible-thread Note that when found, such shorthand overrides the EVL_CLONE_PRIVATE visibility attribute which might have been mentioned in the creation flags for the same call. The slash character is invalid in any other part of the element name, although it would be silently remapped to a placeholder for private elements without leading to an API error.\n Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Using libevl",
    "uri": "/ru/core/user-api/"
  },
  {
    "content": "Общие вопросы isolcpus это тоже наш друг Изолировать некоторые процессоры в командной строке ядра с помощью опции isolcpus=, чтобы не допустить, чтобы балансировщик нагрузки перегружал на них внутриполосную работу, - это не только хорошая идея с PREEMPT_RT, но и для любого инструмента настройки двойного ядра.\nТаким образом, некоторая случайная внутриполосная работа по удалению строк кэша на процессоре, где потоки реального времени ненадолго отключаются, менее вероятна, что увеличивает вероятность дорогостоящих пропусков кэша, что положительно сказывается на значениях задержек, которые вы можете получить. Даже если небольшое ядро EVL имеет ограниченную подверженность воздействию такого рода помех, экономия нескольких микросекунд стоит того, когда наихудший показатель уже находится в пределах десятых долей микросекунд.\nCONFIG_DEBUG_HARD_LOCKS это круто, но разрушает гарантии в реальном времени Когда включена функция CONFIG_DEBUG_HARD_LOCKS, механизм зависимостей блокировки (CONFIG_LOCKDEP), который помогает отслеживать взаимоблокировки и другие проблемы, связанные с блокировками, также включен для жестких блокировок Ласточкиного хвоста (Dovetail), который лежит в основе большинства механизмов сериализации, используемых ядром EVL.\nЭто хорошо, так как у него есть валидатор блокировки, который также контролирует жесткие блокировки, используемые EVL. Однако это связано с высокой ценой с точки зрения задержки: не редкость видеть сотни микросекунд, проведенных в валидаторе, время от времени с жесткими прерываниями. Запуск утилиты мониторинга задержки (она же latmus) которая является частью libevl в этой конфигурации, должен дать вам довольно уродливые цифры.\nКороче говоря, это нормально, если включить CONFIG_DEBUG_HARD_LOCKS для отладки некоторого шаблона блокировки в EVL, но вы не сможете одновременно выполнять требования в режиме реального времени в такой конфигурации.\nМасштабирование частоты процессора (обычно) оказывает негативное влияние на задержку Включение регулятора ondemand CPUFreq - или любого регулятора, выполняющего динамическую настройку частоты процессора, - может привести к значительной задержке EVL в вашей системе, от десяти микросекунд до более чем ста в зависимости от оборудования. Выбор так называемого регулятора производительности (performance) является безопасным вариантом, который гарантирует, что никогда не произойдет никакого частотного перехода, сохраняя процессоры на максимальной скорости обработки.\nДругими словами, если CONFIG_CPU_FREQ должен быть включен в вашей конфигурации, включение исключительно CONFIG_CPU_FREQ_DEFAULT_GOV_PERFORMANCE и CONFIG_CPU_FREQ_GOV_PERFORMANCE чаще всего является лучшим способом предотвращения неожиданно высоких пиков задержки.\nОтключите CONFIG_SMP для лучшей задержки в одноядерных системах На одноядерном оборудовании может по-прежнему выполняться некоторый нестандартный код для работы с различными типами спин-блокировок с помощью сборки SMP, что приводит к дополнительным ветвям процессора и пропускам кэша. На низкоуровневом оборудовании эти накладные расходы могут быть заметны.\nПоэтому, если вам не нужна поддержка SMP или параметры отладки ядра которые зависят от инструментирования конструкций spinlock (например CONFIG_DEBUG_PREEMPT), вы можете отключить все связанные параметры ядра, начиная с CONFIG_SMP.\nПроблемы, связанные с архитектурой x86   GCC 10.x может генерировать код, вызывающий преждевременный сбой процесса загрузки SMP, как сообщается в этом сообщении. В качестве обходного решения вы можете отключить CONFIG_STACKPROTECTOR_STRONG в конфигурации ядра.\n  CONFIG_ACPI_PROCESSOR_IDLE может увеличить задержку при пробуждении по IRQ из режима ожидания на некоторых SoC (до 30 наблюдаемых нами) на x86. Этот параметр неявно выбирается следующей цепочкой конфигурации: CONFIG_SCHED_MC_PRIO → CONFIG_INTEL_PSTATE → CONFIG_ACPI_PROCESSOR. Если на вашем оборудовании x86 наблюдаются показатели задержки вне диапазона, отключение этой цепочки может помочь.\n  Когда HPET отключен, сторожевой таймер, который отслеживает работоспособность текущего источника синхронизации (clocksource) для ядра, может использовать refined-jiffies в качестве эталонного источника синхронизации для сравнения. К сожалению, такой источник синхронизации довольно неточен для хронометража, так как прерывания таймера могут быть пропущены. Это, в свою очередь, может вызвать ложные срабатывания сторожевого таймера, что в конечном итоге приведет к объявлению источника синхронизации TSC ‘нестабильным’. Например, было замечено, что включение CONFIG_FUNCTION_GRAPH_TRACER на некотором устаревшем оборудовании будет систематически вызывать такое поведение при загрузке. Следующее предупреждение, появляющееся в журнале ядра, является симптомом этой проблемы:\nclocksource: timekeeping watchdog on CPU0: Marking clocksource 'tsc-early' as unstable because the skew is too large: clocksource: 'refined-jiffies' wd_now: fffb7018 wd_last: fffb6e9d mask: ffffffff clocksource: 'tsc-early' cs_now: 68a6a7070f6a0 cs_last: 68a69ab6f74d6 mask: ffffffffffffffff tsc: Marking TSC unstable due to clocksource watchdog Это проблема, потому что TSC является источником синхронизации с лучшим рейтингом и напрямую доступен из vDSO,](/ru/dovetail/porting/clocksource/#time-vdso-access), что ускоряет операции с отметкой времени. Если известно, что TSC на вашем оборудовании в порядке и, тем не менее, вы столкнулись с этой проблемой, вы можете передать tsc=nowatchdog ядру, чтобы предотвратить это, или даже tsc=reliable, если все TSC достаточно надежны для синхронизации между процессорами. Если TSC действительно нестабилен на каком-либо устаревшем оборудовании, и вы не можете игнорировать предупреждение сторожевого таймера, вы все равно можете оставить его другим источникам синхронизации, таким как acpi_pm. Вызовы evl_read_clock() были бы медленнее по сравнению с прямым считыванием без системного вызова из vDSO, но ядру EVL, тем не менее, удалось бы получить временные метки из своих встроенных часов за счет внеполосного системного вызова, хотя и без участия внутриполосной стадии. Вы определенно хотите убедиться, что на вашей платформе все правильно в отношении считывания временных меток, запустив тест latmus, который может обнаружить любую связанную с этим проблему.\nВы можете получить текущий источник синхронизации, используемый ядром, следующим образом:\n# cat /sys/devices/system/clocksource/clocksource0/current_clocksource tsc   Сбор данных perf на основе NMI может привести к тому, что ядро будет выполнять крайне медленный код драйвера ACPI при каждом событии. Поскольку отключение CONFIG_PERF не является опцией, передача nmi_watchdog=0 в командной строке ядра при загрузке может помочь.\n  ВниманиеПередача nmi_watchdog=0 отключает обнаружение жесткой блокировки для внутриполосного ядра. Однако EVL по-прежнему будет обнаруживать запущенные потоки EVL, застрявшие во внеполосном выполнении, если включена функция CONFIG_EVL_WATCHDOG.\n  Последнее изменение: Wed, 03 Nov 2021 13:47:15 MSK content/core/caveat.md 129a74f Последнее изменение: 03-11-2021 13:47:15 MSK ___Оригинал: content/core/caveat.md Коммит: 129a74f ",
    "description": "",
    "tags": null,
    "title": "Это вы определенно хотите знать",
    "uri": "/ru/core/caveat/"
  },
  {
    "content": "The real-time I/O support EVL provides for is based on the ability some kernel drivers have to handle out-of-band interrupts triggered by the hardware, along with out-of-band requests issued by applications. To achieve this, such driver may depend on the EVL kernel API which in turn depends on Dovetail. This way, both the incoming events and outgoing requests are expedited by the EVL core, without being delayed by the regular GPOS work which is still handled normally on the in-band execution stage.\nIntegrated vs separated real-time I/O support Over the years, the usual approach followed by dual kernel systems in order to provide for real-time I/O support has been to build their own separate, ad hoc driver stack, re-implementing a marginal portion of the common driver stack. The arguments invoked for doing so have been that:\n  a strict separation between both code bases would prevent any non-deterministic behavior from the common Linux driver stack to spill over into the real-time execution, by design.\n  such a separate implementation would be required in order to optimize for real-time performance.\n  the real-time driver stack would be easier to maintain, not being affected by conflicting updates to the original kernel code it is based on.\n  The typical way to implement such a driver is to start from a copy of a mainline kernel driver available for the target device, amending the sources heavily in order to use the real-time core API instead of the common kernel API, dropping any code which would not be required for supporting the real-time applications in the same move. At the end of the day, such real-time variant of a driver has diverged into an almost irreconcilable fork of the original code.\nUnfortunately, after two decades using this model in the RTAI and Xenomai 3 projects, we ended up with a massive issue which is a severe bit rotting of the real-time driver stack. Because the dual kernel ecosystem runs on very few contributors despite it has many industrial users, every real-time driver once merged would receive little to no maintenance effort in the long run. As a result, continuous updates to the original mainline driver which address bugs, extend support to new hardware models and versions, is lost for the real-time driver. In turn, the consequences for such dual kernel systems are bleak:\n  narrower hardware support than mainline drivers have.\n  more reliability issues than mainline drivers suffer from.\n  high cost of updating the real-time driver variant with the bug fixes and improvements available from the original mainline implementation, which in most cases discourages potential contributors from tackling such a task.\n  Instead of reinventing the wheel with a separate driver stack, the option EVL follows builds on these observations:\n  we can define the notion of operating mode for most common drivers, either in-band or out-of-band. Device probing, initialization, changes in basic device management are in-band operations by nature, which should never overlap with out-of-band I/O transactions aimed at transferring data since the latter are supposed to be expedited and time-bound. So both aspects of a real-time capable driver should be able to coexist, although active at different times, on a mutually exclusive basis.\n  we would only need a limited subset of the Linux driver stack to deliver ultra-low latency performances via out-of-band execution. DMA, SPI, GPIO, UART, network interface, acquisition cards, CAN and other fieldbus devices are among those which are typically involved in systems with real-time requirements.\n  With this in mind, the idea would be to add out-of-band capabilities to the existing mainline drivers we are interested in for dealing with real-time I/O, while keeping the original, in-band support available. These capabilities would be known from the EVL core, and exclusively accessible via its API under well-defined conditions. When sharing the implementation with an existing driver is not possible, either because adding out-of-band capabilities would be clumsly, or simply because there is no mainline driver for the target device (e.g. some custom FPGA), it should be a non-issue to implement a dedicated real-time driver from scratch, using the EVL kernel API.\nSuch approach is not at odds with the motivations which prevailed for using a strictly separated driver stack though:\n  there are two ways the GPOS kernel code can adversely affect the real-time behavior of the out-of-band code:\n- if the out-of-band code spuriously calls into the common, in-band kernel API. In such a case, Dovetail can detect whether some code running on the out-of-band stage is wrongfully calling into the non real-time code, so such fact would hardly go unnoticed. Which API calls are legit in the context of out-of-band processing, and which are not, is well-defined for EVL and such rules are applicable to any out-of-band code regardless of its location. In this respect, having a separate implementation for the driver stack brings no upside.\n if the in-band code changes the hardware state in a way which may cause high latency peaks (e.g. lengthy cache maintenance operations). In this respect, if a real-time capable driver is a fork of a mainline kernel driver, any pre-existing issue of that kind in the latter implementation would be initially imported into the former, by definition. Therefore, a careful audit of the code is required in any case, whether it is forked off of a mainline driver or we are adding out-of-band capabilities to the original driver directly.    there is no reason for the out-of-band services available from a common driver to underperform if the implementation clearly gives exclusive access to the hardware to the EVL core while out-of-band requests are being processed. For instance, reserving exclusive access to a SPI bus for out-of-band transfers only until we are done with an entire session is an acceptable restriction on in-band usage for the purpose of delivering real-time performances. Once such out-of-band runtime mode is left, the driver becomes usable anew for regular, in-band operations. In some cases, I/O transaction management might also be entirely left to the out-of-band code, proxying in-band I/O requests via some low-priority queue which would be processed by the former on idle time, provided we can still meet the real-time requirements doing so.\n  although the risk of merge or logic conflicts does exist by definition as the extended driver is rebased over later kernel releases, it seems an acceptable burden compared to bit rotting of a separate code base, which is definitely not. How soundly the out-of-band support is integrated into the original driver code will make a difference when it comes to rebasing it.\n  Would such integrated approach cover all the needs for real-time I/O in a dual kernel system such as EVL? Certainly not. Typically, when drivers are endpoints of a complex protocol stack such as an IP network stack attached to network interfaces, the issue of handling such protocol within a bounded execution time would still not be solved. In that particular case, a separate real-time capable IP stack is going to be needed too. However, finding a proper way to extend existing NIC drivers to serve as endpoints in this real-time IP stack seems a more tractable problem than maintaining a truckload of functionally redundant, separate drivers like RTnet currently requires.\nWhy doesn’t EVL implement RTDM? RTDM as an abstract driver model for dual kernel systems was aimed at addressing two major issues with the latter in the early days:\n  provide a common call interface between applications and the real-time core, in order to replace the variety of ad hoc mechanisms which application developers came up with over time. Some would use FIFOs, others shared memory, others some specific system call only available from a given flavour of dual kernel system, and so on. Every application would come with its own I/O interface to the kernel, which was kind of weird. RTDM replaced all of them by a single POSIXish API, which strives to mimic the common character-based interface, and socket API.\n  establish a common kernel API which all RTDM drivers would use, so that they would be portable across multiple flavours of dual kernel systems implementing the RTDM interface.\n  RTDM enshrines the notion that a dual kernel system should provide for its own driver stack aside of the Linux driver model, as a result it opposes in principle to what EVL aims at. Achieving a closer integration of real-time I/O support into the mainline kernel code whenever possible is a fundamental goal of EVL.\nTo this end, EVL extends both the regular Linux character-based I/O and socket interfaces between applications and real-time drivers to support out-of-band operations on common files and sockets.\nAs a consequence, there is not much point in implementing the RTDM interface over EVL, except maybe as a compatibility layer for porting RTAI or Xenomai 3 Cobalt-originated drivers, although this would be easily done by converting them directly to the EVL kernel API without needing any wrapper of the kind. As a matter of fact, the EVL kernel API and the portion of the Cobalt core API which has been used as a reference when designing RTDM share all key semantics.\nOut-of-band capable I/O drivers This table gives an overview of the current support for real-time I/O in EVL, which is not much yet, but poised to improve since the infrastructure is ready:\n  #oobdrv { width: 70%; align: left; } #oobdrv td:nth-child(1) { text-align: center; } #oobdrv th:nth-child(1) { text-align: center; }   Device Type Support   DMA bcm2835-dma, imx-sdma   SPI spi-bcm2835   GPIO * gpio-mxc    * In theory, any GPIO pin controller based on the generic GPIOLIB services should be real-time ready since the latter implements the out-of-band interface for all of them, at the exception of controllers which can sleep from their -\u003eget() and -\u003eset() pin state handlers. In practice, each controller we may want to use in such context should still be audited though, in order to make sure that no in-band service (e.g. common non-raw spinlocks) is called under the hood by these handlers. Only the controllers which are known to work in out-of-band mode at the time of this writing are listed in the table.\n Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Real-time I/O drivers",
    "uri": "/ru/core/oob-drivers/"
  },
  {
    "content": "Представляем Dovetail Использование Linux в качестве хоста для легких программных ядер, специализирующихся на обеспечении очень короткого и ограниченного времени отклика, на протяжении многих лет было популярным способом поддержки приложений реального времени во встроенном пространстве.\nЭтот двухъядерный дизайн вводит небольшую инфраструктуру реального времени в ядро Linux, которая немедленно обрабатывает критически важные по времени внеполосные действия независимо от текущей работы ядра общего назначения. Потоки приложений, совместно управляемые этой инфраструктурой, по-прежнему получают преимущества от общих служб ядра, таких как управление виртуальной памятью; они также могут использовать богатый набор функций объектов групповой политики, предоставляемых Linux, таких как сеть, хранилище данных или графические интерфейсы.\nСуществует значительный потенциал в том, чтобы отделять ядро реального времени от инфраструктуры объектов групповой политики:\n  поскольку автономное ядро не зависит от основной логики ядра для выполнения критически важных по времени операций, операции в реальном времени не сериализуются с операциями объектов групповой политики внутри. Это устраняет потенциальные задержки, вызванные последним, в результате строительства. В результате нет необходимости в том, чтобы все операции объектов групповой политики были детализированы и в любое время могли быть вытеснены, что в противном случае может привести к заметным накладным расходам, поскольку наследование приоритетов задач и потоковая передача IRQ должны применяться в масштабах всей системы, затрагивая все задачи, включая те, которые имеют нулевые требования в реальном времени. Подводя итог, можно сказать, что чем меньше усилий для ядра по поддержанию внутренних гарантий в режиме реального времени, тем большая пропускная способность процессора доступна приложениям.\n  при отладке проблемы в реальном времени функциональная изоляция инфраструктуры реального времени от остальной части кода ядра ограничивает поиск ошибок рамками небольшого автономного ядра, исключая большинство взаимодействий с очень большой базой ядра объектов групповой политики.\n  благодаря выделенной инфраструктуре, предоставляющей определенный, четко определенный набор услуг в режиме реального времени, приложения могут однозначно определять, какие вызовы API доступны для поддержки критически важных по времени работ, исключая все остальные как потенциально недетерминированные в отношении времени отклика.\nИными словами, ваше предположение, что каждая процедура из glibc становится доступной в режиме реального времени исключительно благодаря работе в системе собственного вытеснения (preemption)? Конечно, вы этого не сделаете, поэтому в любом случае тщательно выберите набор служб, которые ваше приложение реального времени может вызывать из своего критичного по времени рабочего цикла. По этой причине предоставление компактного специализированного API, который экспортирует набор услуг, специально предназначенных для использования в режиме реального времени, явно является преимуществом, а не ограничением.\n  В этой документации представлен Dovetail, интерфейс ядра, который вводит этап выполнения с высоким приоритетом в основную логику ядра. В любое время внеполосные действия, выполняемые на этом этапе, могут помешать общей работе. Программное ядро для конкретной задачи, такое как ядро реального времени, может подключаться к этому интерфейсу для получения ограниченного времени отклика на внешние прерывания и возможностю планирования с ультранизкой задержкой. Это приводит к реализации Dovetail следующим образом:\n  конвейер прерываний, который создает этап выполнения с высоким приоритетом для запуска автономного программного ядра.\n  поддержка так называемого альтернативного планирования между основным ядром и автономным программным ядром для совместного использования kthreads и user задач.\n  Хотя оба уровня, вероятно, потребуются для реализации некоторого автономного ядра, на ранней стадии переноса Ласточкиного хвоста необходимо включить только конвейер прерываний. Поддержка альтернативного планирования основывается на последнем и может - и должна - быть отложена до тех пор, пока конвейер не станет полностью функциональным на целевой архитектуре или платформе. База кода специально поддерживается таким образом, чтобы обеспечить такой инкрементный процесс.\nЗаметкаЛасточкин хвост только вводит базовые механизмы размещения автономного ядра в ядро Linux, позволяя использовать общую модель программирования для своих приложений в пользовательском пространстве. Он не реализует ядро программного обеспечения как таковое, которое вместо этого должно предоставляться отдельным компонентом ядра, таким как ядро EVL.\n Зачем нам это нужно? Двухъядерные системы на базе Linux требуют некоторого уровня интерфейса для сопряжения вторичного ядра (обычно с поддержкой реального времени, такого как ядро EVL) с логикой ядра, в которое оно встроено, чтобы воспользоваться богатым набором функций Linux при запуске специализированных приложений с жесткими требованиями к реальному времени. Типичной реализацией такого рода интерфейса является I-pipe, который на протяжении многих лет служил как RTAI , так и Xenomai 3 Cobalt. По нескольким причинам, описанным в этом документе, поддержание I-pipe оказалось сложным, поскольку изменения в основном ядре регулярно вызывали нетривиальные конфликты кода, иногда неприятные регрессии для сопровождающих I-pipe ниже по потоку. Хотя концепция конвейерной обработки прерываний оказалась правильной в обеспечении короткого времени отклика с разумно ограниченными изменениями в исходном ядре, то, как этот механизм интегрирован в основной код, показывает его возраст.\nDovetail является преемником I-pipe, со следующими целями:\n  ввод этапа выполнения с высоким приоритетом для критически важных по времени операций в логику ядра, позволяя всем прерываниям устройства вести себя как NMI с точки зрения ядра.\n  предоставление простого интерфейса автономным ядрам для выполнения задач Linux на этом этапе выполнения с высоким приоритетом, когда это необходимо, обеспечивая сверхтонко раздробленное упреждение всех других действий ядра в таком случае.\n  включение общей модели программирования Linux для приложений, которые должны управляться автономным ядром для предоставления услуг со сверхнизкой задержкой (адресное пространство частного пользователя, многопоточность, возможности SMP, системные вызовы и т.д.).\n  сделать возможным поддерживать Dovetail (и, в конечном счете, ядро EVL , основанное на нем) с общими знаниями о разработке ядра, за небольшую долю затрат на проектирование и техническое обслуживание, которые требует собственное упреждение. Как правило, Ласточкин хвост всегда выступает за расширение существующих подсистем ядра с возможностью работы на новом этапе выполнения, вместо того, чтобы предпринимать побочные шаги. Например, логика конвейера прерываний напрямую интегрирована в общий слой IRQ.\n  Развивающийся Dovetail Работа над “Ласточкиным хвостом” является обычной разработкой ядра Linux, следуя общему набору правил и рекомендаций, которые преобладают в основном ядре.\nИнтерфейс “Ласточкин хвост” поддерживается в следующем репозитории GIT, который отслеживает основную линию ядра Linux:\n git@git.xenomai.org:Xenomai/linux-dovetail.git https://git.xenomai.org/linux-dovetail.git  Аудитория Этот документ предназначен для людей, обладающих общими знаниями в области разработки ядра, которые могут быть заинтересованы в создании автономного программного ядра на Ласточкином хвосте, переносе его на свою архитектуру или платформу по выбору. Знание основ потока прерываний, драйверов микросхем IRQ и устройств синхронизации событий в дереве ядра было бы обязательным требованием для переноса этого кода.\nОднако этот документ не подходит для того, чтобы намочить ноги при разработке ядра, поскольку он предполагает, что читатель уже знаком с основами ядра.\n Последнее изменение: Fri, 05 Nov 2021 22:21:21 MSK content/dovetail/_index.md c56874c ",
    "description": "",
    "tags": null,
    "title": "Интерфейс Dovetail",
    "uri": "/ru/dovetail/"
  },
  {
    "content": "The autonomous core has to act upon device interrupts with no delay, regardless of the other kernel operations which may be ongoing when the interrupt is received by the CPU. Therefore, there is a basic requirement for prioritizing interrupt masking and delivery between the autonomous core and GPOS operations, while maintaining consistent internal serialization for the kernel.\nHowever, to protect from deadlocks and maintain data integrity, Linux hard disables interrupts around any critical section of code which must not be preempted by interrupt handlers on the same CPU, enforcing a strictly serialized execution among those contexts. The unpredictable delay this may cause before external events can be handled is a major roadblock for kernel components requiring predictable and very short response times to external events, in the range of a few microseconds.\nTo address this issue, Dovetail introduces a mechanism called interrupt pipelining which turns all device IRQs into pseudo-NMIs, only to run NMI-safe interrupt handlers from the perspective of the main kernel activities. This is achieved by substituting real interrupt masking in a CPU by a software-based, virtual interrupt masking when the in-band stage is active on such CPU. This way, the autonomous core can receive IRQs as long as it did not mask interrupts in the CPU, regardless of the virtual interrupt state maintained by the in-band side. Dovetail monitors the virtual state to decide when IRQ events should be allowed to flow down to the in-band stage where the main kernel executes. This way, the assumptions the in-band code makes about running interrupt-free or not are still valid.\nTwo-stage IRQ pipeline Interrupt pipelining is a lightweight approach based on the introduction of a separate, high-priority execution stage for running out-of-band interrupt handlers immediately upon IRQ receipt, which cannot be delayed by the in-band, main kernel work. By immediately, we mean unconditionally, regardless of whether the in-band kernel code had disabled interrupts when the event arrived, using the common local_irq_save(), local_irq_disable() helpers or any of their derivatives. IRQs which have no handlers in the high priority stage may be deferred on the receiving CPU until the out-of-band activity has quiesced on that CPU. Eventually, the preempted in-band code can resume normally, which may involve handling the deferred interrupts.\nIn other words, interrupts are flowing down from the out-of-band to the in-band interrupt stages, which form a two-stage pipeline for prioritizing interrupt delivery. The runtime context of the out-of-band interrupt handlers is known as the out-of-band stage of the pipeline, as opposed to the in-band kernel activities sitting on the in-band stage:\nAn autonomous core can base its own activities on the out-of-band stage, interposing on specific IRQ events, for delivering real-time capabilities to a particular set of applications. Meanwhile, the main kernel operations keep going over the in-band stage unaffected, only delayed by short preemption times for running the out-of-band work.\nOptimistic interrupt protection Predictable response time of out-of-band handlers to IRQ receipts requires the in-band kernel work not to be allowed to delay them by masking interrupts in the CPU.\nHowever, critical sections delimited this way by the in-band code must still be enforced for the in-band stage, so that system integrity is not at risk. This means that although out-of-band IRQ handlers may run at any time while the out-of-band stage is accepting interrupts, in-band IRQ handlers should be allowed to run only when the in-band stage is accepting interrupts too. So we need to decouple the interrupt masking and delivery logic which applies to the out-of-band stage from the one in effect on the in-band stage, by implementing a dual interrupt control.\nIn the pipelined interrupt model, the CPU can receive interrupts most of the time, but the delivery logic of those events may be deferred by a software mechanism until the kernel actually accepts them. This approach is said to be optimistic because it is assumed that the overhead of maintaining such mechanism should be small as the in-band code seldom receives an interrupt while masking them.\nThis approach was fully described by Stodolsky, Chen \u0026 Bershad in [Fast Interrupt Priority Management in Operating System Kernels] (https://www.usenix.org/legacy/publications/library/proceedings/micro93/full_papers/stodolsky.txt).\nVirtual interrupt disabling To this end, a software logic managing a virtual interrupt disable flag is introduced by the interrupt pipeline between the hardware and the generic IRQ management layer. This logic can mask IRQs from the perspective of the in-band kernel work when local_irq_save(), local_irq_disable() or any lock-controlled masking operations like spin_lock_irqsave() is called, while still accepting IRQs from the CPU for immediate delivery to out-of-band handlers.\nWhen a real IRQ arrives while interrupts are virtually masked, the event is logged for the receiving CPU, kept there until the virtual interrupt disable flag is cleared at which point it is dispatched as if it just happened. The principle of deferring interrupt delivery based on a software flag coupled to an event log has been originally described as Optimistic interrupt protection in this paper. It was originally intended as a low-overhead technique for manipulating the processor interrupt state, reducing the cost of interrupt masking for the common case of absence of interrupts.\nIn Dovetail’s two-stage pipeline, the out-of-band stage protects from interrupts by disabling them in the CPU’s status register as usual, while the in-band stage disables interrupts only virtually. A stage for which interrupts are disabled is said to be stalled. Conversely, unstalling a stage means re-enabling interrupts for it.\nВниманиеObviously, stalling the out-of-band stage implicitly means disabling further IRQ receipts for the in-band stage down the pipeline too.\n Interrupt deferral for the in-band stage When the in-band stage is stalled because the virtual interrupt disable flag is set, any IRQ event which was not immediately delivered to the out-of-band stage is recorded into a per-CPU log, postponing delivery to the in-band kernel handler.\nSuch delivery is deferred until the in-band kernel code clears the virtual interrupt disable flag by calling local_irq_enable() or any of its variants, which unstalls the in-band stage. When this happens, the interrupt state is resynchronized by playing the log, firing the in-band handlers for which an IRQ event is pending.\n/* Both stages unstalled on entry */ local_irq_save(flags); \u003cIRQx received: no out-of-band handler\u003e (pipeline logs IRQx event) ... local_irq_restore(flags); (pipeline plays IRQx event) handle_IRQx_interrupt(); If the in-band stage is unstalled at the time of the IRQ receipt, the in-band handler is immediately invoked, just like with the non-pipelined IRQ model.\nAll interrupts are (seemingly) NMIs From the standpoint of the in-band kernel code (i.e. the one running over the in-band interrupt stage) , the interrupt pipelining logic virtually turns all device IRQs into NMIs, for running out-of-band handlers.\nFor this reason, out-of-band code may generally not re-enter in-band code, for preventing creepy situations like this one:\n/* in-band context */ spin_lock_irqsave(\u0026lock, flags); \u003cIRQx received: out-of-band handler installed\u003e handle_oob_event(); /* attempted re-entry to in-band from out-of-band. */ in_band_routine(); spin_lock_irqsave(\u0026lock, flags); \u003cDEADLOCK\u003e ... ... ... ... spin_unlock irqrestore(\u0026lock, flags); Even in absence of an attempt to get a spinlock recursively, the outer in-band code in the example above is entitled to assume that no access race can occur on the current CPU while interrupts are (virtually) masked for the in-band stage. Re-entering in-band code from an out-of-band handler would invalidate this assumption.\nIn rare cases, we may need to fix up the in-band kernel routines in order to allow out-of-band handlers to call them. Typically, generic atomic helpers are such routines, which serialize in-band and out-of-band callers.\nFor all other cases, the IRQ work API is available for scheduling the execution of a routine from the out-of-band stage, which will be invoked later from a safe place on the in-band stage as soon as it gets back in control on the current CPU (i.e. typically when in-band interrupts are allowed again).\nPipeline entry context The pipeline entry denotes the code which runs in order to accept a hardware interrupt, either dispatching it to its out-of-band handler immediately, or logging it as a pending event for the in-band stage. This runtime section starts from the call to handle_irq_pipeline_prepare(), ends after the call to handle_irq_pipeline_finish(). These calls are issued by the architecture code every time it wants to feed the pipeline upon an incoming hardware interrupt.\nPipelining and RCU As described earlier, from the standpoint of the in-band context, an interrupt entry is unsafe in a similar way a NMI is when pipelining is in effect, since it may preempt almost anywhere as IRQs are only virtually masked most of the time, including inside (virtually) interrupt-free sections.\nFor this reason, Dovetail declares a (pseudo-)NMI entry to RCU when a hardware interrupt is fed to the pipeline, so that it may enter RCU read sides to safely access RCU-protected data. Typically, handle_domain_irq() would need this guarantee to resolve IRQ mappings for incoming hardware interrupts.\nTo this end, the RCU code considers a pipeline entry as a non-maskable context alongside in_nmi(), which ends before the companion core is allowed to reschedule.\nOn hardware interrupt, the execution flow is as follows:\n\u003cIRQ\u003e /* hard irqs off */ core_notify_irq_entry /* disable oob reschedule */ \u003cpipeline_entry\u003e rcu_nmi_enter oob_irq_delivery OR, inband_irq_logging rcu_nmi_exit \u003c/pipeline_entry\u003e core_notify_irq_exit /* re-enable oob reschedule */ synchronize_irq_log inband_irq_delivery /* hard irqs on */ \u003c/IRQ\u003e /* in-band reschedule */ As a result, the pipeline core may safely assume that while in a pipeline entry:\n  RCU read sides may be entered to access protected data since RCU is watching,\n  the virtual interrupt state (stall bit) of the in-band stage might legitimately be different from the hardware interrupt state.\n  This also means that RCU-awareness is denied to activities running on the out-of-band stage outside of the pipeline entry context. As a result, the companion core has no RCU protection against accessing stale data, except from its interrupt handlers (i.e. those marked as running out-of-band with the IRQF_OOB flag).\nAll of the above is possible because hardware interrupts are always disabled when running the pipeline entry code, therefore pipeline entries are strictly serialized on a given CPU. The following guarantees allow this:\n  out-of-band handlers called from handle_oob_irq() may not re-enable hard interrupts.\n  synchronizing the in-band log with hard interrupts enabled is done outside of the section marked as a pipeline entry.\n   Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Interrupt pipeline",
    "uri": "/ru/dovetail/pipeline/"
  },
  {
    "content": "Serializing threads with mutexes EVL provides common mutexes for serializing thread access to a shared resource from out-of-band context, with semantics close to the POSIX specification for the basic operations.\nMutex services   int evl_create_mutex(struct evl_mutex *mutex, int clockfd, unsigned int ceiling, int flags, const char *fmt, ...)  This call creates a mutex, returning a file descriptor representing the new object upon success. This is the generic call form; for creating a mutex with common pre-defined settings, see [evl_new_mutex()}(#evl_new_mutex).\nmutexAn in-memory mutex descriptor is constructed by evl_create_mutex(), which contains ancillary information other calls will need. mutex is a pointer to such descriptor of type struct evl_mutex.\n\nclockfdSome mutex-related calls are timed like evl_timedlock_mutex which receives a timeout value. You can specify the EVL clock this timeout refers to by passing its file descriptor as clockfd. Built-in EVL clocks are accepted here.\n\nceilingWhen non-zero, EVL interprets this value as the priority ceiling of the new mutex. In this case, the behavior is similar to POSIX’s PRIO_PROTECT protocol. The priority must be in the [1-99] range, so that priority ceiling can apply to any scheduling policy EVL supports.\nWhen zero, EVL applies a priority inheritance protocol such as described for POSIX’s PRIO_INHERIT protocol.\n\nЗаметкаFor priority protection/ceiling, EVL actually changes the priority of the lock owner to the ceiling value only when/if the rescheduling procedure is invoked while such thread is current on the CPU. Since mutex-protected critical sections should be short and the lock owner is unlikely to schedule out while holding such lock, this reduces the overhead of dealing with priority protection, whilst providing the same guarantees compared to changing the priority immediately upon acquiring that lock.\n flagsA set of creation flags ORed in a mask which defines the new mutex type and visibility:\n  EVL_MUTEX_NORMAL for a normal, non-recursive mutex.\n  EVL_MUTEX_RECURSIVE for a recursive mutex a thread may lock multiple times in a nested way.\n  EVL_CLONE_PUBLIC denotes a public element which is represented by a device file in the /dev/evl file hierarchy, which makes it visible to other processes for sharing.\n  EVL_CLONE_PRIVATE denotes an element which is private to the calling process. No device file appears for it in the /dev/evl file hierarchy.\n  EVL_CLONE_NONBLOCK sets the file descriptor of the new mutex in non-blocking I/O mode (O_NONBLOCK). By default, O_NONBLOCK is cleared for the file descriptor.\n  \nfmtA printf-like format string to generate the mutex name. See this description of the [naming convention] (/ru/core/user-api/#element-naming-convention).\n\n...The optional variable argument list completing the format.\n\nevl_create_mutex() returns the file descriptor of the newly created mutex on success. Otherwise, a negated error code is returned:\n  -EEXIST\tThe generated name is conflicting with an existing mutex, event, semaphore or flag group name.\n  -EINVAL\tEither flags is wrong, clockfd does not refer to a valid EVL clock, or the generated name is badly formed.\n  -ENAMETOOLONG\tThe overall length of the device element’s file path including the generated name exceeds PATH_MAX.\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -ENOMEM\tNo memory available.\n  -ENXIO The EVL library is not initialized for the current process. Such initialization happens implicitly when evl_attach_self() is called by any thread of your process, or by explicitly calling evl_init(). You have to bootstrap the library services in a way or another before creating an EVL mutex.\n  #include \u003cevl/mutex.h\u003e static struct evl_mutex mutex; void create_new_mutex(void) { int fd; /* Create a (private) non-recursive mutex with priority inheritance enabled. */ fd = evl_create_mutex(mutex, EVL_CLOCK_MONOTONIC, 0, EVL_MUTEX_NORMAL, \"name_of_mutex\"); /* skipping checks */ return fd; }   int evl_new_mutex(struct evl_mutex *mutex, const char *fmt, ...)  This call is a shorthand for creating a normal (non-recursive) mutex enabling the priority inheritance protocol, timed on the built-in EVL monotonic clock. It is identical to calling:\nevl_create_mutex(mutex, EVL_CLOCK_MONOTONIC, 0, EVL_MUTEX_NORMAL|EVL_CLONE_PRIVATE, fmt, ...);  ИнформацияNote that if the [generated name] (/ru/core/user-api/#element-naming-convention) starts with a slash ('/') character, EVL_CLONE_PRIVATE would be automatically turned into EVL_CLONE_PUBLIC internally.\n   EVL_MUTEX_INITIALIZER((const char *) name, (int) clockfd, (int) ceiling, (int) flags)  The static initializer you can use with events. All arguments to this macro refer to their counterpart in the call to evl_create_mutex().\n/* Create a (public) recursive mutex with priority ceiling enabled (prio=90). */ struct evl_mutex mutex = EVL_MUTEX_INITIALIZER(\"name_of_mutex\", EVL_CLOCK_MONOTONIC, 90, EVL_MUTEX_RECURSIVE|EVL_CLONE_PUBLIC); /* which is strictly equivalent to: */ struct evl_mutex mutex = EVL_MUTEX_INITIALIZER(\"/name_of_mutex\", EVL_CLOCK_MONOTONIC, 90, EVL_MUTEX_RECURSIVE);   int evl_open_mutex(struct evl_mutex *mutex, const char *fmt, ...)  You can open an existing mutex, possibly from a different process, by calling evl_open_mutex().\nmutexAn in-memory mutex descriptor is constructed by evl_open_mutex(), which contains ancillary information other calls will need. mutex is a pointer to such descriptor of type struct evl_mutex. The information is retrieved from the existing mutex which was opened.\n\nfmtA printf-like format string to generate the name of the mutex to open. This name must exist in the EVL device file hierarchy at /dev/evl/monitor. See this description of the naming convention.\n\n...The optional variable argument list completing the format.\n\nevl_open_mutex() returns the file descriptor referring to the opened mutex on success, Otherwise, a negated error code is returned:\n  -EINVAL\tThe name refers to an existing object, but not to a mutex.\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -ENOMEM\tNo memory available.\n    int evl_lock_mutex(struct evl_mutex *mutex)  The current thread may lock a mutex by calling this service, which guarantees exclusive access to the resource it protects on success, until it eventually releases mutex.\nIf mutex is unlocked, or it is of recursive type and the current thread already owns it on entry to the call, the lock nesting count is incremented by one then evl_lock_mutex() returns immediately with a success status.\nOtherwise, the caller blocks until the current owner releases it by a call to evl_unlock_mutex(). If multiple threads wait for acquiring the lock, the one with the highest scheduling priority which has been waiting for the longest time is served first.\nЗаметкаAs long as the caller holds an EVL mutex, switching to in-band mode is wrong since this would introduce a priority inversion. For this reason, EVL threads which undergo the SCHED_WEAK policy are kept running out-of-band by the core until the last mutex they have acquired is dropped.\n mutexThe in-memory mutex descriptor constructed by either evl_create_mutex or evl_open_mutex(), or statically built with EVL_MUTEX_INITIALIZER. In the latter case, an implicit call to evl_create_mutex() is issued for mutex before a locking operation is attempted, which may trigger a transition to the in-band execution mode for the caller.\n\nevl_lock_mutex() returns zero on success. Otherwise, a negated error code may be returned if:\n-EAGAIN mutex is recursive and the lock nesting count would exceed 2^31.\n-EINVAL\tmutex does not represent a valid in-memory mutex descriptor. If that pointer is out of the caller’s address space or points to read-only memory, the caller bluntly gets a memory access exception.\nIf mutex was statically initialized with EVL_MUTEX_INITIALIZER, then any error returned by evl_create_mutex() may be passed back to the caller in case the implicit initialization call fails.\n  int evl_timedlock_mutex(struct evl_mutex *mutex, const struct timespec *timeout)  This call is a variant of evl_lock_mutex() which allows specifying a timeout on the locking operation, so that the caller is unblocked after a specified delay without being able to acquire the lock.\nmutexThe in-memory mutex descriptor constructed by either evl_create_mutex or evl_open_mutex(), or statically built with EVL_MUTEX_INITIALIZER. In the latter case, an implicit call to evl_create_mutex() is issued for mutex before a locking operation is attempted, which may trigger a transition to the in-band execution mode for the caller.\n\ntimeoutA time limit to wait for the caller to be unblocked before the call returns on error. The clock mentioned in the call to evl_create_mutex() will be used for tracking the elapsed time.\n\nThe possible return values include any status from evl_lock_mutex(), plus:\n-ETIMEDOUT\tThe timeout fired, after the amount of time specified by timeout.\n  int evl_trylock_mutex(struct evl_mutex *mutex)  This call attempts to lock the mutex like evl_lock_mutex() would do, but returns immediately both on success or failure to do so, without waiting for the lock to be available in the latter case.\nmutexThe in-memory mutex descriptor constructed by either evl_create_mutex or evl_open_mutex(), or statically built with EVL_MUTEX_INITIALIZER. In the latter case, an implicit call to evl_create_mutex() for mutex is issued before a wait is attempted, which may trigger a transition to the in-band execution mode for the caller.\n\nevl_trylock_mutex() returns zero on success. Otherwise, a negated error code may be returned if:\n-EAGAIN mutex is already locked by another thread.\n-EINVAL\tmutex does not represent a valid in-memory mutex descriptor. If that pointer is out of the caller’s address space or points to read-only memory, the caller bluntly gets a memory access exception.\nIf mutex was statically initialized with EVL_MUTEX_INITIALIZER, then any error returned by evl_create_mutex() may be passed back to the caller in case the implicit initialization call fails.\n  int evl_unlock_mutex(struct evl_mutex *mutex)  This routine releases a mutex previously acquired by evl_lock_mutex(), evl_trylock_mutex() or evl_timedlock_mutex() by the current EVL thread.\nЗаметкаOnly the thread which acquired an EVL mutex may release it.\n mutexThe in-memory mutex descriptor constructed by either evl_create_mutex or evl_open_mutex().\n\nIf mutex is recursive, the lock nesting count is decremented by one until it reaches zero, at which point the lock is actually released.\nevl_unlock_mutex() returns zero on success. Otherwise, a negated error code may be returned if:\n-EPERM\tThe caller does not own the mutex.\n-EINVAL\tmutex does not represent a valid in-memory mutex descriptor. If that pointer is out of the caller’s address space or points to read-only memory, the caller bluntly gets a memory access exception.\n  int evl_set_mutex_ceiling(struct evl_mutex *mutex, unsigned int ceiling)  Change the priority ceiling value for a mutex. If the mutex is currently owned by a thread, the change may not apply immediately, depending on whether the priority ceiling was committed for that thread already (see this note).\nmutexThe in-memory mutex descriptor constructed by either evl_create_mutex or evl_open_mutex().\n\nceilingThe new priority ceiling in the [1-99] range.\n\nevl_set_mutex_ceiling returns zero on success. Otherwise, a negated error code may be returned if:\n-EINVAL\tmutex does not enable priority protection.\n-EINVAL\tceiling is out of range.\n-EINVAL\tmutex does not represent a valid in-memory mutex descriptor. If that pointer is out of the caller’s address space or points to read-only memory, the caller bluntly gets a memory access exception.\n  int evl_get_mutex_ceiling(struct evl_mutex *mutex)  Retrieve the current priority ceiling value for mutex. This call also succeeds for a mutex which does not enable priority protection (but priority inheritance instead), returning zero in such a case.\nmutexThe in-memory mutex descriptor constructed by either evl_create_mutex or evl_open_mutex().\n\nevl_get_mutex_ceiling returns the current priority ceiling value on success. Otherwise, a negated error code may be returned if:\n-EINVAL\tmutex does not represent a valid in-memory mutex descriptor. If that pointer is out of the caller’s address space or points to read-only memory, the caller bluntly gets a memory access exception.\n  int evl_close_mutex(struct evl_mutex *mutex)  You can use evl_close_mutex to dispose of an EVL mutex, releasing the associated file descriptor, at which point mutex will not be valid for any subsequent operation from the current process. However, this mutex is kept alive in the EVL core until all file descriptors opened on it by call(s) to evl_open_mutex() have been released, whether from the current process or any other process.\nmutexThe in-memory descriptor of the mutex to dismantle.\n\nevl_close_mutex returns zero upon success. Otherwise, a negated error code is returned:\n-EINVAL\tmutex does not represent a valid in-memory mutex descriptor. If that pointer is out of the caller’s address space or points to read-only memory, the caller bluntly gets a memory access exception.\nClosing a statically initialized mutex descriptor which has never been locked always returns zero.\n Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Mutex",
    "uri": "/ru/core/user-api/mutex/"
  },
  {
    "content": "Since we have service calls between the applications and the EVL core, we depend on a certain format of request codes, arguments and return values when exchanging information between both ends, like the contents of the argument structure passed to the various ioctl(2) and oob_ioctl() requests. This convention forms the EVL ABI. As hard as we might try to keep the ABI stable over time, there are times when this cannot be achieved, at the very least because we may want to make new features added to the EVL core visible to applications. Generally speaking, EVL being a new kid in the old dual kernel town, it is too early - for the time being - to write the current convention in stone. This fact of life calls for a way to unambiguously:\n  determine which (range of subsequent) revision(s) of the ABI is implemented by the running EVL core, from the application side.\n  state which revision of the ABI an application requires.\n  In some instance, the EVL core may also be able to provide a backward-compatible interface to applications which spans multiple subsequent ABI revisions. Because ABI stability is a strong goal, backward compatibility may become the norm as the EVL core feature set stabilizes.\nTo this end, the following apply (since ABI revision #19):\n  an ABI is versioned. Each version is represented by a non-null integer starting at 1, incremented by one after each revision.\n  the EVL core defines two symbols, namely EVL_ABI_BASE and EVL_ABI_LEVEL, which represent the range of ABI revisions the core supports (inclusively), from oldest (EVL_ABI_BASE) to current (EVL_ABI_LEVEL) respectively. Differing values means that the EVL core honors multiple subsequent revisions by way of backward compatible support of some sort.\n  СоветBy convention, changes to EVL_ABI_BASE and/or EVL_ABI_LEVEL belong to the same GIT commit which triggered the ABI revision. This way, it is easy to map a revision to a particular change unambiguously. For the same reason, non-related changes to the ABI appear in separate commits, each one forming a particular ABI revision.\n   libevl states the ABI revision it follows by defining the EVL_ABI_PREREQ symbol. When libevl initializes, this prerequisite is checked against the range of ABI revisions the running EVL core advertises. If a mismatch is detected, the initialization fails and -ENOEXEC is returned to the caller; otherwise the execution proceeds normally. An easy way to figure out which ABI a libevl installation requires is to ask the evl command about it as follows:\n~ # evl -V evl.0 -- #1c6115c (2020-03-06 16:24:00 +0100) [requires ABI 19]   Locating ABI boundaries and prereqs {git-evlabi} When developing (with) the EVL core, it may sometimes be useful to find which code changes in the GIT commit history correspond to ABI revisions and conversely. It may also be useful to determine the history of ABI prerequisites in libevl, so that you can map any particular point the commit history with the ABI revision it is compatible with. For this, you can use a simple script called git-evlabi, which is available in the linux-evl repository. You just need to make sure this script is executable and reachable from the $PATH variable.\ngit-evlabi looks for EVL ABI definitions or prereqs in GIT trees cloned from either linux-evl or libevl respectively. It does so depending on the value and type of the GIT object mentioned (or not) in the command:\n$ git evlabi -h usage: git-evlabi [-r \u003crev\u003e][--git-dir \u003cdir\u003e][-s][-h] [\u003cobject\u003e]   if \u003cobject\u003e is a regular commit (SHA-1) hash, it extracts the ABI information for that particular commit.\n  if \u003cobject\u003e matches a GIT refname (typically a branch name), it scans the commit history backward from that point, displaying the ABI information for each revision encountered during the traversal.\n  if \u003cobject\u003e is omitted, it defaults to HEAD, which falls back to the refname case above.\n  The output is of the form:\n\u003cstart\u003e \u003crange\u003e \u003crevision\u003e [\u003cshortlog\u003e], where:\n  \u003cstart\u003e is the predecessor of the earliest commit implementing/requesting the ABI revision\n  \u003crange\u003e is the span of the ABI revision in the history (usable with git log for instance).\n  \u003crevision\u003e is the ABI revision number\n  \u003cshortlog\u003e is the subject line describing \u003cstart\u003e\n   e.g. asking for all EVL ABI revisions in the kernel tree\n {rpm@cobalt} cd ~/git/linux-evl \u0026\u0026 git evlabi evl/master 4bc9b71134f0 96c899ef3a6c..bc7ca4ad0b5c 19 02ae0b8783cb 7f082bc5bef1..96c899ef3a6c 18 c2a51a7e68a7 56e217799766..7f082bc5bef1 17 5266e26ca729 857c199beead..56e217799766 16 54559813955b 91f7d9d3bc73..857c199beead 15 fb0de980dac9 7795aa5979a1..91f7d9d3bc73 14 437fee2b84ee 1b627b1977a1..7795aa5979a1 13 60c937d72a72 03f35888596c..1b627b1977a1 12 db431047b9bd 68b176e50081..03f35888596c 11 8f5b07b678ea 92f04516c8dd..68b176e50081 10 b0eb03db5c77 8f9bb48957ba..92f04516c8dd 9 b5f675b3e2c0 5e640e734385..8f9bb48957ba 8 7892bfe4886e 9ddba326d41b..5e640e734385 7 9ddba326d41b 61e9a6b78aa5..9ddba326d41b 6 badb3bf8cee8 ba708547b903..61e9a6b78aa5 5 ba708547b903 7d9aa309a81d..ba708547b903 4 06b78dd09bfa 3dd263191153..7d9aa309a81d 3 6170f4f1d9fd fd56dfbfdbbf..3dd263191153 2 800484643595 f8ef2539553e..fd56dfbfdbbf 1 f8ef2539553e fec5d8902f49..f8ef2539553e 0 With the information above, it is easy to figure out the span of a given ABI revision in the kernel code (here ABI 17 starting at commit #c2a51a7e68a7), as follows:\n e.g. looking for span of ABI 17 in the kernel tree\n {rpm@cobalt} git log --oneline 56e217799766..7f082bc5bef1 7f082bc5bef1 evl/work: export interface to modules 54b365d416c7 evl/stax: export interface to modules c268cadb0396 evl/stax: force the caller to in-band mode on WOSX 77d1de6add6e evl/clock: y2038: remove use of struct timex 05fc1d0e5e96 evl/thread: fix rescheduling leak c2a51a7e68a7 evl/thread: replace SIGEVL_ACTION_HOME with a RETUSER event The same logic applies to the libevl tree, this time for determining the ABI prerequisites of commits in the API.\n e.g. walking the history of ABI prerequisites\n {rpm@cobalt} git evlabi next dde33b5 1c6115c..dde33b5 19 Which means that looking at the current state of branch next in the libevl tree, ABI revision 19 in the EVL core starts being a prerequisite for the code starting at commit #dde33b5.\nConversely, you could ask for matching a particular ABI revision to a commit, using the -r switch.\n e.g. matching a particular ABI revision (asking for the subject line too)\n {rpm@cobalt} git evlabi -r 16 -s 5266e26ca729 857c199beead..56e217799766 16 evl: introduce synchronous breakpoint support  Информацияgit-evlabi cannot detect ABI revisions earlier than 19 in the libevl tree, since the marker it looks for was introduced by that particular revision.\n List of ABI revisions This document summarizes the various ABI revisions and their respective purpose, matching them to the corresponding libevl release implementing the change.\n Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC content/core/under-the-hood/abi.md 09017f1 ",
    "description": "",
    "tags": null,
    "title": "The EVL ABI",
    "uri": "/ru/core/under-the-hood/abi/"
  },
  {
    "content": "Этот раздел (незавершенное производство) поможет вам сориентироваться в разработке и реализации ядра EVL. Он написан с точки зрения разработчика в качестве практического примера реализации сопутствующего ядра, живущего в ядре Linux, с акцентом на деталях того, как для этой цели используется интерфейс Dovetail. Эта информация предназначена для того, чтобы помочь всем, кто интересуется или просто интересуется “другим путем к режиму реального времени в Linux”, независимо от того, полезно ли это для разработки вашей собственной двухъядерной системы на базе Linux, внесения вклада в EVL или в образовательных целях.\nHow do applications request services from the EVL core? An EVL application interacts with so-called EVL elements. Each element usually exports a mix of in-band and out-of-band file operations implemented by the EVL core (in a few cases, only either side is implemented). Therefore, an EVL application sees every core element as a file, and uses a regular file descriptor to interact with it, either directly or indirectly by calling routines from the standard glibc or libevl whenever there is a real-time requirement.\nTo sum up, an application (indirectly) issues file I/O requests on element devices to obtain services from the EVL core. The system calls involved in the interface between an application and the EVL core are exclusively open(2), close(2), read(2), write(2), ioctl(2), mmap(2) for the in-band side, and oob_ioctl(), oob_read(), oob_write() for the out-of-band side.\nlibevl hides the nitty-gritty details of forming these I/O requests, presenting a high-level API to be used in applications.\nWhere are the EVL core services implemented? The EVL core can be described as a multi-device driver, each device type representing an EVL element. From the standpoint of the Linux device driver model, the EVL core is composed of a set of character-based device drivers, one per element type. The core is implemented under the kernel/evl hierarchy.\nElement creation The basic operation every element driver implements is cloning, which creates a new instance of the element type upon request from the application. For instance, a thread element is created each time evl_attach_self() is invoked from libevl. To do so, create_evl_element() sends a request to the special clone device exported by the core at /dev/evl/thread/clone. Upon return, the new thread element is live, and can be accessed by opening the newly created device at /dev/evl/thread/\u003cname\u003e, where \u003cname\u003e is the thread name passed to evl_attach_self(). Eventually, the file descriptor obtained is usable for issuing requests for core services to that particular element instance via file I/O requests.\nThe same logic applies to all other types of named elements, such as proxies, cross-buffers or monitors which underlie mutexes, events, semaphores and flags.\nElement factory In order to avoid code duplication, the EVL core implements a so-called element factory. The factory refers to EVL class descriptors of type struct evl_factory, which describes how a particular element type should be handled by the generic factory code.\nThe factory performs the following tasks:\n  it populates the initial device hierarchy under /dev/evl so that applications can issue requests to the EVL core. The main aspect of this task is to register a Linux device class for each element type, creating the related clone device.\n  it implements the generic portion of the ioctl(EVL_IOC_CLONE) request, eventually calling the proper type-specific handler, such as thread_factory_build() for threads, monitor_factory_build() for monitors and so on.\n  it maintains a reference count on every element instantiated in the system, so as to automatically remove elements when they have no more referrers. Typically, closing the last file descriptor referring to the file underlying an element would cause such removal (unless some kernel code is still withholding references to the same element).\n  ИнформацияUnlike other elements, a thread may exist in absence of any file reference. The disposal still happens automatically when the thread exits or voluntarily detaches by calling evl_detach_self().\n ",
    "description": "",
    "tags": null,
    "title": "Под капотом",
    "uri": "/ru/core/under-the-hood/"
  },
  {
    "content": "Synchronizing on a semaphore EVL implements the classic Dijkstra semaphore construct, with an API close to the POSIX specification for the basic operations.\nSemaphore services   int evl_create_sem(struct evl_sem *sem, int clockfd, init initval, int flags, const char *fmt, ...)  This call creates a semaphore, returning a file descriptor representing the new object upon success. This is the generic call form; for creating a semaphore with common pre-defined settings, see [evl_new_sem()}(#evl_new_sem).\nsemAn in-memory semaphore descriptor is constructed by evl_create_sem(), which contains ancillary information other calls will need. sem is a pointer to such descriptor of type struct evl_sem.\n\nclockfdSome semaphore-related calls are timed like evl_timedget_sem() which receives a timeout value. You can specify the EVL clock this timeout refers to by passing its file descriptor as clockfd. Built-in EVL clocks are accepted here.\n\ninitvalThe initial value of the semaphore count.\n\nflagsA set of creation flags for the new element, defining its visibility:\n  EVL_CLONE_PUBLIC denotes a public element which is represented by a device file in the /dev/evl file hierarchy, which makes it visible to other processes for sharing.\n  EVL_CLONE_PRIVATE denotes an element which is private to the calling process. No device file appears for it in the /dev/evl file hierarchy.\n  EVL_CLONE_NONBLOCK sets the file descriptor of the new semaphore in non-blocking I/O mode (O_NONBLOCK). By default, O_NONBLOCK is cleared for the file descriptor.\n  \nfmtA printf-like format string to generate the semaphore name. See this description of the [naming convention] (/ru/core/user-api/#element-naming-convention).\n\n...The optional variable argument list completing the format.\n\nevl_create_sem() returns the file descriptor of the newly created semaphore on success. Otherwise, a negated error code is returned:\n  -EEXIST\tThe generated name is conflicting with an existing mutex, event, semaphore or flag group name.\n  -EINVAL\tEither clockfd does not refer to a valid EVL clock, or the generated semaphore name is badly formed, likely containing invalid character(s), such as a slash. Keep in mind that it should be usable as a basename of a device element’s file path.\n  -ENAMETOOLONG\tThe overall length of the device element’s file path including the generated name exceeds PATH_MAX.\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -ENOMEM\tNo memory available.\n  -ENXIO The EVL library is not initialized for the current process. Such initialization happens implicitly when evl_attach_self() is called by any thread of your process, or by explicitly calling evl_init(). You have to bootstrap the library services in a way or another before creating an EVL semaphore.\n  #include \u003cevl/sem.h\u003e static struct evl_sem sem; void create_new_sem(void) { int fd; fd = evl_create_sem(sem, EVL_CLOCK_MONOTONIC, 1, EVL_CLONE_PRIVATE, \"name_of_semaphore\"); /* skipping checks */ return fd; }   int evl_new_sem(struct evl_sem *sem, const char *fmt, ...)  This call is a shorthand for creating a zero-initialized private semaphore, timed on the built-in EVL monotonic clock. It is identical to calling:\nevl_create_sem(sem, EVL_CLOCK_MONOTONIC, 0, EVL_CLONE_PRIVATE, fmt, ...);  ИнформацияNote that if the [generated name] (/ru/core/user-api/#element-naming-convention) starts with a slash ('/') character, EVL_CLONE_PRIVATE would be automatically turned into EVL_CLONE_PUBLIC internally.\n   EVL_SEM_INITIALIZER((const char *) name, (int) clockfd, (int) initval, (int) flags)  The static initializer you can use with semaphores. All arguments to this macro refer to their counterpart in the call to evl_create_sem().\nstruct evl_sem sem = EVL_SEM_INITIALIZER(\"name_of_semaphore\", EVL_CLOCK_MONOTONIC, 1, EVL_CLONE_PUBLIC);   int evl_open_sem(struct evl_sem *sem, const char *fmt, ...)  You can open an existing semaphore, possibly from a different process, by calling evl_open_sem().\nsemAn in-memory semaphore descriptor is constructed by evl_open_sem(), which contains ancillary information other calls will need. sem is a pointer to such descriptor of type struct evl_sem. The information is retrieved from the existing semaphore which was opened.\n\nfmtA printf-like format string to generate the name of the semaphore to open. This name must exist in the EVL device file hierarchy at /dev/evl/monitor. See this description of the naming convention.\n\n...The optional variable argument list completing the format.\n\nevl_open_sem() returns the file descriptor referring to the opened semaphore on success, Otherwise, a negated error code is returned:\n  -EINVAL\tThe name refers to an existing object, but not to a semaphore.\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -ENOMEM\tNo memory available.\n    int evl_get_sem(struct evl_sem *sem)  This service decrements a semaphore by one. If the resulting semaphore value is negative, the caller is put to sleep by the core until a subsequent call to evl_put_sem() eventually releases it. Otherwise, the caller returns immediately. Waiters are queued by order of scheduling priority.\nsemThe in-memory semaphore descriptor constructed by either evl_create_sem() or evl_open_sem(), or statically built with EVL_SEM_INITIALIZER. In the latter case, an implicit call to evl_create_sem() for sem is issued before a get operation is attempted, which may trigger a transition to the in-band execution mode for the caller.\n\nevl_get_sem() returns zero on success. Otherwise, a negated error code may be returned if:\n-EINVAL\tsem does not represent a valid in-memory semaphore descriptor. If that pointer is out of the caller’s address space or points to read-only memory, the caller bluntly gets a memory access exception.\nIf sem was statically initialized with EVL_SEM_INITIALIZER, then any error returned by evl_create_sem() may be passed back to the caller in case the implicit initialization call fails.\n  int evl_timedget_sem(struct evl_sem *sem, const struct timespec *timeout)  This call is a variant of evl_get_sem() which allows specifying a timeout on the get operation, so that the caller is unblocked after a specified delay sleeping without being unblocked by a subsequent call to evl_put_sem().\nsemThe in-memory semaphore descriptor constructed by either evl_create_sem() or evl_open_sem(), or statically built with EVL_SEM_INITIALIZER. In the latter case, an implicit call to evl_create_sem() is issued for sem before a get operation is attempted, which may trigger a transition to the in-band execution mode for the caller.\n\ntimeoutA time limit to wait for the caller to be unblocked before the call returns on error. The clock mentioned in the call to evl_create_sem() will be used for tracking the elapsed time.\n\nThe possible return values include any status from evl_get_sem(), plus:\n-ETIMEDOUT\tThe timeout fired, after the amount of time specified by timeout.\n  int evl_put_sem(struct evl_sem *sem)  This call posts a semaphore by incrementing its count by one. If a thread is sleeping on the semaphore as a result of a previous call to evl_get_sem() or evl_timedget_sem(), the thread heading the wait queue is unblocked.\nsemThe in-memory semaphore descriptor constructed by either evl_create_sem() or evl_open_sem(), or statically built with EVL_SEM_INITIALIZER. In the latter case, an implicit call to evl_create_sem() for sem is issued before the semaphore is posted, which may trigger a transition to the in-band execution mode for the caller.\n\nevl_put_sem() returns zero upon success. Otherwise, a negated error code is returned:\n-EINVAL\tsem does not represent a valid in-memory semaphore descriptor. If that pointer is out of the caller’s address space or points to read-only memory, the caller bluntly gets a memory access exception.\nIf sem was statically initialized with EVL_SEM_INITIALIZER but not passed to any semaphore-related call yet, then any error status returned by evl_create_sem() may be passed back to the caller in case the implicit initialization call fails.\n  int evl_tryget_sem(struct evl_sem *sem, int *r_bits)  This call attempts to decrement the semaphore provided the result does not lead to a negative count.\nsemThe in-memory semaphore descriptor constructed by either evl_create_sem() or evl_open_sem(), or statically built with EVL_SEM_INITIALIZER. In the latter case, an implicit call to evl_create_sem() for sem is issued before a wait is attempted, which may trigger a transition to the in-band execution mode for the caller.\n\nevl_tryget_sem() returns zero on success. Otherwise, a negated error code may be returned if:\n-EAGAIN sem count value is zero or negative at the time of the call.\n-EINVAL\tsem does not represent a valid in-memory semaphore descriptor. If that pointer is out of the caller’s address space or points to read-only memory, the caller bluntly gets a memory access exception.\nIf sem was statically initialized with EVL_SEM_INITIALIZER, then any error returned by evl_create_sem() may be passed back to the caller in case the implicit initialization call fails.\n  int evl_peek_sem(struct evl_sem *sem, int *r_val)  This call returns the count value of the semaphore. If a negative count is returned in *r_val, its absolute value can be interpreted as the number of waiters sleeping on the semaphore’s wait queue (at the time of the call). A zero or positive value means that the semaphore is not contended.\nsemThe in-memory semaphore descriptor constructed by either evl_create_sem() or evl_open_sem(), or statically built with EVL_SEM_INITIALIZER. In the latter case, the semaphore becomes valid for a call to evl_peek_sem() only after a put or [try]get operation was issued for it.\n\nr_valThe address of an integer which contains the semaphore value on successful return from the call.\n\nevl_peek_sem() returns zero on success along with the current semaphore count. Otherwise, a negated error code may be returned if:\n-EINVAL\tsem does not represent a valid in-memory semaphore descriptor. If that pointer is out of the caller’s address space or points to read-only memory, the caller bluntly gets a memory access exception.\n  int evl_close_sem(struct evl_sem *sem)  You can use evl_close_sem() to dispose of an EVL semaphore, releasing the associated file descriptor, at which point sem will not be valid for any subsequent operation from the current process. However, this semaphore is kept alive in the EVL core until all file descriptors opened on it by call(s) to evl_open_sem() have been released, whether from the current process or any other process.\nsemThe in-memory descriptor of the semaphore to dismantle.\n\nevl_close_sem() returns zero upon success. Otherwise, a negated error code is returned:\n-EINVAL\tsem does not represent a valid in-memory semaphore descriptor. If that pointer is out of the caller’s address space or points to read-only memory, the caller bluntly gets a memory access exception.\nClosing a statically initialized semaphore descriptor which has never been used in get or put operations always returns zero.\n Events pollable from a semaphore file descriptor The evl_poll() interface can monitor the following events occurring on a semaphore file descriptor:\n POLLIN and POLLRDNORM are set whenever the semaphore count is strictly positive, which means that a subsequent attempt to deplete it by a call to evl_get_sem(), evl_tryget_sem() or evl_timedget_sem() might be successful without blocking (i.e. unless another thread sneaks in in the meantime and fully depletes the semaphore).   Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Semaphore",
    "uri": "/ru/core/user-api/semaphore/"
  },
  {
    "content": "Using the observer pattern for event-driven applications The EVL core provides a simple yet flexible support for implementing the observer design pattern in your application, based on the Observable element. The Observable receives a stream of so-called notices, which may be events, state changes or any data which fits within a 64bit word, delivering them as notifications (with additional meta-data) to subscribed threads called observers, when they ask for it. Subscribers do not have to be attached to the EVL core, any thread can observe an Observable element. Whether the threads which produce the events, the Observable and the observers live in the same or different processes is transparent.\nBroadcast mode By default, an Observable broadcasts a copy of every event it receives to every subscribed observer:\n    Master mode In some cases, you may want to use an Observable to dispatch work submitted as events to the set of observers which forms a pool of worker threads. Each message would be sent to a single worker, each worker would be picked on a round-robin basis so as to implement a naive load-balancing strategy. Setting the Observable in the so-called master mode (see EVL_CLONE_MASTER) at creation time enables this behavior:\n    Creating an Observable   int evl_create_observable(int flags, const char *fmt, ...)  This call creates an Observable element, returning a file descriptor representing the new object upon success. This is the generic call form; for creating an Observable with common pre-defined settings, see evl_new_observable().\nAn Observable can operate either in broadcast or master mode:\n  in broadcast mode, a copy of every event received by the Observable is sent to every observer.\n  in master mode, each message received by the Observable is sent to a single observer. The subscriber to send a message to is picked according to a simple round-robin strategy among all observers.\n  flagsA set of creation flags for the new element, defining its visibility and operation mode:\n  EVL_CLONE_PUBLIC denotes a public element which is represented by a device file in the /dev/evl file hierarchy, which makes it visible to other processes for sharing.\n  EVL_CLONE_PRIVATE denotes an element which is private to the calling process. No device file appears for it in the /dev/evl file hierarchy.\n  EVL_CLONE_MASTER enables the master mode for the Observable.\n  EVL_CLONE_NONBLOCK sets the file descriptor of the new Observable in non-blocking I/O mode (O_NONBLOCK). By default, O_NONBLOCK is cleared for the file descriptor.\n  \nfmtA printf-like format string to generate the Observable name. See this description of the [naming convention] (/ru/core/user-api/#element-naming-convention).\n\n...The optional variable argument list completing the format.\n\nevl_create_observable() returns the file descriptor of the newly created Observable on success. Otherwise, a negated error code is returned:\n  -EEXIST\tThe generated name is conflicting with an existing Observable name.\n  -EINVAL\tEither flags is wrong, or the generated name is badly formed.\n  -ENAMETOOLONG\tThe overall length of the device element’s file path including the generated name exceeds PATH_MAX.\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -ENOMEM\tNo memory available.\n  -ENXIO The EVL library is not initialized for the current process. Such initialization happens implicitly when evl_attach_self() is called by any thread of your process, or by explicitly calling evl_init(). You have to bootstrap the library services in a way or another before creating an EVL Observable.\n  #include \u003cevl/observable.h\u003e void create_new_observable(void) { int efd; efd = evl_create_observable(EVL_CLONE_PUBLIC|EVL_CLONE_MASTER, \"name_of_observable\"); /* skipping checks */ return efd; }   int evl_new_observable(const char *fmt, ...)  This call is a shorthand for creating a private observable operating in broadcast mode. It is identical to calling:\nevl_create_observable(EVL_CLONE_PRIVATE, fmt, ...);  ИнформацияNote that if the [generated name] (/ru/core/user-api/#element-naming-convention) starts with a slash ('/') character, EVL_CLONE_PRIVATE would be automatically turned into EVL_CLONE_PUBLIC internally.\n  Working with an Observable Once an Observable is created, using it entails the following steps, performed by either the thread(s) issuing the stream of events to the Observable, or those observing these events:\nObservers need to subscribe to the Observable. Observers can subscribe and unsubscribe at will, come and go freely during the Observable’s lifetime. Depending on its subscription flags, the observer may ask for merging consecutive identical notices or receiving all of them unfiltered (See EVL_NOTIFY_ONCHANGE).\n    Event streamers can send notices to the Observable by calling evl_update_observable().\n    Observers can read notifications from the Observable by calling evl_read_observable().\n    Once an Observable has become useless, you only need to close all the file descriptors referring to it in order to release it, like for any ordinary file.\n  int evl_update_observable(int efd, const struct evl_notice *ntc, int nr)  This call sends up to nr notices starting at address ntc in memory to the Observable referred to by efd. This call never blocks the caller. It may be used by any thread, including non-EVL ones.\nA notice contains a tag, and an opaque event value. It is defined by the following C types:\nunion evl_value { int32_t val; int64_t lval; void *ptr; }; struct evl_notice { uint32_t tag; union evl_value event; }; All notices sent to the Observable should carry a valid issuer tag in the evl_notice.tag field. For applications, a valid tag is an arbitrary value greater or equal to EVL_NOTICE_USER (lower tag values are reserved to the core for HM diag codes).\nevl_update_observable() returns the number of notices which have been successfully queued. Each notice which have been successfully queued for consumption by at least one observer counts for one in the return value. Zero is returned whenever no observer was subscribed to the Observable at the time of the call, or no buffer space was available for queuing any notification for any observer. Otherwise, a negated error code is returned on error:\n  -EBADF\tefd is not a valid file descriptor.\n  -EPERM\tefd does not refer to an Observable element. This may happen if efd refers to a valid EVL thread which was not created with the EVL_CLONE_OBSERVABLE flag.\n  -EINVAL\tSome notice has an invalid tag.\n  -EFAULT\tif ntc points to invalid memory.\n   Sending a notice to an observable\n void send_notice(int ofd) { struct evl_notice notice; int ret; notice.tag = EVL_NOTICE_USER; notice.event.val = 42; ret = evl_update_observable(ofd, \u0026notice, 1); /* ret should be 1 on success. */ }   int evl_read_observable(int efd, struct evl_notification *nf, int nr)  This call receives up to nr notifications starting at address nf in memory from the Observable referred to by efd. It may only be used by observers subscribed to this particular Observable. If O_NONBLOCK is clear for efd, the caller might block until at least one notification arrives.\nA notification contains the tag and the event value sent by the issuer of the corresponding notice, plus some meta-data. A notification is defined by the following C type:\nunion evl_value { int32_t val; int64_t lval; void *ptr; }; struct evl_notification { uint32_t tag; uint32_t serial; int32_t issuer; union evl_value event; struct timespec date; }; The meta-data is defined as follows:\n  serial is a monotonically increasing counter of notices sent by the Observable referred to by efd. In broadcast mode, this serial number is identical in every copy of a given original notice forwarded to the observers present.\n  issuer is the pid of the thread which issued the notice, zero if it was sent by the EVL core, such as with HM diagnostics.\n  date is a timestamp at receipt of the original notice, based on the built-in EVL_CLOCK_MONOTONIC clock.\n  evl_read_observable() returns the number of notifications which have been successfully copied to nf. This count may be lower than nr. Otherwise, a negated error code is returned on error:\n  -EAGAIN\tO_NONBLOCK is set for efd and no notification is pending at the time of the call.\n  -EBADF\tefd is not a valid file descriptor.\n  -EPERM\tefd does not refer to an Observable element. This may happen if efd refers to a valid EVL thread which was not created with the EVL_CLONE_OBSERVABLE flag.\n  -ENXIO\tthe calling thread is not subscribed to the Observable referred to by efd.\n  -EFAULT\tif nf points to invalid memory.\n   Receiving a notification from an observable\n void receive_notification(int ofd) { struct evl_notification notification; int ret; ret = evl_read_observable(ofd, \u0026notification, 1); /* ret should be 1 on success. */ }  Events pollable from an Observable The evl_poll() and poll(2) interfaces can monitor the following events occurring on an Observable:\n  POLLIN and POLLRDNORM are set whenever at least one notification is pending for the calling observer thread. This means that a subsequent call to evl_read_observable() by the same thread would return at least one valid notification immediately.\n  POLLOUT and POLLWRNORM are set whenever at least one observer subscribed to the Observable has enough room in its backlog to receive at least one notice. This means that a subsequent call to evl_update_observable() would succeed in queuing at least one notice to one observer. In case multiple threads may update the Observable concurrently, which thread might succeed in doing so cannot be determined (typically, there would be no such guarantee for the caller of evl_poll() and poll(2)).\n  In addition to these flags, POLLERR might be returned in case the caller did not subscribe to the Observable, or some file descriptor from the polled set refer to an EVL thread which was not created with the EVL_CLONE_OBSERVABLE flag.\nObserving EVL threads An EVL thread is in and of itself an Observable element. Observability of a thread can be enabled by passing the EVL_CLONE_OBSERVABLE flag when attaching a thread to the EVL core. In this case, the file descriptor obtained from evl_attach_thread() may be subsequently used in Observable-related operations. If the thread was also made public (EVL_CLONE_PUBLIC), then there is a way for remote processes to monitor it via an access to its device.\nThreads can monitor events sent to an observable thread element by subscribing to it. An observable thread typically relays health monitoring information to subscribers. An observable thread can also do introspection, by subscribing to itself then reading the HM diagnostics it has received.\n Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Observable",
    "uri": "/ru/core/user-api/observable/"
  },
  {
    "content": "Zero-latency I/O to in-band files A common issue with dual kernel systems stems from the requirement not to issue in-band system calls while running time-critical code in out-of-band context. Problem is that sometimes, you may need - for instance - to write to regular files such as for logging information to the console or elsewhere from an out-of-band work loop. Doing so by calling common stdio(3) routines directly is therefore not an option for latency reasons.\nThe EVL core solves this problem with the file proxy feature, which can push data to an arbitrary target file, and/or conversely read data pulled from such target file, providing an out-of-band I/O interface to the data producer and consumer threads. This works by offloading the I/O transfers to internal worker threads running in-band which relay the data to/from the target file. For this reason, I/O transfers may be delayed until the in-band stage resumes on a worker’s CPU. For instance, evl_printf() formats then emits the resulting string to fileno(stdout) via an internal proxy created by libevl.so at initialization.\n      Typically, out-of-band writers would send data through the proxy file descriptor using EVL’s oob_write() system call, which in-band readers could receive using the read(2) system call. Conversely, out-of-band readers would receive data through the proxy file descriptor using EVL’s oob_read() system call, which in-band writers could send using the write(2) system call. You can associate any type of file with a proxy, including a socket(2), eventfd(2), signalfd(2), pipe(2) and so on (see also the discussion below about the transfer granularity). This means that you could also use a proxy for signaling an in-band object from the out-of-band context, if doing so can be done using a regular write(2) system call, like eventfd(2) and signalfd(2).\nThe proxy also handles input and output operations from callers running on the in-band stage transparently.\n  ВниманиеStarting from kernel v5.10, a restricted set of file types supports proxying compared to earlier releases, specifically those for which the corresponding driver implements the read_iter(), write_iter() file operations. Regular files, sockets, pipes, tty and eventfd can still be used as proxy targets though.\n Logging debug messages via a proxy For instance, you may want your application to dump debug information to some arbitray file as it runs, including when running time-critical code out-of-band. Admittedly, this would add some overhead, but still low enough to keep the system happy, while giving you precious debug hints. Obviously, you cannot get away with calling the plain printf(3) service for this, since it would downgrade the calling EVL thread to in-band mode. However, you may create a proxy to the debug file:\n#include \u003cfcntl.h\u003e #include \u003cevl/proxy.h\u003e int proxyfd, debugfd; debugfd = open(\"/tmp/debug.log\", O_WRONLY|O_CREAT|O_TRUNC, 0600); ... /* Create a proxy offering a 1 MB buffer. */ proxyfd = evl_new_proxy(debugfd, 1024*1024, \"log:%d\", getpid()); ... Then channel debug output from anywhere in your code you see fit through the proxy this way:\n#include \u003cevl/proxy.h\u003e evl_print_proxy(proxyfd, \"some useful debug information\"); Synchronizing in-band threads on out-of-band events with via a proxy There are a couple of ways which you could use in order to wake up an in-band thread waiting for some event to occur on the out-of-band side of your application, while preventing the waker from being demoted as a result of sending such a signal. One would involve running the in-band thread in the SCHED_WEAK scheduling class, waiting on some EVL synchronization object, such as a semaphore or event flag group. Another one would use a cross-buffer for sending some wakeup datagram from the out-of-band caller to the in-band waiter.\nThe file proxy can also help in implementing such construct, by connecting it to a regular eventfd(2), which can be signaled using the common write(2) system call:\n#include \u003csys/types.h\u003e #include \u003csys/eventfd.h\u003e #include \u003cevl/proxy.h\u003e int evntfd; evntfd = eventfd(0, EFD_SEMAPHORE); ... /* Create a private proxy for output, allow up to 3 notifications to pile up. */ proxyfd = evl_create_proxy(evntfd, sizeof(uint64_t) * 3, sizeof(uint64_t), 0, \"event-relay\"); ... Note the specific granularity value mentioned in the creation call above: we do want the proxy to write 64-bit values at each transfer, in order to cope with the requirements of the eventfd(2) interface. Once the proxy is created, set up for routing all write requests to the regular event file descriptor by 64-bit chunks, the in-band thread can wait for out-of-band events reading from the other side of the channel as follows:\n#include \u003cstdint.h\u003e #include \u003cevl/proxy.h\u003e void oob_waker(int proxyfd) { uint64 val = 1; ssize_t ret; ret = oob_write(proxyfd, \u0026val, sizeof(val)); ... } void inband_sleeper(int evntfd) { ssize_t ret; uint64 val; for (;;) { /* Wait for the next wakeup signal. */ ret = read(evntfd, \u0026val, sizeof(val)); ... } } Export of memory mappings EVL proxies can be also be used for carrying over memory mapping requests to a final device which actually serves the mapping. The issuer of such request does not have to know which device driver will actually be handling that request: the proxy acts as an anchor point agreed between peer processes in order to locate a particular memory-mappable resource, and the proxy simply redirects the mapping requests it receives to the device driver handling the target file it is proxying for.\nExporting process-private memfd memory to the outer world For instance, this usage of a public proxy comes in handy when you need to export a private memory mapping like those obtained by memfd_create(2) to a peer process, assuming you don’t want to deal with the hassle of the POSIX shm_open(3) interface for sharing memory objects. In this case, all this peer has to know is the name of the proxy which is associated with the memory-mappable device. It can open(2) that proxy device in /dev/evl/proxy, then issue the mmap(2) system call for receiving a mapping to the backing device’s memory. From the application standpoint, creating then sharing a 1 KB RAM segment with other peer processes may be as simple as this:\n#include \u003csys/types.h\u003e #include \u003csys/memfd.h\u003e #include \u003csys/mman.h\u003e #include \u003cunistd.h\u003e #include \u003cevl/proxy.h\u003e int memfd, ret; void *ptr; memfd = memfd_create(\"whatever\", 0); ... /* Set the size of the underlying segment then map it locally. */ ret = ftruncate(memfd, 1024); ptr = mmap(NULL, 1024, PROT_READ|PROT_WRITE, MAP_SHARED, memfd, 0); ... /* Create a public proxy others can find in the /dev/evl/proxy hierarchy. */ proxyfd = evl_new_proxy(memfd, 0, \"/some-fancy-proxy\"); ... Any remote process peer could then do:\n#include \u003csys/types.h\u003e #include \u003csys/mman.h\u003e #include \u003cfcntl.h\u003e void *ptr; int fd; /* Open the proxy device relaying mapping requests to memfd(). */ fd = open(\"/dev/evl/proxy/some-fancy-proxy\", O_RDWR); ... /* Map the private RAM segment created by our peer. */ ptr = mmap(NULL, 1024, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0); ... The other way you could do this is by passing the memfd file descriptor via a socket-based control message, but this would require significantly more logic in both peers than using a proxy.\nProxy services  int evl_create_proxy(int targetfd, size_t bufsz, size_t granularity, int flags, const char *fmt, ...)  This call creates a new proxy to the target file referred to by targetfd, then returns a file descriptor for accessing the proxy upon success. oob_write() should be used to send data through the proxy to the target file. Conversely, oob_read() should be used to receive data through the proxy from the target file.\ntargetfdA file descriptor referring to the target file which should be associated with the proxy.\n\nbufszThe size in bytes of the I/O ring buffer where the relayed data is kept. bufsz must not exceed 2^30. A ring is allocated for each direction enabled in flags (see EVL_CLONE_OUTPUT, EVL_CLONE_INPUT). If granularity is non-zero, bufsz must be a multiple of this value. Out-of-band readers/writers may block on an out-of-band file operation if the buffer is either full on output or empty on input, unless the proxy descriptor is set to non-blocking mode (O_NONBLOCK). Zero is an acceptable value if you plan to use this proxy exclusively for exporting a shared memory mapping, in which case there is no point in reserving I/O ring buffers.\n\ngranularityIn some cases, the target file may have special semantics, which requires a fixed amount of data to be submitted at each read/write operation, like the eventfd(2) file which requires 64-bit words to be read or written from/to it at each transfer. When granularity is less than 2, the proxy is free to read or write as many bytes as possible from/to the target file at each transfer performed by the worker. Conversely, a granularity value greater than 1 is used as the exact count of bytes which may be read from or written to the target file by the in-band worker at each transfer. For instance, in the eventfd(2) use case, we would use sizeof(uint64_t). You may pass zero for a memory mapping proxy since no granularity is applicable in this case.\n\nflagsA set of creation flags for the new element, defining how it should be operated, along with its visibility:\n  EVL_CLONE_OUTPUT enables the output side of the proxy, so that writing to the proxy file descriptor pushes data to targetfd. If busfz is non-zero and flags does not mention EVL_CLONE_INPUT, EVL_CLONE_OUTPUT is assumed (default behavior).\n  EVL_CLONE_INPUT enables the input side of the proxy, so that reading from the proxy file descriptor pulls data from targetfd.\n  EVL_CLONE_PUBLIC denotes a public element which is represented by a device file in the /dev/evl file hierarchy, which makes it visible to other processes for sharing.\n  EVL_CLONE_PRIVATE denotes an element which is private to the calling process. No device file appears for it in the /dev/evl file hierarchy.\n  EVL_CLONE_NONBLOCK sets the file descriptor of the new proxy in non-blocking I/O mode (O_NONBLOCK). By default, O_NONBLOCK is cleared for the file descriptor.\n  \nfmtA printf-like format string to generate the proxy name. See this description of the [naming convention] (/ru/core/user-api/#element-naming-convention).\n\n...The optional variable argument list completing the format.\n\nevl_create_proxy() returns the file descriptor of the new proxy on success. If the call fails, a negated error code is returned instead:\n  -EEXIST\tThe generated name is conflicting with an existing proxy name.\n  -EINVAL\tEither flags is wrong, bufsz and/or granularity are wrong, or the generated name is badly formed.\n  -EINVAL\tbufsz is zero but flags mentions any of EVL_CLONE_INPUT or EVL_CLONE_OUTPUT.\n  -EBADF\ttargetfd is not a valid file descriptor.\n  -ENAMETOOLONG\tThe overall length of the device element’s file path including the generated name exceeds PATH_MAX.\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -ENOMEM\tNo memory available.\n  -ENXIO The EVL library is not initialized for the current process. Such initialization happens implicitly when evl_attach_self() is called by any thread of your process, or by explicitly calling evl_init(). You have to bootstrap the library services in a way or another before creating an EVL proxy.\n    int evl_new_proxy(int targetfd, size_t bufsz, const char *fmt, ...)  This call is a shorthand for creating a private proxy with no particular transfer granularity. It is identical to calling:\nevl_create_proxy(targetfd, bufsz, 0, EVL_CLONE_PRIVATE, fmt, ...); If bufsz is non-zero, the output side of the proxy is implicitly enabled. If busfz is zero, then a mapping-only proxy is created.\nИнформацияNote that if the [generated name] (/ru/core/user-api/#element-naming-convention) starts with a slash ('/') character, EVL_CLONE_PRIVATE would be automatically turned into EVL_CLONE_PUBLIC internally.\n   ssize_t evl_send_proxy(int proxyfd, const void *buf, size_t count)  This is a shorthand checking the current execution stage before sending the output through a proxy channel via the proper system call, i.e. write(2) if in-band, or oob_write() if out-of-band. You can use this routine to emit output from a portion of code which may be used from both stages.\nproxyfdA descriptor referring to the proxy which should handle the output data.\n\nbufA buffer containing the data to be written.\n\ncountThe number of bytes to write starting from buf. Zero is acceptable and leads to a null-effect.\n\nevl_send_proxy() returns the number of bytes sent through the proxy. A negated error code is returned on failure, which corresponds to the errno value received from either write(2) or oob_write() depending on the calling stage.\n  int evl_vprint_proxy(int proxyfd, const char *fmt, ...)  A routine which formats a printf(3)-like input string before sending the resulting output through a proxy channel.\nproxyfdA descriptor referring to the proxy which should handle the output data.\n\nfmtThe format string.\n\n...The optional variable argument list completing the format.\n\nevl_print_proxy() returns the number of bytes sent through the proxy. A negated error code is returned on failure, which may correspond to either a formatting error, or to a sending error in which case the error codes returned by evl_send_proxy() apply.\n  int evl_vprint_proxy(int proxyfd, const char *fmt, va_list ap)  This call is a variant of evl_print_proxy() which accepts format arguments specified as a pointer to a variadic parameter list.\nproxyfdA descriptor referring to the proxy which should handle the output data.\n\nfmtThe format string.\n\napA pointer to a variadic parameter list.\n\nevl_vprint_proxy() returns the number of bytes sent through the proxy. A negated error code is returned on failure, which may correspond to either a formatting error, or to a sending error in which case the error codes returned by evl_send_proxy() apply.\n  int evl_printf(const char *fmt, ...)  A shorthand which sends formatted output through an internal proxy targeting fileno(stdout). This particular proxy is built by the EVL library automatically when it initializes as a result of a direct or indirect call to evl_init().\n Events pollable from a proxy file descriptor The evl_poll() interface can monitor the following events occurring on a proxy file descriptor:\n  POLLOUT and POLLWRNORM are set whenever the output ring buffer some polled file descriptor refers to is empty AND allocated, which means that a proxy initialized with a zero-sized output buffer never raises these events. You would typically monitor the POLLOUT condition in order to wait for all of the buffered output to have been sent to the target file.\n  POLLIN and POLLRDNORM are set whenever some polled file descriptor refers to a proxy for which data is available on input.\n  POLLERR is returned whenever some polled file descriptor refers to a proxy which may not be either read or written (see EVL_CLONE_OUTPUT, EVL_CLONE_INPUT from (#evl_create_proxy)).\n   Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "File proxy",
    "uri": "/ru/core/user-api/proxy/"
  },
  {
    "content": "Requesting an out-of-band IRQ Dovetail introduces the new interrupt type flag IRQF_OOB, denoting an out-of-band handler to the generic interrupt API routines:\n setup_irq() for early registration of special interrupts request_irq() for device interrupts __request_percpu_irq() for per-CPU interrupts  An IRQ action handler bearing this flag will run from out-of-band context over the out-of-band stage, regardless of the current interrupt state of the in-band stage. If no out-of-band stage is present, the flag will be ignored, with the interrupt handler running on the in-band stage as usual.\nConversely, out-of-band handlers can be dismissed using the usual calls, such as:\n free_irq() for device interrupts free_percpu_irq() for per-CPU interrupts  Out-of-band IRQ handling has the following constraints:\n If the IRQ is shared, with multiple action handlers registered for the same event, all other handlers on the same interrupt channel must bear the IRQF_OOB flag too, or the request will fail.  ВниманиеIf meeting real-time requirements is your goal, sharing an IRQ line among multiple devices operating from different execution stages (in-band vs out-of-band) can only be a bad idea design-wise. You should resort to this in desparate hardware situations only.\n  Obviously, out-of-band handlers cannot be threaded (IRQF_NO_THREAD is implicit, IRQF_ONESHOT is ignored).   Installing an out-of-band handler for a device interrupt\n #include \u003clinux/interrupt.h\u003e static irqreturn_t oob_interrupt_handler(int irq, void *dev_id) { ... return IRQ_HANDLED; } init __init driver_init_routine(void) { int ret; ... ret = request_irq(DEVICE_IRQ, oob_interrupt_handler, IRQF_OOB, \"Out-of-band device IRQ\", device_data); if (ret) goto fail; return 0; fail: /* Unwind upon error. */ ... } Telling the companion kernel about entering, leaving the IRQ context Your companion core will most likely want to be notified each time a new interrupt context is entered, typically in order to block any further task rescheduling on its end. Conversely, this core will also want to be notified when such context is exited, so that it can start its rescheduling procedure, applying any change to the scheduler state which occurred during the execution of the interrupt handler(s), such as waking up a thread which was waiting for the incoming event.\nTo provide such support, Dovetail calls irq_enter_pipeline() on entry to the pipeline when it receives an IRQ from the hardware, then irq_exit_pipeline() right before it leaves the interrupt frame. It defines empty placeholders for these hooks as follows, which are picked in absence of a companion core in the kernel tree:\n linux/include/dovetail/irq.h\n /* SPDX-License-Identifier: GPL-2.0 */ #ifndef _DOVETAIL_IRQ_H #define _DOVETAIL_IRQ_H /* Placeholders for pre- and post-IRQ handling. */ static inline void irq_enter_pipeline(void) { } static inline void irq_exit_pipeline(void) { } #endif /* !_DOVETAIL_IRQ_H */ As an illustration, the EVL core overrides the placeholders by interposing the following file which comes earlier in the inclusion order of C headers, providing its own set of hooks as follows:\n linux-evl/include/asm-generic/evl/irq.h\n /* SPDX-License-Identifier: GPL-2.0 */ #ifndef _ASM_GENERIC_EVL_IRQ_H #define _ASM_GENERIC_EVL_IRQ_H #include \u003cevl/irq.h\u003e static inline void irq_enter_pipeline(void) { #ifdef CONFIG_EVL evl_enter_irq(); #endif } static inline void irq_exit_pipeline(void) { #ifdef CONFIG_EVL evl_exit_irq(); #endif } #endif /* !_ASM_GENERIC_EVL_IRQ_H */ Eventually, the EVL core implements the evl_enter_irq() and evl_exit_irq() routines in a final support header like this:\n linux-evl/include/evl/irq.h\n /* * SPDX-License-Identifier: GPL-2.0 * * Copyright (C) 2017 Philippe Gerum \u003crpm@xenomai.org\u003e */ #ifndef _EVL_IRQ_H #define _EVL_IRQ_H #include \u003cevl/sched.h\u003e /* hard irqs off. */ static inline void evl_enter_irq(void) { struct evl_rq *rq = this_evl_rq(); rq-\u003elocal_flags |= RQ_IRQ; } /* hard irqs off. */ static inline void evl_exit_irq(void) { struct evl_rq *this_rq = this_evl_rq(); this_rq-\u003elocal_flags \u0026= ~RQ_IRQ; /* * CAUTION: Switching stages as a result of rescheduling may * re-enable irqs, shut them off before returning if so. */ if ((this_rq-\u003eflags|this_rq-\u003elocal_flags) \u0026 RQ_SCHED) { evl_schedule(); if (!hard_irqs_disabled()) hard_local_irq_disable(); } } #endif /* !_EVL_IRQ_H */  Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "IRQ handling",
    "uri": "/ru/dovetail/pipeline/irq_handling/"
  },
  {
    "content": "Waiting for events on file descriptors Every EVL element is represented by a device file accessible in the /dev/evl file hierarchy, and requests can be sent to any of those elements by writing to common file descriptors obtained on the corresponding files. Conversely, information is returned to the caller by reading from those file descriptors. EVL applications can poll for out-of-band events occurring on a set of file descriptors opened on elements, just like in-band applications can use poll(2) with other files.\nTo access the out-of-band polling services, an application first needs to obtain a file descriptor on a new polling set by a call to evl_poll(). Such descriptor is an access point for registering other file descriptors to monitor, then sensing events occurring on them. evl_poll() or evl_timedpoll() fill an array of structures indicating which events have been received on each notified file descriptor. The type of this structure is as follows:\nstruct evl_poll_event { int32_t fd; int32_t events; }; Applications can wait for any event which poll(2) can monitor such as POLLIN, POLLOUT, POLLRDNORM, POLLWRNORM and so on. Which event can occur on a given file descriptor is defined by the out-of-band driver managing the file it refers to.\n Pollable elements   Semaphore   Event flag group   Timer   Proxy   Cross-buffer   Polling services  int evl_new_poll(void)  This call creates a polling set, returning a file descriptor representing the new object upon success. There is no arbitrary limit on the number of polling sets an application can create, which is only limited to the available system resources.\nevl_new_poll() returns the file descriptor of the newly created polling set on success. Otherwise, a negated error code is returned:\n  -EMFILE\tThe per-process limit on the number of open file descriptors has been reached.\n  -ENFILE\tThe system-wide limit on the total number of open files has been reached.\n  -ENOMEM\tNo memory available.\n  -ENXIO\tThe EVL library is not initialized for the current process. Such initialization happens implicitly when evl_attach_self() is called by any thread of your process, or by explicitly calling evl_init(). You have to bootstrap the library services in a way or another before creating an EVL polling set.\n    int evl_add_pollfd(int efd, int newfd, unsigned int events)  Adds a file descriptor to a polling set. This service registers the newfd file descriptor for being polled for events by the efd polling set.\nefdThe file descriptor of a polling set obtained by a previous call to evl_new_poll().\n\nnewfdA file descriptor to monitor for events. Once newfd is added to the polling set, it becomes a source of events which is monitored when reading from efd. You can add as many file descriptor as you need to a polling set in order to perform synchronous I/O multiplexing over multiple data sources or sinks.\n\neventsA bitmask representing the set of events to monitor for newfd. These events are common to the in-band poll(2) interface. POLLERR and POLLHUP are always implicitly polled, even if they are not mentioned in events. POLLNVAL cannot be awaited for. Which event is meaningful in the context of the call depends on the driver managing the file, which defines the logic for raising them. For instance, if newfd refers to an EVL semaphore, you could monitor POLLIN and POLLRDNORM, no other event would occur for this type of file.\n\nevl_add_pollfd() returns zero on success. Otherwise, a negated error code is returned:\n  -EBADF\tefd is not a valid file descriptor referring to an EVL polling set.\n  -EBADF\tnewfd is not a valid file descriptor to an EVL file. Note that you cannot register a file descriptor referring to a common file (i.e. managed by an in-band driver) into an EVL polling set. An EVL file is created by [out-of-band drivers] (/ru/core/user-api/io/) for referring to resources which can be accessed from the [out-of-band execution] (/ru/dovetail/altsched/) stage.\n  -ELOOP\tAdding newfd to the polling set would lead to a cyclic dependency (such as newfd referring to the polling set).\n  -ENOMEM\tNo memory available.\n    int evl_del_pollfd(int efd, int delfd)  Removes a file descriptor from a polling set. As a result, reading this polling set does not monitor delfd anymore. This change applies to subsequent calls to evl_poll() or evl_timedpoll() for such set; ongoing waits are not affected.\nefdThe file descriptor of a polling set obtained by a previous call to evl_new_poll().\n\ndelfdThe file descriptor to stop monitoring for events. This descriptor should have been registered by an earlier call to evl_add_pollfd().\n\nevl_del_pollfd() returns zero on success. Otherwise, a negated error code is returned:\n  -EBADF\tefd is not a valid file descriptor referring to an EVL polling set.\n  -ENOENT\tdelfd is not a file descriptor present into the polling set referred to by efd.\n    int evl_mod_pollfd(int efd, int modfd, unsigned int events)  Modifies the events being monitored for a file descriptor already registered in a polling set. This change applies to subsequent calls to evl_poll() or evl_timedpoll() for such set; ongoing waits are not affected.\nefdThe file descriptor of a polling set obtained by a previous call to evl_new_poll().\n\nmodfdThe file descriptor to update the event mask for. This descriptor should have been registered by an earlier call to evl_add_pollfd().\n\nevl_mod_pollfd() returns zero on success. Otherwise, a negated error code is returned:\n  -EBADF\tefd is not a valid file descriptor referring to an EVL polling set.\n  -ENOENT\tpollfd is not a file descriptor present into the polling set referred to by efd.\n    int evl_poll(int efd, struct evl_poll_event *pollset, int nrset)  Waits for events on the file descriptors present in the polling set referred to by efd.\nefdThe file descriptor of a polling set obtained by a previous call to evl_new_poll().\n\npollsetThe start of an array of structures of type evl_poll_event containing the set of file descriptors which have received at least one event when the call returns.\n\nnrsetThe number of structures in the pollset array. If zero, evl_poll() returns immediately with a zero count.\n\nOn success, evl_poll() returns the count of file descriptors from the polling set for which an event is pending, each of them will appear in a evl_poll_event entry in pollset. Otherwise, a negated error code is returned:\n  -EBADF\tefd is not a valid file descriptor referring to an EVL polling set.\n  -EINVAL\tnrset is negative.\n  -EAGAIN\tfd is marked as [non-blocking (O_NONBLOCK)] (http://man7.org/linux/man-pages/man2/fcntl.2.html), and no file descriptor from the polling set has any event pending.\n  -EFAULT\tpollset points to invalid memory.\n    int evl_timedpoll(int efd, struct evl_poll_event *pollset, int nrset, struct timespec *timeout)  This call is a variant of evl_poll() which allows specifying a timeout on the polling operation, so that the caller is unblocked after a specified delay sleeping without receiving any event.\nefdThe file descriptor of a polling set obtained by a previous call to evl_new_poll().\n\npollsetThe start of an array of structures of type evl_poll_event containing the set of file descriptors which have received at least one event when the call returns.\n\nnrsetThe number of structures in the pollset array. If zero, evl_poll() returns immediately with a zero count.\n\ntimeoutA time limit to wait for any event to be notified before the call returns on error. Timeouts are always based on the built-in EVL_CLOCK_MONOTONIC clock. If both the tv_sec and tv_nsec members of timeout are set to zero, evl_timedpoll() behaves identically to evl_poll().\n\nOn success, evl_timedpoll() returns the count of file descriptors from the polling set for which an event is pending, each of them will appear in a evl_poll_event entry in pollset. Otherwise, a negated error code is returned:\n  -EBADF\tefd is not a valid file descriptor referring to an EVL polling set.\n  -EINVAL\tnrset is negative.\n  -EFAULT\tpollset or timeout point to invalid memory.\n  -EINVAL\tany of tv_sec or tv_nsec in timeout is invalid.\n  -ETIMEDOUT\tThe timeout fired, after the amount of time specified by timeout.\n   Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Polling file descriptors",
    "uri": "/ru/core/user-api/poll/"
  },
  {
    "content": "The pipeline introduces an additional type of interrupts, which are purely software-originated, with no hardware involvement. These IRQs can be triggered by any kernel code. Synthetic IRQs are inherently per-CPU events. Because the common pipeline flow applies to synthetic interrupts, it is possible to attach them to out-of-band and/or in-band handlers, just like device interrupts.\nSynthetic interrupts abide by the normal rules with respect to interrupt masking: such IRQs may be deferred until the stage they should be handled from is unstalled.\nЗаметкаSynthetic interrupts and softirqs differ in essence: the latter only exist in the in-band context, and therefore cannot trigger out-of-band activities. Synthetic interrupts used to be called virtual IRQs (or virq for short) by the legacy I-pipe implementation, Dovetail’s ancestor; such rename clears the confusion with the way abstract interrupt numbers defined within interrupt domains may be called elsewhere in the kernel code base (i.e. virtual interrupts too).\n Allocating synthetic interrupts Synthetic interrupt vectors are allocated from the synthetic_irq_domain, using the irq_create_direct_mapping() routine.\nA synthetic interrupt handler can be installed for running on the in-band stage upon a scheduling request (i.e. being posted) from an out-of-band context as follows:\n#include \u003clinux/irq_pipeline.h\u003e  static irqreturn_t sirq_handler(int sirq, void *dev_id) { do_in_band_work(); return IRQ_HANDLED; } static struct irqaction sirq_action = { .handler = sirq_handler, .name = \"In-band synthetic interrupt\", .flags = IRQF_NO_THREAD, }; unsigned int alloc_sirq(void) { unsigned int sirq; sirq = irq_create_direct_mapping(synthetic_irq_domain); if (!sirq) return 0; setup_percpu_irq(sirq, \u0026sirq_action); return sirq; } A synthetic interrupt handler can be installed for running from the oob stage upon a trigger from an in-band context as follows:\nstatic irqreturn_t sirq_oob_handler(int sirq, void *dev_id) { do_out_of_band_work(); return IRQ_HANDLED; } unsigned int alloc_sirq(void) { unsigned int sirq; sirq = irq_create_direct_mapping(synthetic_irq_domain); if (!sirq) return 0; ret = __request_percpu_irq(sirq, sirq_oob_handler, IRQF_OOB, \"Out-of-band synthetic interrupt\", dev_id); if (ret) { irq_dispose_mapping(sirq); return 0; } return sirq; } Scheduling in-band execution of a synthetic interrupt handler The execution of sirq_handler() in the in-band context can be scheduled (or posted) from the out-of-band context in two different ways:\nUsing the common injection service irq_pipeline_inject(sirq); Using the lightweight injection method (requires interrupts to be disabled in the CPU) unsigned long flags = hard_local_irqsave(); irq_post_inband(sirq); hard_local_irqrestore(flags);  СоветAssuming that no interrupt may be pending in the event log for the oob stage at the time this code runs, the second method relies on the invariant that in a pipeline interrupt model, IRQs pending for the in-band stage will have to wait for the oob stage to quiesce before they can be handled. Therefore, it is pointless to check for synchronizing the interrupts pending for the in-band stage from the oob stage, which the irq_pipeline_inject() service would do systematically. irq_post_inband() simply marks the event as pending in the event log of the in-band stage for the current CPU, then returns. This event would be played as a result of synchronizing the log automatically when the current CPU switches back to the in-band stage.\n It is also valid to post a synthetic interrupt to be handled on the in-band stage from an in-band context, using irq_pipeline_inject(). In such a case, the normal rules of interrupt delivery apply, depending on the state of the virtual interrupt disable flag for the in-band stage: the IRQ is immediately delivered, with the call to irq_pipeline_inject() returning only after the handler has run.\nTriggering out-of-band execution of a synthetic interrupt handler Conversely, the execution of sirq_handler() on the oob stage can be triggered from the in-band context as follows:\nirq_pipeline_inject(sirq); Since the oob stage has precedence over the in-band stage for execution of any pending event, this IRQ is immediately delivered, with the call to irq_pipeline_inject() returning only after the handler has run.\nIt is also valid to post a synthetic interrupt to be handled on the oob stage from an out-of-band context, using irq_pipeline_inject(). In such a case, the normal rules of interrupt delivery apply, depending on the state of the virtual interrupt disable flag for the oob stage.\nЗаметкаCalling irq_post_oob(sirq) from the in-band stage to trigger an out-of-band event is most often not the right way to do this, because this service would not synchronize the interrupt log before returning. In other words, the sirq event would still be pending for the oob stage despite the fact that it should have preempted the in-band stage before returning to the caller.\n  Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Synthetic IRQs",
    "uri": "/ru/dovetail/pipeline/synthetic/"
  },
  {
    "content": "Sending out-of-band IPIs to remote CPUs Although the pipeline does not directly use IPIs internally, it exposes two generic IPI vectors which autonomous cores may use in SMP configuration for signaling the following events across CPUs:\n  RESCHEDULE_OOB_IPI, the cross-CPU task reschedule request. This is available to the core’s scheduler for kicking the task rescheduling procedure on remote CPUs, when the state of their respective runqueue has changed. For instance, a task sleeping on CPU #1 may be unblocked by a system call issued from CPU #0: in this case, the scheduler code running on CPU #0 is supposed to tell CPU #1 that it should reschedule. Typically, the EVL core does so from its test_resched() routine.\n  TIMER_OOB_IPI, the cross-CPU timer reschedule request. Because software timers are in essence per-CPU beasts, this IPI is available to the core’s timer management code for kicking the hardware timer programming procedure on remote CPUs, when the state of some software timer has changed. Typically, stopping a timer from a remote CPU, or migrating a timer from a CPU to another should trigger such signal. The EVL core does so from its evl_program_remote_tick() routine, which is called whenever the timer with the earliest timeout date enqueued on a remote CPU, may have changed.\n  As their respective name suggests, those two IPIs can be sent from out-of-band context (as well as in-band), by calling the irq_send_oob_ipi() service.\n  void irq_send_oob_ipi(unsigned int ipi, const struct cpumask *cpumask)  ipiThe IPI number to send. There are only two legit values for this argument: either RESCHEDULE_OOB_IPI, or TIMER_OOB_IPI. This is a low-level service with not much parameter checking, so any other value is likely to cause havoc.\n\ncpumaskA CPU bitmask defining the target CPUs for the signal. The current CPU is allowed to send a signal to itself, although this may not be the faster path to running a local handler.\n\nIn order to receive these IPIs, an out-of-band handler must have been set for them, mentioning the [IRQF_OOB flag]({{ \u003c relref “dovetail/pipeline/irq_handling.md” \u003e}}).\nirq_send_oob_ipi() serializes callers internally so that it may be used from either stages: in-band or out-of-band.\n Injecting an IRQ event for the current CPU In some very specific cases, we may need to inject an IRQ into the pipeline by software as if such hardware event had happened on the current CPU. irq_inject_pipeline() does exactly this.\n  int irq_inject_pipeline(unsigned int irq)  irqThe IRQ number to inject. A valid interrupt descriptor must exist for this interrupt.\n\nirq_inject_pipeline() fully emulates the receipt of a hardware event, which means that the common interrupt pipelining logic applies to the new event:\n  first, any out-of-band handler is considered for delivery,\n  then such event may be passed down the pipeline to the common in-band handler(s) in absence of out-of-band handler(s).\n  The pipeline priority rules apply accordingly:\n  if the caller is in-band, and an out-of-band handler is registered for the IRQ event, and the out-of-band stage is unstalled, the execution stage is immediately switched to out-of-band for running the later, then restored to in-band before irq_inject_pipeline() returns.\n  if the caller is out-of-band and there is no out-of-band handler, the IRQ event is deferred until the in-band stage resumes execution on the current CPU, at which point it is delivered to any in-band handler(s).\n  in any case, should the current stage receive the IRQ event, the virtual interrupt state of that stage is always considered before deciding whether this event should be delivered immediately to its handler by irq_inject_pipeline() (unstalled case), or deferred until the stage is unstalled (stalled case).\n  This call returns zero on successful injection, or -EINVAL if the IRQ has no valid descriptor.\nЗаметкаIf you look for a way to schedule the execution of a routine in the in-band interrupt context from the out-of-band stage, you may want to consider the extended irq_work API which provides a high level interface to this feature.\n  Direct logging of an IRQ event Sometimes, running the full interrupt delivery logic irq_inject_pipeline() implements for feeding an interrupt into the pipeline may be overkill when we may make assumptions about the current execution context, and which stage should handle the event. The following fast helpers can be used instead in this case:\n  void irq_post_inband(unsigned int irq)  irqThe IRQ number to inject into the in-band stage. A valid interrupt descriptor must exist for this interrupt.\n\nThis routine may be used to mark an interrupt as pending directly into the current CPU’s log for the in-band stage. This is useful in either of these cases:\n  you know that the out-of-band stage is current, therefore this event has to be deferred until the in-band stage resumes on the current CPU later on. This means that you can simply post it to the in-band stage directly.\n  you know that the in-band stage is current but stalled, therefore this event can’t be immediately delivered, so marking it as pending into the in-band stage is enough.\n  Interrupts must be hard disabled in the CPU before calling this routine.\n  void irq_post_oob(unsigned int irq)  irqThe IRQ number to inject into the out-of-band stage. A valid interrupt descriptor must exist for this interrupt.\n\nThis routine may be used to mark an interrupt as pending directly into the current CPU’s log for the out-of-band stage. This is useful in only one situation: you know that the out-of-band stage is current but stalled, therefore this event can’t be immediately delivered, so marking it as pending into the out-of-band stage is enough.\nInterrupts must be hard disabled in the CPU before calling this routine. If the out-of-band stage is stalled as expected on entry to this helper, then interrupts must be hard disabled in the CPU as well anyway.\n Extended IRQ work API Due to the NMI-like nature of interrupts running out-of-band code from the standpoint of the main kernel, such code might preempt in-band activities in the middle of a critical section. For this reason, it would be unsafe to call any in-band routine from an out-of-band context.\nHowever, we may schedule execution of in-band work handlers from out-of-band code, using the regular irq_work_queue() service which has been extended by the IRQ pipeline core. Such work request from the out-of-band stage is scheduled for running on the in-band stage on the issuing CPU as soon as the out-of-band activity quiesces on this processor. As its name implies, the work handler runs in (in-band) interrupt context.\nЗаметкаThe interrupt pipeline forces the use of a synthetic IRQ as a notification signal for the IRQ work machinery, instead of a hardware-specific interrupt vector. This special IRQ is labeled in-band work when reported by /proc/interrupts. irq_work_queue() may invoke the work handler immediately only if called from the in-band stage with hard irqs on. In all other cases, the handler execution is deferred until the in-band log is synchronized.\n  Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "IRQ injection",
    "uri": "/ru/dovetail/pipeline/pipeline_inject/"
  },
  {
    "content": "Disabling interrupts in the CPU The local_irq_save() and local_irq_disable() helpers are no more disabling interrupts in the CPU when interrupt pipelining is enabled, but only disable interrupt events virtually for the in-band stage.\nA set of helpers is provided for manipulating the interrupt disable flag in the CPU instead. When CONFIG_IRQ_PIPELINE is disabled, this set maps 1:1 over the regular local_irq_*() API.\n   Original/Virtual Non-virtualized call     local_save_flags(flags) flags = hard_local_save_flags()   local_irq_disable() hard_local_irq_disable()   local_irq_enable() hard_local_irq_enable()   local_irq_save(flags) flags = hard_local_irq_save()   local_irq_restore(flags) hard_local_irq_restore(flags)   irqs_disabled() hard_irqs_disabled()   irqs_disabled_flags(flags) hard_irqs_disabled_flags(flags)    Stalling the out-of-band stage Just like the in-band stage is affected by the state of the virtual interrupt disable flag, the interrupt state of the oob stage is controlled by a dedicated stall bit flag in the oob stage’s status. In combination with the interrupt disable bit in the CPU, this software bit controls interrupt delivery to the oob stage.\nWhen this stall bit is set, interrupts which might be pending in the oob stage’s event log of the current CPU are not played. Conversely, the out-of-band handlers attached to pending IRQs are fired when the stall bit is clear. The following table represents the equivalent calls affecting the stall bit for each stage:\n   In-band stage operation OOB stage operation     local_save_flags(flags) -none-   local_irq_disable() oob_irq_disable()   local_irq_enable() oob_irq_enable()   local_irq_save(flags) flags = oob_irq_save()   local_irq_restore(flags) oob_irq_restore(flags)   irqs_disabled() oob_irqs_disabled()   irqs_disabled_flags(flags) -none-     Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Interrupt protection",
    "uri": "/ru/dovetail/pipeline/interrupt_protection/"
  },
  {
    "content": "Talking to real-time capable device drivers Using the EVL kernel API, you can extend an existing driver for supporting out-of-band I/O operations, or even write one from scratch. Both character-based I/O and socket protocol drivers are supported.\nOn the user side, application can exchange data with, send requests to these real-time capable drivers from the out-of-band execution stage with the a couple of additional services libevl provides.\nYou may notice that several POSIX file I/O services such as open(2), socket(2), close(2), fcntl(2), mmap(2) and so on have no out-of-band counterpart in the following list. The reason is that we don’t need them: opening, closing or mapping a file are inherently non-deterministic operations, which may block for an unspecified amount of time for a number of reasons, depending on the underlying file and current runtime conditions. Besides, those are hardly useful in a time-critical loop.\nHowever, issuing data transfers and control requests to the driver is definitely something we may want to happen within a bounded time, hence directly from the out-of-band execution stage.\nСоветSince the EVL core exports every public element as a character device which can be accessed from /dev/evl, libevl can interface with elements from other processes through the out-of-band I/O requests documented here, which are sent to the corresponding devices.\n Opening an out-of-band capable I/O channel Since the EVL core does not redefine the open(2) and socket(2) calls, there has to be a way to tell the kernel code managing the device and/or protocol that we want to enable out-of-band operations.\nIn most cases, we don’t have to do so though, because the purpose of the corresponding device driver is all about providing out-of-band services, so enabling them for any connecting file is implicit. For instance, most of the drivers accessed through the /dev/evl file hierarchy turn on out-of-band services automatically.\nHowever, some drivers might distinguish between out-of-band capable files and others, providing a different set of services. Typically, a regular in-band driver which is extended in order to handle out-of-band requests too should be told when to do so for any given file.\nTo meet this requirement, Dovetail introduces the additional open flag O_OOB, which can be passed to open(2) ORed into the flags argument. Similarly, it defines the SOCK_OOB flag which can be passed to socket(2) ORed into the type argument for the same purpose. If the receiving driver implements opt-in out-of-band services, passing this flag when opening a file/socket should enable them.\nВниманиеNot all devices drivers may support out-of-band operations (the overwhelming majority does not). Whether passing either O_OOB or SOCK_OOB to them when opening a file/socket would cause an error, or the flag would just be ignored depends on the driver code.\n Out-of-band I/O services  ssize_t oob_read(int efd, void *buf, size_t count)  This is the strict equivalent to the standard read(2) system call, for sending the request from the out-of-band stage to an EVL driver. In other words, oob_read() attempts to read up to count bytes from file descriptor fd into the buffer starting at buf, from the out-of-band execution stage.\nThe caller must be an EVL thread, which may be switched automatically by the EVL core to the out-of-band execution stage as a result of this call.\nefdA file descriptor obtained by opening a real-time capable driver which we want to read from.\n\nbufA buffer to receive the data.\n\ncountThe number of bytes to read at most, which should fit into buf.\n\noob_read() returns the actual number of bytes read, copied to buf on success. Otherwise, -1 is returned, and errno is set to the error code:\nEBADF\tif fd does not refer to a real-time capable driver, or fd was not opened for reading.\nEINVAL if fd does not support the .oob_read operation.\nEFAULT\tif buf points to invalid memory.\nEAGAIN\tfd is marked as [non-blocking (O_NONBLOCK)] (http://man7.org/linux/man-pages/man2/fcntl.2.html), and the read operation would block.\nOther driver-specific error codes may be returned, such as:\nENOBUFS fd is a cross-buffer file descriptor, and there is no ring buffer space associated with the outbound traffic (i.e. o_bufsz parameter was zero when creating the cross-buffer).\nEINVAL fd is a cross-buffer file descriptor, and count is greater than the size of the ring buffer associated with the traffic direction. (i.e. either the i_bufsz or o_bufsz parameter given when creating the cross-buffer).\nENXIO\tfd is a proxy file descriptor which is not available for input. See EVL_CLONE_INPUT.\n  ssize_t oob_write(int efd, const void *buf, size_t count)  This is the strict equivalent to the standard write(2) system call, for sending the request from the out-of-band stage to an EVL driver. In other words, oob_write() attempts to write up to count bytes to file descriptor fd from the buffer starting at buf, from the out-of-band execution stage.\nThe caller must be an EVL thread, which may be switched automatically by the EVL core to the out-of-band execution stage as a result of this call.\nefdA file descriptor obtained by opening a real-time capable driver which we want to read from.\n\nbufA buffer containing the data to be written.\n\ncountThe number of bytes to write starting from buf.\n\noob_write() returns the actual number of bytes written from buf on success. Otherwise, -1 is returned, and errno is set to the error code:\nEBADF\tif fd does not refer to a real-time capable driver, or fd was not opened for writing.\nEINVAL if fd does not support the .oob_write operation.\nEFAULT\tif buf points to invalid memory.\nEAGAIN\tfd is marked as [non-blocking (O_NONBLOCK)] (http://man7.org/linux/man-pages/man2/fcntl.2.html), and the write operation would block.\nOther driver-specific error codes may be returned, such as:\nEFBIG\tfd is a proxy file descriptor, and count is larger than the size of the output buffer as specified in the call to [evl_create_proxy()] (/ru/core/user-api/proxy/#evl_create_proxy).\nEINVAL\tfd is a proxy file descriptor, and count is not a multiple of the output granularity as specified in the call to [evl_create_proxy()] (/ru/core/user-api/proxy/#evl_create_proxy).\nENOBUFS fd is a cross-buffer file descriptor, and there is no ring buffer space associated with the inbound traffic (i.e. i_bufsz parameter was zero when creating the cross-buffer).\nEINVAL fd is a cross-buffer file descriptor, and count is greater than the size of the ring buffer associated with the traffic direction. (i.e. either the i_bufsz or o_bufsz parameter given when creating the cross-buffer).\nENXIO\tfd is a proxy file descriptor which is not available for output. See EVL_CLONE_OUTPUT.\n  int oob_ioctl(int efd, unsigned long request, ...)  This is the strict equivalent to the standard ioctl(2) system call, for sending the I/O control request from the out-of-band stage to an EVL driver. In other words, oob_ioctl() issues request to file descriptor fd from the out-of-band execution stage.\nThe caller must be an EVL thread, which may be switched automatically by the EVL core to the out-of-band execution stage as a result of this call.\nefdA file descriptor obtained by opening a real-time capable driver which we want to send a request to.\n\nrequestThe I/O control request code.\n\n...An optional variable argument list which applies to request.\n\noob_ioctl() returns zero on success. Otherwise, -1 is returned, and errno is set to the error code:\nEBADF\tif fd does not refer to a real-time capable driver.\nENOTTY if fd does not support the .oob_ioctl operation, or the driver does not implement request.\nEFAULT\tif buf points to invalid memory.\nEAGAIN fd is marked as non-blocking (O_NONBLOCK), and the control request would block.\nOther driver-specific error codes may be returned.\n  ssize_t oob_recvmsg(int s, struct oob_msghdr *msghdr, const struct timespec *timeout, int flags)  This is an equivalent to the standard recvmsg(2) system call, for sending the request from the out-of-band stage to an EVL driver with a socket-based interface. In other words, oob_recvmsg() is used to receive messages from an out-of-band capable EVL socket from the out-of-band execution stage.\nThe caller must be an EVL thread, which may be switched automatically by the EVL core to the out-of-band execution stage as a result of this call.\nsA socket descriptor obtained from a regular socket(2) call, with the SOCK_OOB flag set in the type argument, denoting that out-of-band services are enabled for the socket.\n\nmsghdrA pointer to a structure containing the multiple arguments to this call, which is described below.\n\ntimeoutA time limit to wait for a message before the call returns on error. The built-in clock EVL_CLOCK_MONOTONIC is used for tracking the elapsed time. If NULL is passed, the call is allowed to wait indefinitely for a message.\n\nflagsA set of flags further qualifying the operation. Only the following flags should be recognized for out-of-band requests:\n  MSG_DONTWAIT causes the call to fail with the error EAGAIN if no message is immediately available at the time of the call. MSG_DONTWAIT is implied if O_NONBLOCK was set for the socket descriptor via the fcntl(2) F_SETFL operation.\n  MSG_PEEK causes the receive operation to return data from the beginning of the receive queue without removing that data from the queue. Thus, a subsequent receive call will return the same data.\n  \noob_recvmsg() returns the actual number of bytes received on success. Otherwise, -1 is returned, and errno is set to the error code:\nEBADF if s does not refer to a valid socket opened with the SOCK_OOB type flag set, or s was not opened for reading.\nEINVAL if s does not support the .oob_ioctl operation.\nEFAULT\tif msghdr, or any buffer it refers to indirectly points to invalid memory.\nEAGAIN\ts is marked as non-blocking (O_NONBLOCK), or MSG_DONTWAIT is set in flags, and the receive operation would block.\nETIMEDOUT the timeout fired before the operation could complete successfully.\n  ssize_t oob_sendmsg(int s, const struct oob_msghdr *msghdr, const struct timespec *timeout, int flags)  This call is equivalent to the standard sendmsg(2) system call, for sending the request from the out-of-band stage to an EVL driver with a socket-based interface. In other words, oob_sendmsg() is used to send messages to an out-of-band capable EVL socket from the out-of-band execution stage.\nThe caller must be an EVL thread, which may be switched automatically by the EVL core to the out-of-band execution stage as a result of this call.\nsA socket descriptor obtained from a regular socket(2) call, with the SOCK_OOB flag set in the type argument, denoting that out-of-band services are enabled for the socket.\n\nmsghdrA pointer to a structure containing the multiple arguments to this call, which is described below.\n\ntimeoutA time limit to wait for an internal buffer to be available for sending the message before the call returns on error. The built-in clock EVL_CLOCK_MONOTONIC is used for tracking the elapsed time. If NULL is passed, the call is allowed to wait indefinitely for a buffer.\n\nflagsA set of flags further qualifying the operation. Only the following flag should be recognized for out-of-band requests:\n MSG_DONTWAIT causes the call to fail with the error EAGAIN if no buffer is immediately available at the time of the call for sending the message. MSG_DONTWAIT is implied if O_NONBLOCK was set for the socket descriptor via the fcntl(2) F_SETFL operation.  \noob_sendmsg() returns the actual number of bytes sent on success. Otherwise, -1 is returned, and errno is set to the error code:\nEBADF\tif s does not refer to a valid socket opened with the SOCK_OOB type flag set, or s was not opened for writing.\nEINVAL if s does not support the .oob_ioctl operation.\nEFAULT\tif msghdr, or any buffer it refers to indirectly points to invalid memory.\nEAGAIN\ts is marked as non-blocking (O_NONBLOCK), or MSG_DONTWAIT is set in flags, and the send operation would block.\nETIMEDOUT the timeout fired before the operation could complete successfully.\n–\nThe out-of-band message header The structure oob_msghdr which is passed to the oob_recvmsg() and oob_sendmsg() calls is defined as follows:\n struct oob_msghdr { void *msg_name; /* Optional address */ socklen_t msg_namelen; /* Size of address */ struct iovec *msg_iov; /* Scatter/gather array */ size_t msg_iovlen; /* # elements in msg_iov */ void *msg_control; /* Ancillary data, see below */ size_t msg_controllen; /* Ancillary data buffer len */ int msg_flags; /* Flags on received message */ struct timespec msg_time; /* Optional time, see below */ }; struct iovec { /* Scatter/gather array items */ void *iov_base; /* Starting address */ size_t iov_len; /* Number of bytes to transfer */ };  The msg_name field points to a caller-allocated buffer that is used to return the source address if the socket is unconnected. The caller should set msg_namelen to the size of this buffer before this call. On success, oob_recvmsg() updates msg_namelen to contain the length of the returned address. If the application does not need to know the source address, msg_name can be specified as NULL.\nThe fields msg_iov and msg_iovlen describe scatter-gather locations pointing at the message data being sent or received, as discussed in readv(2).\nThe field msg_control points to a buffer for other protocol control-related messages or miscellaneous ancillary data. When either oob_recvmsg() or oob_sendmsg() is called, msg_controllen should contain the length of the available buffer in msg_control. On success, oob_recvmsg() updates msg_controllen to contain the actual length of the control message sequence returned by the call.\nThe msg_flags field is only set on return of oob_recvmsg(). It can contain any of the flags which may be returned by recvmsg(2).\nmsg_time may be used to send or receive timestamping information to/from the protocol driver implementing out-of-band operations.\nВниманиеProtocol drivers should no attach any meaning to MSG_OOB when operating in out-of-band mode, so that no additional confusion arises with the common usage of this flag with recvmsg(2).\n  Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Out-of-band I/O services",
    "uri": "/ru/core/user-api/io/"
  },
  {
    "content": "Before you can direct the incoming interrupt flow to out-of-band handlers, you need to install the out-of-band interrupt stage. Conversely, you need to remove the out-of-band stage from the interrupt pipeline when you are done with receiving out-of-band events.\n  int enable_oob_stage(const char *name)  nameA symbolic name describing the high priority interrupt stage which is being installed. This information is merely used in kernel messages, so it should be short but descriptive enough. For instance, the EVL core installs the “EVL” stage.\n\nThis call enables the out-of-band stage context in the interrupt pipeline, which in turn allows an autonomous core to install out-of-band handlers for interrupts. It returns zero on success, or a negated error code if something went wrong:\n-EBUSY\tThe out-of-band stage is already enabled.\n  void disable_oob_stage(void)  This call disables the out-of-band stage context in the interrupt pipeline. From that point, the interrupt flow is exclusively directed to the in-band stage.\nВниманиеThis call does not perform any serialization with ongoing interrupt handling on remote CPUs whatsoever. The autonomous core must synchronize with remote CPUs before calling disable_oob_stage() to prevent them from running out-of-band handlers while the out-of-band stage is being dismantled. This is particularly important if these handlers belong to a dynamically loaded module which might be unloaded right after disable_oob_stage() returns. In that case, you certainly don’t want the .text section containing interrupt handlers to vanish while they are still running.\n  Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Installing the out-of-band stage",
    "uri": "/ru/dovetail/pipeline/stage_push/"
  },
  {
    "content": "Sometimes you may need to escalate the current execution stage from in-band to out-of-band, only for running a particular routine. This can be done using run_oob_call(). For instance, the EVL core is using this service to escalate calls to its rescheduling procedure to the out-of-band stage, as described in the discussion about switching task contexts with Dovetail’s support for alternate scheduling.\n  int run_oob_call(int (*fn)(void *arg), void *arg)  fnThe address of the routine to execute on the out-of-band stage.\n\nargThe routine argument.\n\nrun_oob_call() first switches the current execution stage to out-of-band - if need be - then calls the routine with hard interrupts disabled (i.e. disabled in the CPU). Upon return, the integer value returned by fn() is passed back to the caller.\nBecause the routine may switch the execution stage back to in-band for the calling context, run_oob_call() restores the original stage only if it did not change in the meantime. In addition, the interrupt log of the current CPU is synchronized before returning to the caller. The following matrix describes the logic for determining which epilogue should be performed before leaving run_oob_call(), depending on the active stage on entry to the latter and on return from fn():\n   On entry to run_oob_call() At exit from fn() Epilogue     out-of-band out-of-band sync current stage if not stalled   in-band out-of-band switch to in-band + sync both stages   out-of-band in-band sync both stages   in-band in-band sync both stages    Заметкаrun_oob_call() is a lightweight operation that switches the CPU to the out-of-band interrupt stage for the duration of the call, whatever the underlying context may be. This is different from switching a task context to the out-of-band stage by offloading it to the autonomous core for scheduling. The latter operation would involve a more complex procedure.\n  Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Stage escalation",
    "uri": "/ru/dovetail/pipeline/stage_escalation/"
  },
  {
    "content": "Additional spinlock types The pipeline core introduces two spinlock types:\n hard spinlocks manipulate the CPU interrupt mask, and don’t affect the kernel preemption state in locking/unlocking operations.  This type of spinlock is useful for implementing a critical section to serialize concurrent accesses from both in-band and out-of-band contexts, i.e. from in-band and oob stages. Obviously, sleeping into a critical section protected by a hard spinlock would be a very bad idea. In other words, hard spinlocks are not subject to virtual interrupt masking, therefore can be used to serialize with out-of-band activities, including from the in-band kernel code. At any rate, those sections ought to be quite short, for keeping latency low.\n hybrid spinlocks are used internally by the pipeline core to protect access to IRQ descriptors (struct irq_desc::lock), so that we can keep the original locking scheme of the generic IRQ core unmodified for handling out-of-band interrupts.  Mutable spinlocks behave like hard spinlocks when traversed by the low-level IRQ handling code on entry to the pipeline, or common raw spinlocks otherwise, preserving the kernel (virtualized) interrupt and preemption states as perceived by the in-band context. This type of lock is not meant to be used in any other situation.\nLockdep support The lock validator automatically reconciles the real and virtual interrupt states, so it can deliver proper diagnosis for locking constructs defined in both in-band and out-of-band contexts. This means that hard and hybrid spinlocks are included in the validation set when LOCKDEP is enabled.\nВниманиеThese two additional types are subject to LOCKDEP analysis. However, be aware that latency figures are likely to be really bad when LOCKDEP is enabled, due to the large amount of work the lock validator may have to do with interrupts disabled for the CPU (i.e. hard locking) for enforcing critical sections.\n  Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Locking",
    "uri": "/ru/dovetail/pipeline/locking/"
  },
  {
    "content": "A set of ancillary services which are not directly related to EVL elements.\n  struct evl_version evl_get_version(void)  This function returns the available version information about the current libevl API and EVL core.\nA structure of type struct evl_version containing such information is returned (this call cannot fail). The definition of this type is as follows:\nstruct evl_version { int api_level;\t/* libevl.so: __EVL__ */ int abi_level;\t/* core: EVL_ABI_PREREQ, -1 for ESHI */ const char *version_string; };   api_level matches the value carried by the __EVL__ macro-definition when the libevl code was compiled. A list of released API versions is available in this document.\n  the abi_level is dynamically returned from the EVL core running on the current machine. Details about ABI management in EVL can be found in this document.\n  a version string which collates all the revision information available for pretty-printing.\n    void evl_sigdebug_handler(int sig, siginfo_t *si, void *ctxt)  This routine is a basic signal handler for the SIGDEBUG signal which simply prints out the HM diagnostics received to stdout then returns.\nlibevl does not install this handler by default, this is up to your application to do so if need be.\nsig, si and ctxt correspond to the parameters received by the sa_sigaction handler which your application should install using sigaction() (SA_SIGINFO must be set in the action flags).\n Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Miscellanea",
    "uri": "/ru/core/user-api/misc/"
  },
  {
    "content": "Перенос интерфейса “Ласточкин хвост” на другой выпуск ядра или другую архитектуру процессора включает в себя сначала включение конвейера прерываний, а затем функцию альтернативного планирования, которая основана на первом.\nКонвейерная обработка прерываний включается путем включения CONFIG_IRQ_PIPELINE: обеспечение безупречной работы этой функции является необходимым условием для продолжения работы остальной части порта “Ласточкин хвост”. По крайней мере, вы должны убедиться, что эти функции ядра совместимы с конвейерной обработкой прерываний:\n CONFIG_PREEMPT CONFIG_TRACE_IRQFLAGS (выбран с помощью CONFIG_IRQSOFF_TRACER, CONFIG_PROVE_LOCKING) CONFIG_LOCKDEP (выбран с помощью CONFIG_PROVE_LOCKING, CONFIG_LOCK_STAT, CONFIG_DEBUG_WW_MUTEX_SLOWPATH, CONFIG_DEBUG_LOCK_ALLOC) CONFIG_CPU_IDLE  Только тогда, когда этот слой будет прочным, вы должны начать работать над включением поддержки альтернативного планирования, которая позволит вашему автономному ядру планировать задачи, созданные ядром. В конце этого процесса включение CONFIG_DOVETAIL должно обеспечить полную поддержку подключения вашего автономного ядра к ядру Linux.\nЧтобы прояснить, что такое включает в себя перенос, давайте рассмотрим основные подсистемы/функции ядра, на которые повлияли изменения, связанные с “Ласточкиным хвостом”:\nIRQFLAGS Это самое основное изменение, которое необходимо внести в ядро, чтобы превратить специфичный для архитектуры API, управляющий флагом прерывания процессора, в эквивалентные виртуализированные вызовы, предоставляемые конвейером прерываний. Виртуализируя эти операции, “Ласточкин хвост” обеспечивает поступление событий аппаратного прерывания, сохраняя при этом внутриполосный код ядра от ненадлежащей доставки прерываний, как описано в этом документе. Вам необходимо следовать этой процедуре для реализации такой виртуализации.\nАтомарные ОПЕРАЦИИ После того, как IRQFLAGS были адаптированы для прерывания конвейерной обработки, исходные атомарные операции, которые основаны на явном отключении аппаратных прерываний для обеспечения атомарности, больше не могут работать, если только сайты вызовов не ограничены внутриполосным контекстом, что не является вариантом, поскольку они, безусловно, понадобятся нам и для выполнения атомарных операций из внеполосного контекста. Поэтому нам нужно сгладить их таким образом, чтобы добавить обратную сериализацию между абонентами во время обновлений, независимо от контекста вызывающего абонента. Для достижения этой цели необходимо исправить как несколько общих, так и специфичных для архитектуры операций как описано здесь.\nОбработка IPI С аппаратным обеспечением, поддерживающим SMP, ядро использует межпроцессорные прерывания (они же IPI) для уведомления удаленных ядер о конкретных событиях, которые могли произойти на стороне отправки. Чтобы “Ласточкин хвост” позволял автономному ядру запускать и получать некоторые из этих специальных прерываний, таких как любое обычное (например, устройство) прерывание, требуется некоторый специфичный для архитектуры код. Например, автономному ядру с поддержкой SMP потребуется перепланировка IPI, определенная “Ласточкиным хвостом”, чтобы заставить удаленное ядро перепланировать свои задачи.\nЗапись в ядре Запись ядра начинается с низкоуровневого кода ассемблера, получающего прерывания, ловушки/исключения и системные вызовы. По этой причине нам необходим следующий набор изменений:\n  во-первых, мы хотим направить события IRQ в конвейер прерываний, вместо того, чтобы доставлять IRQ непосредственно в исходный низкоуровневый внутриполосный обработчик (например, handle_arch_irq() для ARM). С этим изменением конвейер может немедленно отправлять события внешним обработчикам, если таковые имеются, а затем условно отправлять их также во внутриполосный код, если он принимает прерывания, или откладывать их до тех пор, пока это не произойдет. Это изменение является необходимым условием для включения конвейера прерываний.\n  поскольку ошибки/исключения могут возникать при выполнении на внеполосной стадии (например, некоторые задачи, выполняющие внеполосные ошибки, приводящие к нарушению доступа к памяти), возврат после обработки ошибок должен пропустить код эпилога, который проверяет наличие внутриполосных условий, таких как возможности для перепланирования задач пользователя или/и упреждение ядра.\n  системные вызовы - это особый тип синхронных ловушек, добровольно запускаемых кодом приложения для того, чтобы ядро выполнило для них какое-либо действие. Dovetail направляет системные вызовы, выполняемые задачами, для которых альтернативное планирование включено в сопутствующее ядро. В наши дни большинство архитектур Dovetail поддерживают обработку системных вызовов из кода C (например, x86 и arm64). Некоторые все еще могут делать это из секции сборки низкого уровня (например, ARM), и в этом случае логика маршрутизации Dovetail выполняется оттуда. В основном ядре наблюдается четкая тенденция к переносу большей части кода обработки системных вызовов из сборки в (универсальную) реализацию на языке C, от которой “Ласточкин хвост” уже выигрывает.\n   Последнее изменение: Sat, 06 Nov 2021 13:05:07 MSK content/dovetail/porting/_index.md 5cd5fdc ",
    "description": "",
    "tags": null,
    "title": "Перенос Dovetail",
    "uri": "/ru/dovetail/porting/"
  },
  {
    "content": "Generic requirements The interrupt pipeline requires the following features to be available from the target Linux kernel:\n  Generic IRQ handling (CONFIG_GENERIC_IRQ) and IRQ domains (CONFIG_IRQ_DOMAIN), which most architectures should support these days.\n  Generic clock event abstraction (CONFIG_GENERIC_CLOCKEVENTS).\n  Generic clock source abstraction (!CONFIG_ARCH_USES_GETTIMEOFFSET).\n  Other assumptions ARM   a target ARM machine port must be allowed to specify its own IRQ handler at run time (CONFIG_MULTI_IRQ_HANDLER).\n  only armv6 CPUs and later are supported, excluding older generations of ARM CPUs. Support for ASID (CONFIG_CPU_HAS_ASID) is required.\n  machine does not have VIVT cache.\n  Вниманиеarmv5 is not supported due to the use of VIVT caches on these CPUs, which don’t cope well - at all - with low latency requirements. A work aimed at leveraging the legacy FCSE PID register for reducing the cost of cache invalidation in context switches has been maintained until 2013 by Gilles Chanteperdrix, as part of the legacy I-pipe project, Dovetail’s ancestor. This work can still be cloned from this GIT repository.\n  Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Prerequisites",
    "uri": "/ru/dovetail/porting/prerequisites/"
  },
  {
    "content": "Adapting the generic interrupt management (genirq) Interrupt pipelining involves a basic change in controlling the interrupt flow: handle_domain_irq() from the IRQ domain API redirects all parent IRQs to the pipeline entry by calling generic_pipeline_irq(), instead of generic_handle_irq().\nGeneric flow handlers acknowledge the incoming IRQ event in the hardware as usual, by calling the appropriate irqchip routine (e.g. irq_ack(), irq_eoi()) according to the interrupt type. However, the flow handlers do not immediately invoke the in-band interrupt handlers. Instead, they hand the event over to the pipeline core by calling handle_oob_irq().\nIf an out-of-band handler exists for the interrupt received, handle_oob_irq() invokes it immediately, after switching the execution context to the oob stage if not current yet. Otherwise, the event is marked as pending in the in-band stage’s log for the current CPU.\nThe execution flow throughout the kernel code in Dovetail’s pipelined interrupt model is illustrated by the following figure. Note the two-step process: first we try delivering the incoming IRQ to any out-of-band handler if present, then we may play any IRQ pending in the current per-CPU log, among which non-OOB events may reside.\nAs illustrated above, interrupt flow handlers may run twice for a single IRQ in Dovetail’s pipelined interrupt model:\n  first to submit the event immediately to any out-of-band handler which may be interested in it. This is achieved by calling handle_oob_irq(), whose role is to invoke such handler(s) if present, or schedule an in-band handling of the IRQ event if not.\n  finally to run the in-band handler(s) accepting the IRQ event if it was not delivered to any out-of-band handler. To deliver the event to any in-band handler(s), the interrupt flow handler is called again by the pipeline core. When this happens, the flow handler processes the interrupt as usual, skipping the call to handle_oob_irq() though.\n  ЗаметкаAny incoming IRQ event is either dispatched to one or more out-of-band handlers, or one or more in-band handlers, but never to a mix of them. Also, because every interrupt which is not handled by an out-of-band handler will end up into the in-band stage’s event log unconditionally, all external interrupts must have a handler in the in-band code - which should be the case for a sane kernel anyway.\n Once generic_pipeline_irq() has returned, if the preempted execution context was running over the in-band stage unstalled, the pipeline core synchronizes the interrupt state immediately, meaning that all IRQs found pending in the in-band stage’s log are immediately delivered to their respective in-band handlers. In all other situations, the IRQ frame is left immediately without running those handlers. The IRQs may remain pending until the in-band code resumes from preemption, then clears the virtual interrupt disable flag, which would cause the interrupt state to be synchronized, running the in-band handlers eventually.\nIn-band IRQ delivery glue code For delivering pending interrupts to the in-band stage, the generic Dovetail core synchronizing the IRQ stage calls a routine named arch_do_IRQ_pipelined(), which you must provide as part of the pipeline’s arch-specific support code. This function is passed both device IRQs and IPIs, it should dispatch the event accordingly according to the following logic:\ngraph LR; S(IRQ stage sync) -- E[\"arch_do_IRQ_pipelined()\"] style E fill:#99ccff; E -- A{Is IPI?} style A fill:#99ccff; A --|Yes| B[call IPI handler] style B fill:#99ccff; A --|No| C[\"do_domain_irq()\"] style C fill:#99ccff;  For ARM and ARM64, the corresponding code looks like this:\nvoid arch_do_IRQ_pipelined(struct irq_desc *desc) { struct pt_regs *regs = raw_cpu_ptr(\u0026irq_pipeline.tick_regs); unsigned int irq = irq_desc_get_irq(desc); #ifdef CONFIG_SMP /* * Check for IPIs, handing them over to the specific dispatch * code. */ if (irq \u003e= OOB_IPI_BASE \u0026\u0026 irq \u003c OOB_IPI_BASE + NR_IPI + OOB_NR_IPI) { __handle_IPI(irq - OOB_IPI_BASE, regs); return; } #endif do_domain_irq(irq, regs); } A couple of notes reading this code:\n  do_domain_irq() is a routine the generic Dovetail core implements, which fires the in-band handler for a device IRQ.\n  How IPIs differentiate from other IRQs, which handler should be called for them is an arch-specific implementation you should provide in porting Dovetail. In the code example above, the IPI handling routine is named __handle_IPI().\n  Since the interrupt delivery is deferred for the in-band stage until the latter is synchronized eventually, we don’t have access to the preempted register frame for a delayed interrupt event. Said differently, the interrupt context has already returned, only logging the interrupt event but not dispatching it yet, and the stack-based register frame of the preempted context is long gone. Fortunately, the kernel is normally only interested in analyzing frames attached to timer events (e.g. for profiling), so Dovetail only needs to save the register frame corresponding to the last tick event received to the per-CPU irq_pipeline.tick_regs variable. A pointer to such frame for the current CPU can be passed by your implementation of arch_do_IRQ_pipelined() to the interrupt handler.\n  Deferring level-triggered IRQs In absence of any out-of-band handler for the event, the device may keep asserting the interrupt signal until the cause has been lifted in its own registers. At the same time, we might not be allowed to run the in-band handler immediately over the current interrupt context if the in-band stage is currently stalled, we would have to wait for the in-band code to accept interrupts again. However, the interrupt disable bit in the CPU would certainly be cleared in the meantime. For this reason, depending on the interrupt type, the flow handlers as modified by the pipeline code may have to mask the interrupt line until the in-band handler has run from the in-band stage, lifting the interrupt cause. This typically happens with level-triggered interrupts, preventing the device from storming the CPU with a continuous interrupt request.\n The pathological case\n /* no OOB handler, in-band stage stalled on entry leading to deferred dispatch to handler */ asm_irq_entry ... -\u003e generic_pipeline_irq() ... \u003cIRQ logged, delivery deferred\u003e asm_irq_exit /* * CPU allowed to accept interrupts again with IRQ cause not * acknowledged in device yet =\u003e **IRQ storm**. */ asm_irq_entry ... asm_irq_exit asm_irq_entry ... asm_irq_exit Since all of the IRQ handlers sharing an interrupt line are either in-band or out-of-band in a mutually exclusive way, such masking cannot delay out-of-band events.\nСоветThe logic behind masking interrupt lines until events are processed at some point later - out of the original interrupt context - applies exactly the same way to the threaded interrupt model (i.e. IRQF_THREAD). In this case, interrupt lines may be masked until the IRQ thread is scheduled in, after the interrupt handler clears the event cause eventually.\n Adapting the interrupt flow handlers to pipelining The logic for adapting flow handlers dealing with interrupt pipelining is composed of the following steps:\n  (optionally) enter the critical section protected by the IRQ descriptor lock, if the interrupt is shared among processors (e.g. device interrupts). If so, check if the interrupt handler may run on the current CPU (irq_may_run()). By definition, no locking would be required for per-CPU interrupts.\n  check whether we are entering the pipeline in order to deliver the interrupt to any out-of-band handler registered for it. on_pipeline_entry() returns a boolean value denoting this situation.\n  if on pipeline entry, we should pass the event on to the pipeline core by calling handle_oob_irq(). Upon return, this routine tells the caller whether any out-of-band handler was fired for the event.\n  if so, we may assume that the interrupt cause is now cleared in the device, and we may leave the flow handler, after having restored the interrupt line into a normal state. In case of a level-triggered interrupt which has been masked on entry to the flow handler, we need to unmask the line before leaving.\n  if no out-of-band handler was called, we should have performed any acknowledge and/or EOI to release the interrupt line in the controller, while leaving it masked if required before exiting the flow handler. In case of a level-triggered interrupt, we do want to leave it masked for solving the pathological case with interrupt deferral explained earlier.\n    if not on pipeline entry (i.e. second entry of the flow handler), then we must be running over the in-band stage, accepting interrupts, therefore we should fire the in-band handler(s) for the incoming event.\n   Example: adapting the handler dealing with level-triggered IRQs\n --- a/kernel/irq/chip.c +++ b/kernel/irq/chip.c void handle_level_irq(struct irq_desc *desc) { raw_spin_lock(\u0026desc-\u003elock); mask_ack_irq(desc); if (!irq_may_run(desc)) goto out_unlock; +\tif (on_pipeline_entry()) { +\tif (handle_oob_irq(desc)) +\tgoto out_unmask; +\tgoto out_unlock; +\t} + desc-\u003eistate \u0026= ~(IRQS_REPLAY | IRQS_WAITING); /* @@ -642,7 +686,7 @@ void handle_level_irq(struct irq_desc *desc) kstat_incr_irqs_this_cpu(desc); handle_irq_event(desc); - +out_unmask: cond_unmask_irq(desc); out_unlock: raw_spin_unlock(\u0026desc-\u003elock); } This change reads as follows:\n  on entering the pipeline, which means immediately over the interrupt frame context set up by the CPU for receiving the event, tell the pipeline core about the incoming IRQ.\n  if this IRQ was handled by an out-of-band handler (handle_oob_irq() returns true), consider the event to have been fully processed, unmasking the interrupt line before leaving. We can’t do more than this, simply because the in-band kernel code might expect not to receive any interrupt at this point (i.e. the virtual interrupt disable flag might be set for the in-band stage).\n  otherwise, keep the interrupt line masked until handle_level_irq() is called again from a safe context for handling in-band interrupts, at which point the event should be delivered to the in-band interrupt handler of the main kernel. We have to keep the line masked to prevent the IRQ storm which would certainly happen otherwise, since no handler has cleared the cause of the interrupt event in the device yet.\n  Fixing up the IRQ chip drivers We must make sure the following handlers exported by irqchip drivers can operate over the out-of-band context safely:\n irq_mask() irq_ack() irq_mask_ack() irq_eoi() irq_unmask()  For so-called device interrupts, no change is required, because the genirq layer ensures a single CPU at most handles a given IRQ event by holding the per-descriptor irq_desc::lock spinlock across calls to those irqchip handlers, and such lock is automatically turned into an hybrid spinlock when pipelining interrupts. In other words, those handlers are properly serialized, running with interrupts disabled in the CPU as their non-pipelined implementation expects it.\nUnlike for device interrupts, per-CPU interrupt handling does not need to be serialized this way, since by definition, there cannot be multiple CPUs racing for access with such type of events.\nHowever, there might other reasons to fix up some of those handlers:\n  they must not invoke any in-band kernel service, which might cause an invalid context re-entry.\n  there may be inner spinlocks locally defined by some irqchip drivers for serializing access to a common interrupt controller hardware for distinct IRQs which are handled by multiple CPUs concurrently. Adapting such spinlocked sections found in irqchip drivers to support interrupt pipelining may involve converting the related spinlocks to hard spinlocks.\n  Other section of code which were originally serialized by common interrupt disabling may need to be made fully atomic for running consistenly in pipelined interrupt mode. This can be done by introducing hard masking, converting local_irq_save() calls to hard_local_irq_save(), conversely local_irq_restore() to hard_local_irq_restore().\nFinally, IRQCHIP_PIPELINE_SAFE must be added to the struct irqchip::flags member of a pipeline-aware irqchip driver, in order to notify the kernel that such controller can operate in pipelined interrupt mode. Even if you did not introduce any other change to support pipelining, this one is required: it tells the kernel that you did review the code for that purpose.\n Adapting the ARM GIC driver to interrupt pipelining\n --- a/drivers/irqchip/irq-gic.c +++ b/drivers/irqchip/irq-gic.c @@ -93,7 +93,7 @@ struct gic_chip_data { #ifdef CONFIG_BL_SWITCHER -static DEFINE_RAW_SPINLOCK(cpu_map_lock); +static DEFINE_HARD_SPINLOCK(cpu_map_lock); #define gic_lock_irqsave(f)\t\\ raw_spin_lock_irqsave(\u0026cpu_map_lock, (f)) @@ -424,7 +424,8 @@ static const struct irq_chip gic_chip = { .irq_set_irqchip_state\t= gic_irq_set_irqchip_state, .flags\t= IRQCHIP_SET_TYPE_MASKED | IRQCHIP_SKIP_SET_WAKE | -\tIRQCHIP_MASK_ON_SUSPEND, +\tIRQCHIP_MASK_ON_SUSPEND | +\tIRQCHIP_PIPELINE_SAFE, }; void __init gic_cascade_irq(unsigned int gic_nr, unsigned int irq)  Adapting the BCM2835 pin control driver to interrupt pipelining\n --- a/drivers/pinctrl/bcm/pinctrl-bcm2835.c +++ b/drivers/pinctrl/bcm/pinctrl-bcm2835.c @@ -79,7 +79,7 @@ struct bcm2835_pinctrl { struct gpio_chip gpio_chip; struct pinctrl_gpio_range gpio_range; -\traw_spinlock_t irq_lock[BCM2835_NUM_BANKS]; +\thard_spinlock_t irq_lock[BCM2835_NUM_BANKS]; }; /* pins are just named GPIO0..GPIO53 */ @@ -608,6 +608,7 @@ static struct irq_chip bcm2835_gpio_irq_chip = { .irq_ack = bcm2835_gpio_irq_ack, .irq_mask = bcm2835_gpio_irq_disable, .irq_unmask = bcm2835_gpio_irq_enable, +\t.flags = IRQCHIP_PIPELINE_SAFE, }; In some (rare) cases, we might have a bit more work for adapting an interrupt chip driver. For instance, we might have to convert a sleeping spinlock to a raw spinlock first, so that we can convert the latter to a hard spinlock eventually. Hard spinlocks like raw ones should be manipulated via the raw_spin_lock() API, unlike sleeping spinlocks.\nЗаметкаirq_set_chip() will complain loudly with a kernel warning whenever the irqchip descriptor passed does not bear the IRQCHIP_PIPELINE_SAFE flag and CONFIG_IRQ_PIPELINE is enabled. Take this warning as a sure sign that your port of the IRQ pipeline to the target system is incomplete.\n Kernel preemption control (CONFIG_PREEMPT) When pipelining is enabled, Dovetail ensures preempt_schedule_irq() reconciles the virtual interrupt state - which has not been touched by the assembly level code upon kernel entry - with basic assumptions made by the scheduler core, such as entering with interrupts virtually disabled (i.e. the in-band stage should be stalled).\n Последнее изменение: Wed, 27 Jun 2018 15:20:04 +0200 ",
    "description": "",
    "tags": null,
    "title": "Interrupt flow",
    "uri": "/ru/dovetail/porting/irqflow/"
  },
  {
    "content": "Эффект виртуализации защиты от прерываний должен быть отменен для атомарных помощников везде, где требуется отключение прерываний для сериализации вызывающих абонентов, независимо от стадии, на которой они находятся. Как правило, речь идет о следующих файлах:\n include/asm-generic/atomic.h include/asm-generic/cmpxchg-local.h include/asm-generic/cmpxchg.h  Аналогично в коде, зависящем от архитектуры:\n arch/arm/include/asm/atomic.h arch/arm/include/asm/bitops.h arch/arm/include/asm/cmpxchg.h  Это необходимо для того, чтобы эти помощники могли работать с данными, к которым можно получить доступ с обоих этапов. Распространенный способ возврата к такой виртуализации включает замену вызовов API -виртуализированного - local_irq_save(), local_irq_restore() их жесткими невиртуализированными аналогами.\n Восстановление строгой сериализации для операций с универсальными атомарными счетчиками\n --- a/include/asm-generic/atomic.h +++ b/include/asm-generic/atomic.h @@ -80,9 +80,9 @@ static inline void atomic_##op(int i, atomic_t *v)\t\\  {\t\\ unsigned long flags;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\  v-\u003ecounter = v-\u003ecounter c_op i;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\  } #define ATOMIC_OP_RETURN(op, c_op)\t\\ @@ -91,9 +91,9 @@ static inline int atomic_##op##_return(int i, atomic_t *v)\t\\  unsigned long flags;\t\\ int ret;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\  ret = (v-\u003ecounter = v-\u003ecounter c_op i);\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\  \\ return ret;\t\\ } @@ -104,10 +104,10 @@ static inline int atomic_fetch_##op(int i, atomic_t *v)\t\\  unsigned long flags;\t\\ int ret;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\  ret = v-\u003ecounter;\t\\ v-\u003ecounter = v-\u003ecounter c_op i;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\  \\ return ret;\t\\ } Аналогично, такие операции могут существовать в специфичном для архитектуры коде, переопределяя их общие определения. Например, порт ARM определяет свою собственную версию атомарных операций, для которых необходимо восстановить реальную защиту от прерываний:\n Восстановление строгой сериализации для операций с атомарными счетчиками для ARM\n --- a/arch/arm/include/asm/atomic.h +++ b/arch/arm/include/asm/atomic.h @@ -168,9 +168,9 @@ static inline void atomic_##op(int i, atomic_t *v)\t\\  {\t\\ unsigned long flags;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\  v-\u003ecounter c_op i;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\  }\t\\ #define ATOMIC_OP_RETURN(op, c_op, asm_op)\t\\ @@ -179,10 +179,10 @@ static inline int atomic_##op##_return(int i, atomic_t *v)\t\\  unsigned long flags;\t\\ int val;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\  v-\u003ecounter c_op i;\t\\ val = v-\u003ecounter;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\  \\ return val;\t\\ } @@ -193,10 +193,10 @@ static inline int atomic_fetch_##op(int i, atomic_t *v)\t\\  unsigned long flags;\t\\ int val;\t\\ \\ -\traw_local_irq_save(flags);\t\\ +\tflags = hard_local_irq_save();\t\\  val = v-\u003ecounter;\t\\ v-\u003ecounter c_op i;\t\\ -\traw_local_irq_restore(flags);\t\\ +\thard_local_irq_restore(flags);\t\\  \\ return val;\t\\ } @@ -206,11 +206,11 @@ static inline int atomic_cmpxchg(atomic_t *v, int old, int new)  int ret; unsigned long flags; -\traw_local_irq_save(flags); +\tflags = hard_local_irq_save();  ret = v-\u003ecounter; if (likely(ret == old)) v-\u003ecounter = new; -\traw_local_irq_restore(flags); +\thard_local_irq_restore(flags);  return ret; }  Последнее изменение: Sat, 06 Nov 2021 18:13:25 MSK content/dovetail/porting/atomic.md b139a43 ",
    "description": "",
    "tags": null,
    "title": "Атомарные операции",
    "uri": "/ru/dovetail/porting/atomic/"
  },
  {
    "content": "Виртуализация маски прерывания Специфичный для архитектуры код, который управляет флагом прерывания в регистре состояния процессора в arch/\u003cyour-arch\u003e/include/asm/irqflags.h следует разделить между реальным и виртуальным управлением прерываниями. Реальные операции управления прерываниями унаследованы от реализации внутриполосного ядра. Виртуальные должны быть построены на услугах, предоставляемых ядром конвейера прерываний.\n во-первых, исходные помощники arch\\_local_* следует переименовать в помощники native_* , влияющие на состояние аппаратного прерывания в ЦП. Это соглашение об именовании накладывается на код архитектуры универсальными помощниками в include/asm-generic/irq\\_pipeline.h.   Пример: представляем собственные средства доступа к состояниям прерывания для архитектуры ARM\n --- a/arch/arm/include/asm/irqflags.h +++ b/arch/arm/include/asm/irqflags.h  #if __LINUX_ARM_ARCH__ \u003e= 6 #define arch_local_irq_save arch_local_irq_save -static inline unsigned long arch_local_irq_save(void) +static inline unsigned long native_irq_save(void)  { unsigned long flags; asm volatile( -\t\"\tmrs\t%0, \" IRQMASK_REG_NAME_R \"\t@ arch_local_irq_save\\n\" +\t\"\tmrs\t%0, \" IRQMASK_REG_NAME_R \"\t@ native_irq_save\\n\"  \"\tcpsid\ti\" : \"=r\" (flags) : : \"memory\", \"cc\"); return flags; } #define arch_local_irq_enable arch_local_irq_enable -static inline void arch_local_irq_enable(void) +static inline void native_irq_enable(void)  { asm volatile( -\t\"\tcpsie i\t@ arch_local_irq_enable\" +\t\"\tcpsie i\t@ native_irq_enable\"  : : : \"memory\", \"cc\"); } #define arch_local_irq_disable arch_local_irq_disable -static inline void arch_local_irq_disable(void) +static inline void native_irq_disable(void)  { asm volatile( -\t\"\tcpsid i\t@ arch_local_irq_disable\" +\t\"\tcpsid i\t@ native_irq_disable\"  : : : \"memory\", \"cc\"); @@ -69,12 +76,12 @@ static inline void arch_local_irq_disable(void)  * Save the current interrupt enable state \u0026 disable IRQs */ #define arch_local_irq_save arch_local_irq_save -static inline unsigned long arch_local_irq_save(void) +static inline unsigned long native_irq_save(void)  { unsigned long flags, temp; asm volatile( -\t\"\tmrs\t%0, cpsr\t@ arch_local_irq_save\\n\" +\t\"\tmrs\t%0, cpsr\t@ native_irq_save\\n\"  \"\torr\t%1, %0, #128\\n\" \"\tmsr\tcpsr_c, %1\" : \"=r\" (flags), \"=r\" (temp) @@ -87,11 +94,11 @@ static inline unsigned long arch_local_irq_save(void)  * Enable IRQs */ #define arch_local_irq_enable arch_local_irq_enable -static inline void arch_local_irq_enable(void) +static inline void native_irq_enable(void)  { unsigned long temp; asm volatile( -\t\"\tmrs\t%0, cpsr\t@ arch_local_irq_enable\\n\" +\t\"\tmrs\t%0, cpsr\t@ native_irq_enable\\n\"  \"\tbic\t%0, %0, #128\\n\" \"\tmsr\tcpsr_c, %0\" : \"=r\" (temp) @@ -103,11 +110,11 @@ static inline void arch_local_irq_enable(void)  * Disable IRQs */ #define arch_local_irq_disable arch_local_irq_disable -static inline void arch_local_irq_disable(void) +static inline void native_irq_disable(void)  { unsigned long temp; asm volatile( -\t\"\tmrs\t%0, cpsr\t@ arch_local_irq_disable\\n\" +\t\"\tmrs\t%0, cpsr\t@ native_irq_disable\\n\"  \"\torr\t%0, %0, #128\\n\" \"\tmsr\tcpsr_c, %0\" : \"=r\" (temp) @@ -153,11 +160,11 @@ static inline void arch_local_irq_disable(void)  * Save the current interrupt enable state. */ #define arch_local_save_flags arch_local_save_flags -static inline unsigned long arch_local_save_flags(void) +static inline unsigned long native_save_flags(void)  { unsigned long flags; asm volatile( -\t\"\tmrs\t%0, \" IRQMASK_REG_NAME_R \"\t@ local_save_flags\" +\t\"\tmrs\t%0, \" IRQMASK_REG_NAME_R \"\t@ native_save_flags\"  : \"=r\" (flags) : : \"memory\", \"cc\"); return flags; } @@ -166,21 +173,28 @@ static inline unsigned long arch_local_save_flags(void)  * restore saved IRQ \u0026 FIQ state */ #define arch_local_irq_restore arch_local_irq_restore -static inline void arch_local_irq_restore(unsigned long flags) +static inline void native_irq_restore(unsigned long flags)  { asm volatile( -\t\"\tmsr\t\" IRQMASK_REG_NAME_W \", %0\t@ local_irq_restore\" +\t\"\tmsr\t\" IRQMASK_REG_NAME_W \", %0\t@ native_irq_restore\"  : : \"r\" (flags) : \"memory\", \"cc\"); } #define arch_irqs_disabled_flags arch_irqs_disabled_flags -static inline int arch_irqs_disabled_flags(unsigned long flags) +static inline int native_irqs_disabled_flags(unsigned long flags)  { return flags \u0026 IRQMASK_I_BIT; } +static inline bool native_irqs_disabled(void) +{ +\tunsigned long flags = native_save_flags(); +\treturn native_irqs_disabled_flags(flags); +} + +#include \u003casm/irq_pipeline.h\u003e  #include \u003casm-generic/irqflags.h\u003e #endif /* ifdef __KERNEL__ */  в финале, должен быть предоставлен новый набор помощников arch\\_local\\_*, влияющих на флаг отключения виртуального прерывания, реализованный ядром конвейера для управления защитой внутриполосной стадии от прерываний. Рекомендуется реализовать этот набор в отдельном файле, доступном для включения из \u003casm/irq_pipeline.h\u003e.   Пример: предоставление средств доступа к состоянию виртуального прерывания для архитектуры ARM\n --- /dev/null +++ b/arch/arm/include/asm/irq_pipeline.h @@ -0,0 +1,138 @@ +#ifndef _ASM_ARM_IRQ_PIPELINE_H +#define _ASM_ARM_IRQ_PIPELINE_H + +#include \u003casm-generic/irq_pipeline.h\u003e + +#ifdef CONFIG_IRQ_PIPELINE + +static inline notrace unsigned long arch_local_irq_save(void) +{ +\tint stalled = inband_irq_save(); +\tbarrier(); +\treturn arch_irqs_virtual_to_native_flags(stalled); +} + +static inline notrace void arch_local_irq_enable(void) +{ +\tbarrier(); +\tinband_irq_enable(); +} + +static inline notrace void arch_local_irq_disable(void) +{ +\tinband_irq_disable(); +\tbarrier(); +} + +static inline notrace unsigned long arch_local_save_flags(void) +{ +\tint stalled = inband_irqs_disabled(); +\tbarrier(); +\treturn arch_irqs_virtual_to_native_flags(stalled); +} + +static inline int arch_irqs_disabled_flags(unsigned long flags) +{ +\treturn native_irqs_disabled_flags(flags); +} + +static inline notrace void arch_local_irq_restore(unsigned long flags) +{ +\tif (!arch_irqs_disabled_flags(flags)) +\t__inband_irq_enable(); +\tbarrier(); +} + +#else /* !CONFIG_IRQ_PIPELINE */ + +static inline unsigned long arch_local_irq_save(void) +{ +\treturn native_irq_save(); +} + +static inline void arch_local_irq_enable(void) +{ +\tnative_irq_enable(); +} + +static inline void arch_local_irq_disable(void) +{ +\tnative_irq_disable(); +} + +static inline unsigned long arch_local_save_flags(void) +{ +\treturn native_save_flags(); +} + +static inline void arch_local_irq_restore(unsigned long flags) +{ +\tnative_irq_restore(flags); +} + +static inline int arch_irqs_disabled_flags(unsigned long flags) +{ +\treturn native_irqs_disabled_flags(flags); +} + +#endif /* !CONFIG_IRQ_PIPELINE */ + +#endif /* _ASM_ARM_IRQ_PIPELINE_H */  ЗаметкаЭтот новый файл должен включать \u003casm-generic/irq_pipeline.h\u003e заранее, чтобы получить доступ к объявлениям конвейера, в которых он нуждается. Это включение должно быть безусловным, даже если ядро построено с отключенной конфигурацией CONFIG_IRQ_PIPELINE.\n Обеспечение поддержки объединенных состояний прерывания Реализация конвейера общих прерываний требует, чтобы код поддержки на уровне архитектуры обеспечивал пару помощников, направленных на преобразование отключение флага виртуального прерывания в бит прерывания в регистре состояния процессора (например, PSR_I_BIT для ARM) и наоборот. Эти помощники используются для создания комбинированных слов состояний, объединяющих виртуальные и реальные состояния прерывания.\n  arch_irqs_virtual_to_native_flags(int stalled) должно возвращать длинное слово, переназначающее логическое значение @stalled в позию бита прерывания процессора в регистре состояния. Все остальные биты должны быть очищены.\n На ARM это может быть выражено как (stalled ? PSR_I_BIT : 0). на x86 это было бы скорее (stalled ? 0 : X86_EFLAGS_IF).    arch_irqs_native_to_virtual_flags(unsigned long flags) необходимо вернуть длинное слово, переназначающее бит прерывания процессора в @flags в произвольную позицию бита, выбранную так, чтобы не конфликтовать с первым. Другими словами, бит состояния прерывания процессора, полученный в @flags, должен быть смещен в свободное положение, произвольно выбранное в возвращаемом значении. Все остальные биты должны быть очищены.\n  На ARM, используя позицию бита 31 для отражения виртуального состояния, это выражается как (hard_irqs_disabled_flags(flags) ? (1 \u003c\u003c 31) : 0).\n  В любой другой архитектуре реализация была бы аналогичной, используя любую доступную позицию бита, которая не конфликтовала бы с позицией бита прерывания процессора.\n    --- a/arch/arm/include/asm/irqflags.h +++ b/arch/arm/include/asm/irqflags.h /* * CPU interrupt mask handling. */ #ifdef CONFIG_CPU_V7M #define IRQMASK_REG_NAME_R \"primask\" #define IRQMASK_REG_NAME_W \"primask\" #define IRQMASK_I_BIT\t1 +#define IRQMASK_I_POS\t0  #else #define IRQMASK_REG_NAME_R \"cpsr\" #define IRQMASK_REG_NAME_W \"cpsr_c\" #define IRQMASK_I_BIT\tPSR_I_BIT +#define IRQMASK_I_POS\t7  #endif +#define IRQMASK_i_POS\t31  СоветIRQMASK_i_POS (обратите внимание на прописную ‘i’) это позиция свободного бита в комбинированном слове, где порт ARM сохраняет исходное состояние прерывания процессора в комбинированном слове. Эта позиция не может конфликтовать с IRQMASK_I_POS, который является псевдонимом PSR_I_BIT (разрядная позиция 0 или 7).\n --- /dev/null +++ b/arch/arm/include/asm/irq_pipeline.h + +static inline notrace +unsigned long arch_irqs_virtual_to_native_flags(int stalled) +{ +\treturn (!!stalled) \u003c\u003c IRQMASK_I_POS; +} +static inline notrace +unsigned long arch_irqs_native_to_virtual_flags(unsigned long flags) +{ +\treturn (!!hard_irqs_disabled_flags(flags)) \u003c\u003c IRQMASK_i_POS; +}  ИнформацияКак только все эти изменения будут внесены, общие помощники из \u003clinux/irqflags.h\u003e, такие как local_irq_disable() и local_irq_enable(), фактически ссылаются на виртуальную схему защиты, когда прерывания передаются по конвейеру, что в конечном итоге позволяет реализовать отсрочку прерывания для защищенного внутриполосного кода, выполняемого на внутриполосной стадии.\n Адаптация кода ассемблера к конвейерной обработке IRQ Запись прерывания Поскольку общая обработка IRQ является обязательным требованием для поддержки Dovetail, низкоуровневый обработчик прерываний, находящийся в части кода ассемблера архитектуры, все еще может доставлять все события прерывания в исходный обработчик написанный на C, предоставляемый драйвером irqchip. Этот обработчик, в свою очередь, должен вызывать:\n  handle_domain_irq() для всех IRQ родительского устройства\n  generic_handle_irq() для каскадных IRQ-запросов устройств (декодированных из родительского обработчика)\n  Для этих подпрограмм начальная задача вставки прерывания в начале конвейера обрабатывается непосредственно с уровня genirq, к которому они принадлежат. Это означает, что обычно не так много нужно сделать, кроме как быстро проверить реализацию родительского обработчика IRQ в соответствующем драйвере irqchip, тщательно применяя эмпирические правила.\nСоветНа какой-нибудь платформе ARM, оснащенной довольно распространенным контроллером GIC, это означало бы, например, проверку функции gic_handle_irq().\n  специфичный для архитектуры handle_IPI() или эквивалент для специальных межпроцессорных прерываний  IPI должны быть обработаны конкретными изменениями, внесенными портом, о котором мы расскажем позже.\nПрерывание выхода Когда конвейерная обработка прерываний отключена, ядро обычно запускает эпилог после обработки каждого события прерывания или исключения. Если событие произошло, когда процессор выполнял какой-либо код ядра, эпилог проверит потенциальную возможность перепланирования в случае, если включен параметр CONFIG_PREEMPT. Если задача пользовательского пространства была вытеснена событием, будут проверены дополнительные условия, такие как ожидание доставки сигнала для этой задачи.\nПоскольку прерывания практически маскируются только для внутриполосного кода, когда включена конвейерная обработка, ЦП все равно может принимать запросы IRQ и передавать их ассемблерным обработчикам низкого уровня, чтобы они могли входить в конвейер прерываний.\n Запуск обычного эпилога после IRQ допустим только в том случае, если ядро фактически принимало прерывания, когда произошло событие (т.е. Флаг отключения виртуального прерывания был снят), и запускался внутриполосный код.\n Во всех остальных случаях, за исключением конвейера прерываний ядра, остальная часть ядра не ожидает, что эти IRQ когда-либо произойдут в первую очередь. Поэтому запуск эпилога в таких обстоятельствах противоречил бы логике ядра. Кроме того, низкоуровневые обработчики должны быть осведомлены о том, что они могут получить событие в таких условиях.\nНапример, исходный код ARM для обработки IRQ, который вытеснил контекст ядра, будет выглядеть следующим образом:\n__irq_svc: svc_entry irq_handler #ifdef CONFIG_PREEMPT ldr\tr8, [tsk, #TI_PREEMPT]\t@ get preempt count ldr\tr0, [tsk, #TI_FLAGS]\t@ get flags teq\tr8, #0\t@ if preempt count != 0 movne\tr0, #0\t@ force flags to 0 tst\tr0, #_TIF_NEED_RESCHED blne\tsvc_preempt #endif Чтобы правильно обрабатывать прерывания в конвейерной модели доставки, мы должны определить, было ли внутриполосное ядро готово принять такое событие, действуя на него соответствующим образом. С этой целью порт ARM вместо этого передает событие в подпрограмму трамплина (handle_arch_irq_pipelined()), ожидая по возвращении решения о том, должен ли код эпилога выполняться следующим. На рисунке ниже это решение возвращается вызывающему в виде логического статуса, отличного от нуля, что означает, что мы можем запустить эпилог, в противном случае ноль.\n--- a/arch/arm/kernel/entry-armv.S +++ b/arch/arm/kernel/entry-armv.S  .macro\tirq_handler #ifdef CONFIG_MULTI_IRQ_HANDLER -\tldr\tr1, =handle_arch_irq  mov\tr0, sp badr\tlr, 9997f +#ifdef CONFIG_IRQ_PIPELINE +\tldr\tr1, =handle_arch_irq_pipelined +\tmov\tpc, r1 +#else +\tldr\tr1, =handle_arch_irq  ldr\tpc, [r1] -#else +#endif +#elif CONFIG_IRQ_PIPELINE +#error \"Legacy IRQ handling not pipelined\" +#else  arch_irq_handler_default #endif 9997: .endm Процедура трамплина, добавленная в исходный код, сначала доставляет прерывание определяемому машиной обработчику, а затем сообщает вызывающему, может ли обычный эпилог выполняться для такого события.\n--- a/arch/arm/kernel/irq.c +++ b/arch/arm/kernel/irq.c @@ -112,6 +112,15 @@ void __init set_handle_irq(void (*handle_irq)(struct pt_regs *))  } #endif +#ifdef CONFIG_IRQ_PIPELINE +asmlinkage int __exception_irq_entry +handle_arch_irq_pipelined(struct pt_regs *regs) +{ +\thandle_arch_irq(regs); +\treturn running_inband() \u0026\u0026 !irqs_disabled(); +} +#endif + Eventually, the low-level assembly handler receiving the interrupt event is adapted, in order to carry out the earlier decision by handle_arch_irq_pipelined(), skipping the epilogue code if required to.\n--- a/arch/arm/kernel/entry-armv.S +++ b/arch/arm/kernel/entry-armv.S __irq_svc: svc_entry irq_handler +#ifdef CONFIG_IRQ_PIPELINE +\ttst\tr0, r0 +\tbeq\t1f +#endif #ifdef CONFIG_PREEMPT ldr\tr8, [tsk, #TI_PREEMPT]\t@ get preempt count blne\tsvc_preempt #endif +1: svc_exit r5, irq = 1\t@ return from exception  ВниманиеTaking the fast exit path when applicable is critical to the stability of the target system to prevent invalid re-entry of the in-band kernel code.\n Fault exit Similarly to the interrupt exit case, the low-level fault handling code must skip the epilogue code when the fault was taken over an out-of-band context. Upon fault, the current interrupt state is not considered for determining whether we should run the epilogue, since a fault may occur independently of such state.\n Running the regular epilogue after a fault is valid only if that fault was triggered by some in-band code, excluding any fault raised by out-of-band code.\n For instance, the original ARM code for returning from an exception event would be modified as follows:\n--- a/arch/arm/kernel/entry-armv.S +++ b/arch/arm/kernel/entry-armv.S @@ -754,7 +772,7 @@ ENTRY(ret_from_exception) UNWIND(.cantunwind\t) get_thread_info tsk mov\twhy, #0 -\tb\tret_to_user +\tret_to_user_pipelined r1 UNWIND(.fnend\t) With the implementation of ret_to_user_pipelined checking for the current stage, skipping the epilogue if the faulting code was running over an out-of-band context:\n--- a/arch/arm/kernel/entry-header.S +++ b/arch/arm/kernel/entry-header.S +/* + * Branch to the exception epilogue, skipping the in-band work + * if running over the oob interrupt stage. + */ +\t.macro ret_to_user_pipelined, tmp +#ifdef CONFIG_IRQ_PIPELINE +\tldr\t\\tmp, [tsk, #TI_LOCAL_FLAGS] +\ttst\t\\tmp, #_TLF_OOB +\tbne\tfast_ret_to_user +#endif +\tb\tret_to_user +\t.endm + _TLF_OOB is a local thread_info flag denoting a current task running out-of-band code over the out-of-band stage. If set, the epilogue must be skipped.\nReconciling the virtual interrupt state to the epilogue logic A tricky issue to address when pipelining interrupts is about making sure that the logic from the epilogue routine (e.g. do_work_pending(), do_notify_resume()) actually runs in the expected (virtual) interrupt state for the in-band stage.\nReconciling the virtual interrupt state to the in-band logic dealing with interrupts is required because in a pipelined interrupt model, the virtual interrupt state of the in-band stage does not necessarily reflect the CPU’s interrupt state on entry to the early assembly code handling the IRQ events. Typically, a CPU would always automatically disable interrupts hardware-wise when taking an IRQ, which may contradict the software-managed virtual state until both are eventually reconciled.\nThose rules of thumb should be kept in mind when adapting the epilogue routine to interrupt pipelining:\n  most often, such routine is supposed to be entered with (hard) interrupts off when called from the assembly code which handles kernel entry/exit transitions (e.g. arch/arm/kernel/entry-common.S). Therefore, this routine may have to reconcile the virtual interrupt state with such expectation, since according to the interrupt exit rules we discussed earlier, such state has to be originally enabled (i.e. the in-band stall bit is clear) for the epilogue code to run in the first place.\n  conversely, we must keep the hard interrupt state consistent upon return from the epilogue code with the one received on entry. Typically, hard interrupts must be disabled before leaving this code if we entered it that way.\n  calling schedule() should be done with IRQs enabled in the CPU, in order to minimize latency for the out-of-band stage (i.e. hard_irqs_disabled() should return false before the call).\n  generally speaking, while we may need the in-band stage to be stalled when the in-band kernel code expects this, we still want most of the epilogue code to run with hard interrupts enabled to shorten the interrupt latency for the out-of-band stage, where the autonomous core lives.\n   Reconciling the interrupt state in ARM64 epilogue\n #include \u003clinux/errno.h\u003e #include \u003clinux/kernel.h\u003e #include \u003clinux/signal.h\u003e +#include \u003clinux/irq_pipeline.h\u003e #include \u003clinux/personality.h\u003e #include \u003clinux/freezer.h\u003e #include \u003clinux/stddef.h\u003e @@ -915,24 +916,34 @@ static void do_signal(struct pt_regs *regs) asmlinkage void do_notify_resume(struct pt_regs *regs, unsigned long thread_flags) { +\tWARN_ON_ONCE(irq_pipeline_debug() \u0026\u0026 +\t(irqs_disabled() || running_oob())); + /* * The assembly code enters us with IRQs off, but it hasn't * informed the tracing code of that for efficiency reasons. * Update the trace code with the current status. */ -\ttrace_hardirqs_off(); +\tif (!irqs_pipelined()) +\ttrace_hardirqs_off(); do { +\tif (irqs_pipelined()) +\tlocal_irq_disable(); + /* Check valid user FS if needed */ addr_limit_user_check(); if (thread_flags \u0026 _TIF_NEED_RESCHED) { /* Unmask Debug and SError for the next task */ -\tlocal_daif_restore(DAIF_PROCCTX_NOIRQ); +\tlocal_daif_restore(irqs_pipelined() ? +\tDAIF_PROCCTX : DAIF_PROCCTX_NOIRQ); schedule(); } else { local_daif_restore(DAIF_PROCCTX); +\tif (irqs_pipelined()) +\tlocal_irq_enable(); if (thread_flags \u0026 _TIF_UPROBE) uprobe_notify_resume(regs); @@ -950,9 +961,17 @@ asmlinkage void do_notify_resume(struct pt_regs *regs, fpsimd_restore_current_state(); } +\t/* +\t* CAUTION: we may have restored the fpsimd state for +\t* current with no other opportunity to check for +\t* _TIF_FOREIGN_FPSTATE until we are back running on +\t* el0, so we must not take any interrupt until then, +\t* otherwise we may end up resuming with some OOB +\t* thread's fpsimd state. +\t*/ local_daif_mask(); thread_flags = READ_ONCE(current_thread_info()-\u003eflags); -\t} while (thread_flags \u0026 _TIF_WORK_MASK); +\t} while (inband_irq_pending() || (thread_flags \u0026 _TIF_WORK_MASK)); } Mapping descriptor-less per-CPU IRQs to pipelined IRQ numbers Some architecture ports may not assign interrupt descriptors (i.e. struct irq_desc) to per-CPU interrupts. Instead, those per-CPU events are immediately handled by arch-specific code, instead of being channeled through the common generic IRQ layer for delivery to their respective handler. With Dovetail, we do generally need all interrupts to have a valid interrupt descriptor, including per-CPU events, so that we can request them to be handled on the out-of-band stage using the generic IRQ API, like we would request any regular device interrupt.\nThe easiest way to achieve this mapping is to create a new synthetic interrupt domain for IPIs, which are in essence per-CPU events. Therefore, the flow handler for interrupts from this domain should be handle_synthetic_irq().\nPrior to kernel v5.10-rc6, all Dovetail ports (x86, ARM and arm64) required a synthetic interrupt domain for the purpose of mapping descriptor-less per-CPU interrupts. Since v5.10-rc6 and the introduction of a native mapping of each IPI to a common interrupt descriptor for ARM and arm64, only Dovetail/x86 still needs to implement a specific interrupt domain in order to map the APIC system interrupts.\nDealing with IPIs Although the pipeline does not directly use inter-processor interrupts internally, it provides a simple API to autonomous cores for implementing IPI-based messaging between CPUs. This feature requires the Dovetail port to implement a few bits of architecture-specific code. The arch-specific Dovetail implementation must provide support for two operations:\n  sending the additional IPI signals defined by the Dovetail API using the mechanism available from your hardware for inter-processor messaging, upon request from irq_send_oob_ipi().\n  dispatching deferred IPIs to their respective in-band handler upon request from the Dovetail core.\n  Prior to kernel v5.10-rc6 Until v5.10-rc6, the ARM and arm64 implementations would register IPIs from a synthetic IPI interrupt domain, from IRQ2048 (OOB_IPI_BASE) and on.\nThe arch-specific implementation of handle_IPI() in a Dovetail-enabled kernel should generate IRQs from this IPI domain each time an IPI event is received from the hardware, by calling generic_pipeline_irq().\n IPI domain for ARM\n static struct irq_domain *sipic_domain; static void sipic_irq_noop(struct irq_data *data) { } static unsigned int sipic_irq_noop_ret(struct irq_data *data) { return 0; } static struct irq_chip sipic_chip = { .name\t= \"SIPIC\", .irq_startup\t= sipic_irq_noop_ret, .irq_shutdown\t= sipic_irq_noop, .irq_enable\t= sipic_irq_noop, .irq_disable\t= sipic_irq_noop, .irq_ack\t= sipic_irq_noop, .irq_mask\t= sipic_irq_noop, .irq_unmask\t= sipic_irq_noop, .flags\t= IRQCHIP_PIPELINE_SAFE | IRQCHIP_SKIP_SET_WAKE, }; static int sipic_irq_map(struct irq_domain *d, unsigned int irq, irq_hw_number_t hwirq) { irq_set_percpu_devid(irq); irq_set_chip_and_handler(irq, \u0026sipic_chip, handle_synthetic_irq); return 0; } static struct irq_domain_ops sipic_domain_ops = { .map\t= sipic_irq_map, }; static void create_ipi_domain(void) { /* * Create an IRQ domain for mapping all IPIs (in-band and * out-of-band), with fixed sirq numbers starting from * OOB_IPI_BASE. The sirqs obtained can be injected into the * pipeline upon IPI receipt like other interrupts. */ sipic_domain = irq_domain_add_simple(NULL, NR_IPI + OOB_NR_IPI, OOB_IPI_BASE, \u0026sipic_domain_ops, NULL); } void __init arch_irq_pipeline_init(void) { #ifdef CONFIG_SMP create_ipi_domain(); #endif }  ЗаметкаInitializing the IPI domain should be done from the arch_irq_pipeline_init() handler, which Dovetail calls while setting up the interrupt pipelining machinery early at kernel boot.\n Since kernel v5.10-rc6 Since v5.10-rc6, the ARM and arm64 implementations assign a common interrupt descriptor to each IPI, therefore we need no synthetic interrupt domain for that purpose anymore.\nShort of IPI vectors? Multiplex! Your hardware may be limited with respect to the number of distinct IPI signals available from the interrupt controller. Typically, the ARM generic interrupt controller (aka GIC) available with the Cortex CPU series provides 16 distinct IPIs, half of which should be reserved to the firmware, which leaves only 8 IPIs available to the kernel. Since all of them are already in use for in-band work in the mainline implementation, we are short of available IPI vectors for adding the two additional interrupts we need. For this reason, the Dovetail ports for ARM and ARM64 have reshuffled the way IPI signaling is implemented.\nIn the upstream kernel implementation, a 1:1 mapping exists between the logical IPI numbers used by the kernel to refer to inter-processor messages, and the physical, so-called SGI numbers which stands for Software Generated Interrupts:\n   IPI Message Logical IPI number Physical SGI number     IPI_WAKEUP 0 0   IPI_TIMER 1 1   IPI_RESCHEDULE 2 2   IPI_CALL_FUNC 3 3   IPI_CPU_STOP 4 4   IPI_IRQ_WORK 5 5   IPI_COMPLETION 6 6   IPI_CPU_BACKTRACE 7 7    After the introduction of IPI multiplexing by Dovetail, all pre-existing in-band IPIs are now multiplexed over SGI0, which leaves seven SGIs available for adding out-of-band IPI messages, from which we only need two in the current implementation.\nPrior to kernel v5.10-rc6 The resulting mapping is as follows:\n   IPI Message Logical IPI number Physical SGI number Pipelined IRQ number     IPI_WAKEUP 0 0 2048   IPI_TIMER 1 0 2049   IPI_RESCHEDULE 2 0 2050   IPI_CALL_FUNC 3 0 2051   IPI_CPU_STOP 4 0 2052   IPI_IRQ_WORK 5 0 2053   IPI_COMPLETION 6 0 2054   IPI_CPU_BACKTRACE 7 0 2055   TIMER_OOB_IPI 8 1 2056   RESCHEDULE_OOB_IPI 9 2 2057    The implementation of the IPI multiplexing for ARM takes place in arch/arm/kernel/smp.c. The logic - as illustrated below - is fairly straightforward:\n  on the issuer side, if the IPI we need to send belongs to the in-band set (ipinr \u003c NR_IPI), log the pending signal into a per-CPU global bitmask (ipi_messages) then issue SGI0. Otherwise, issue either SGI1 or SGI2 for signaling the corresponding out-of-band IPIs directly.\n  upon receipt, if we received SGI0, iterate over the pending in-band IPIs by reading the per-CPU bitmask (ipi_messages) demultiplexing the logical IPI numbers as we go before pushing the corresponding IRQ event to the pipeline entry, see generic_pipeline_irq(). If SGI1 or SGI2 are received instead, the incoming event is remapped to either TIMER_OOB_IPI or RESCHEDULE_OOB_IPI before it is fed into the pipeline (i.e. IRQ2056 or IRQ2057).\n  static DEFINE_PER_CPU(unsigned long, ipi_messages); static inline void send_IPI_message(const struct cpumask *target, unsigned int ipinr) { unsigned int cpu, sgi; if (ipinr \u003c NR_IPI) { /* regular in-band IPI (multiplexed over SGI0). */ trace_ipi_raise_rcuidle(target, ipi_types[ipinr]); for_each_cpu(cpu, target) set_bit(ipinr, \u0026per_cpu(ipi_messages, cpu)); smp_mb(); sgi = 0; } else\t/* out-of-band IPI (SGI1-2). */ sgi = ipinr - NR_IPI + 1; __smp_cross_call(target, sgi); } static inline void handle_IPI_pipelined(int sgi, struct pt_regs *regs) { unsigned int ipinr, irq; unsigned long *pmsg; if (sgi) {\t/* SGI1-2 */ irq = sgi + NR_IPI - 1 + OOB_IPI_BASE; generic_pipeline_irq(irq, regs); return; } /* In-band IPI (0..NR_IPI - 1) multiplexed over SGI0. */ pmsg = raw_cpu_ptr(\u0026ipi_messages); while (*pmsg) { ipinr = ffs(*pmsg) - 1; clear_bit(ipinr, pmsg); irq = OOB_IPI_BASE + ipinr; generic_pipeline_irq(irq, regs); } } As illustrated in the example of in-band delivery glue code, the ARM ports distinguishes between device IRQs and IPIs based on the pipelined IRQ number, with anything in the range [OOB_IPI_BASE..OOB_IPI_BASE + 10] being dispatched as an IPI to the __handle_IPI() routine.\nSince out-of-band IPI messages are supposed to be exclusively handled by out-of-band handlers, __handle_IPI() is not required to handle them specifically.\nSince kernel v5.10-rc6 The resulting mapping is as follows:\n   IPI Message Logical IPI number Physical SGI number Pipelined IRQ number     IPI_WAKEUP 0 0 ipi_irq_base   IPI_TIMER 1 0 ipi_irq_base   IPI_RESCHEDULE 2 0 ipi_irq_base   IPI_CALL_FUNC 3 0 ipi_irq_base   IPI_CPU_STOP 4 0 ipi_irq_base   IPI_IRQ_WORK 5 0 ipi_irq_base   IPI_COMPLETION 6 0 ipi_irq_base   IPI_CPU_BACKTRACE 7 0 ipi_irq_base   TIMER_OOB_IPI x 1 ipi_irq_base + 1   RESCHEDULE_OOB_IPI x 2 ipi_irq_base + 2    The implementation of the IPI multiplexing for ARM takes place in arch/arm/kernel/smp.c:\n  ipi_irq_base stands for the first logical interrupt number assigned to the series of SGIs. All in-band IPIs are multiplexed over SGI0, therefore all of them are signaled by IRQ #ipi_irq_base.\n  on the issuer side, smp_cross_call() deals with in-band IPIs exclusively, logging them into a per-CPU global bitmask (ipi_messages) before issuing SGI0. irq_send_oob_ipi() sends out-of-band IPIs by triggering SGI1 (TIMER_OOB_IPI) or SGI2 (RESCHEDULE_OOB_IPI).\n  upon receipt of SGI0, the in-band IPI handler (ipi_handler) iterates over the pending in-band IPIs by reading the per-CPU bitmask (ipi_messages) pushing each demultiplexed IRQ event to the pipeline entry, see generic_pipeline_irq(). If SGI1 or SGI2 are received instead, the incoming event is remapped to either TIMER_OOB_IPI or RESCHEDULE_OOB_IPI before it is fed into the pipeline.\n  ЗаметкаThere is no generic out-of-band IPI handler: since each IPI has a dedicated interrupt descriptor, the out-of-band code may request it directly, installing its own handler by a call to __request_percpu_irq().\n static DEFINE_PER_CPU(unsigned long, ipi_messages); static void smp_cross_call(const struct cpumask *target, unsigned int ipinr) { unsigned int cpu; /* regular in-band IPI (multiplexed over SGI0). */ for_each_cpu(cpu, target) set_bit(ipinr, \u0026per_cpu(ipi_messages, cpu)); wmb(); __smp_cross_call(target, 0); } static irqreturn_t ipi_handler(int irq, void *data) { unsigned long *pmsg; unsigned int ipinr; /* * Decode in-band IPIs (0..MAX_IPI - 1) multiplexed over * SGI0. Out-of-band IPIs (SGI1, SGI2) have their own * individual handler. */ pmsg = raw_cpu_ptr(\u0026ipi_messages); while (*pmsg) { ipinr = ffs(*pmsg) - 1; clear_bit(ipinr, pmsg); __this_cpu_inc(ipi_counts[ipinr]); do_handle_IPI(ipinr); } return IRQ_HANDLED; }  Последнее изменение: Wed, 27 Jun 2018 17:07:51 +0200 content/dovetail/porting/arch.md f22892e ",
    "description": "",
    "tags": null,
    "title": "Биты, зависящие от архитектуры",
    "uri": "/ru/dovetail/porting/arch/"
  },
  {
    "content": "Proxy tick device The proxy tick device is a synthetic clock event device for handing over the control of the hardware tick device to a high-precision, out-of-band timing logic, which cannot be delayed by the in-band kernel code. With this proxy in place, any out-of-band code can gain control over the timer hardware for carrying out its own timing duties. In the same move, it is required to honor the timing requests received from the in-band timer layer (i.e. hrtimers) since the latter won’t be able to program timer events directly into the hardware while the proxy is active.\nIn other words, the proxy tick device shares the functionality of the actual device between the in-band and out-of-band contexts, with only the latter actually programming the hardware.\nAdapting clock chip devices for proxying The proxy tick device borrows a real clock chip device from the in-band kernel, controlling it under the hood while substituting for the current tick device. Clock chips which may be controlled by the proxy tick device need their drivers to be specifically adapted for such use, as follows:\n  clockevents_handle_event() must be substituted to any open-coded invocation of the event handler in the interrupt handler.\n  struct clock_event_device::irq must be properly set to the actual IRQ number signaling an event from this device.\n  struct clock_event_device::features must include CLOCK_EVT_FEAT_PIPELINE.\n  __IRQF_TIMER must be set for the action handler of the timer device interrupt.\n   Adapting the ARM architected timer driver to out-of-band timing\n --- a/drivers/clocksource/arm_arch_timer.c +++ b/drivers/clocksource/arm_arch_timer.c @@ -585,7 +585,7 @@ static __always_inline irqreturn_t timer_handler(const int access, if (ctrl \u0026 ARCH_TIMER_CTRL_IT_STAT) { ctrl |= ARCH_TIMER_CTRL_IT_MASK; arch_timer_reg_write(access, ARCH_TIMER_REG_CTRL, ctrl, evt); -\tevt-\u003eevent_handler(evt); +\tclockevents_handle_event(evt); return IRQ_HANDLED; } @@ -704,7 +704,7 @@ static int arch_timer_set_next_event_phys_mem(unsigned long evt, static void __arch_timer_setup(unsigned type, struct clock_event_device *clk) { -\tclk-\u003efeatures = CLOCK_EVT_FEAT_ONESHOT; +\tclk-\u003efeatures = CLOCK_EVT_FEAT_ONESHOT | CLOCK_EVT_FEAT_PIPELINE; if (type == ARCH_TIMER_TYPE_CP15) { if (arch_timer_c3stop)  ЗаметкаOnly oneshot-capable clock event devices can be shared via the proxy tick device.\n  Adapting the ARM Global timer driver to out-of-band timing\n --- a/drivers/clocksource/arm_global_timer.c +++ b/drivers/clocksource/arm_global_timer.c @@ -156,11 +156,11 @@ static irqreturn_t gt_clockevent_interrupt(int irq, void *dev_id) *\tthe Global Timer flag _after_ having incremented *\tthe Comparator register\tvalue to a higher value. */ -\tif (clockevent_state_oneshot(evt)) +\tif (clockevent_is_oob(evt) || clockevent_state_oneshot(evt)) gt_compare_set(ULONG_MAX, 0); writel_relaxed(GT_INT_STATUS_EVENT_FLAG, gt_base + GT_INT_STATUS); -\tevt-\u003eevent_handler(evt); +\tclockevents_handle_event(evt); return IRQ_HANDLED; } @@ -171,7 +171,7 @@ static int gt_starting_cpu(unsigned int cpu) clk-\u003ename = \"arm_global_timer\"; clk-\u003efeatures = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT | -\tCLOCK_EVT_FEAT_PERCPU; +\tCLOCK_EVT_FEAT_PERCPU | CLOCK_EVT_FEAT_PIPELINE; clk-\u003eset_state_shutdown = gt_clockevent_shutdown; clk-\u003eset_state_periodic = gt_clockevent_set_periodic; clk-\u003eset_state_oneshot = gt_clockevent_shutdown; @@ -195,11 +195,6 @@ static int gt_dying_cpu(unsigned int cpu) return 0; } @@ -302,8 +307,8 @@ static int __init global_timer_of_register(struct device_node *np) goto out_clk; } -\terr = request_percpu_irq(gt_ppi, gt_clockevent_interrupt, -\t\"gt\", gt_evt); +\terr = __request_percpu_irq(gt_ppi, gt_clockevent_interrupt, +\tIRQF_TIMER, \"gt\", gt_evt); if (err) { pr_warn(\"global-timer: can't register interrupt %d (%d)\\n\", gt_ppi, err); This is another example of adapting an existing clock chip driver for serving out-of-band timing requests, with a subtle change in the way we should test for the current state of the clock device in the interrupt handler:\n  A real/original device (such as the ARM global timer in this example) is switched to reserved mode when the proxy tick driver hands it over to the autonomous core, which is similar to the detached mode in effect. Therefore, the ARM global timer state is always reserved from the standpoint of the kernel when proxied, never oneshot. For this reason, clockevent_state_oneshot() would always lead to false in this case.\n  However, since a real device controlled by the proxy for receiving out-of-band events has to be driven in one-shot mode under the hood, testing for clockevent_state_oob() in addition to clockevent_state_oneshot() guarantees that we do take the branch, setting the comparator register to ULONG_MAX when proxied too.\n  ВниманиеFailing to fix up the way we test for the clock device state would certainly lead to an interrupt storm with any ARM global timer suffering erratum 740657, quickly locking up the board.\n Theory of operations Calling tick_install_proxy() registers an instance of the proxy tick device on each CPU mentioned in the cpumask it receives. This routine is also passed the address of a routine which should setup the given struct clock_proxy_device descriptor for the current CPU. This routine is called indirectly by tick_install_proxy(), for each CPU marked in cpumask.\n Initializing the proxy descriptor\n tick_install_proxy() prepares the new proxy device for the current CPU, pre-initializing it with settings compatible with the real device’s it interposes on, then calls the setup_proxy() routine. The descriptor is defined as follows:\nstruct clock_proxy_device { struct clock_event_device proxy_device; struct clock_event_device *real_device; void (*handle_oob_event)(struct clock_event_device *dev); /* Internal data - don't depend on this. */ void (*__setup_handler)(struct clock_proxy_device *dev); void (*__original_handler)(struct clock_event_device *dev); }; The user setup call must at least set the .handle_oob_event() handler: this is the address of the routine which should be called each time an out-of-band tick is received from the underlying timer hardware the proxy controls. This is the only information required from the setup handler, the rest may be inherited from the pre-set data if the user does not need any particular setting.\nIf the user code proxying the tick device prefers dealing with nanoseconds instead of clock ticks directly, CLOCK_EVT_FEAT_KTIME should be added to proxy_device.features, along with a valid proxy_device.set_next_ktime() handler and proper min/max delta values.\n A setup_proxy() routine preparing a proxy device\n static DEFINE_PER_CPU(struct clock_proxy_device, tick_device); static void oob_event_handler(struct clock_event_device *dev) { /* * We are running on the out-of-band stage, in NMI-like mode. * Schedule a tick on the proxy device to satisfy the * corresponding timing request asap. */ tick_notify_proxy(); } static void setup_proxy(struct clock_proxy_device *dev) { * Create a proxy which acts as a transparent device, simply * relaying the timing requests to the in-band code, without * any additional out-of-band processing. */ dev-\u003ehandle_oob_event = oob_event_handler; }  СоветThe proxy_device.set_next_event() or proxy_device.set_next_ktime() members can be set with the address of a handler which receives timer requests from the in-band kernel. This handler is normally implemented by the autonomous core which takes control over the timer hardware via the proxy device. Whenever that core determines that a tick is due for an outstanding request received from such handler, it should call tick_notify_proxy() to signal the event to the main kernel.\n Once the user-supplied setup_proxy() routine returns, the following events happen in sequence:\n  the proxy device is registered on the clock event framework.\n  the real device is detached from the clockevent framework, switched to the CLOCK_EVT_STATE_RESERVED state, which makes it non-eligible for any regular operation from the framework. However, its hardware is left in a functional state. In the same move, the proxy device is picked as the new tick device by the framework. From that point, requests to the proxy device may be indirectly channeled to the real device via the proxy when operations on the hardware should be carried out.\n  the proxy device now controls the real device under the hood to carry out timing requests from the in-band kernel. When the hrtimer layer from the in-band kernel wants to program the next shot of the current tick device, it invokes the set_next_event() handler of the proxy device, which was defined by the user (which defaults to the real device’s set_next_event() handler). If the autonomous core implements its own timer management, this handler should be scheduling in-band ticks at the requested time based on such scheme.\n  the timer interrupt triggered by the real device is switched to out-of-band handling. As a result, handle_oob_event() receives tick events sent by the real device hardware directly from the out-of-band stage of the interrupt pipeline. This ensures high-precision timing, which the in-band stage cannot delay via interrupt masking. From that point, the out-of-band code can carry out its own timing duties, in addition to honoring the in-band kernel requests for timing.\n  Step 3. involves emulating ticks scheduled by the in-band kernel by a software logic controlled by some out-of-band timer management, paced by the real ticks received as described in step 4. When this logic decides than the next in-band tick is due, it should call tick_notify_proxy() to trigger the corresponding event for the in-band kernel, which would honor the pending (hr)timer request.\n Under the hood\n [in-band timing request] proxy_dev-\u003eset_next_event(proxy_dev) oob_program_event(proxy_dev) real_dev-\u003eset_next_event(real_dev) ... \u003ctick event\u003e inband_event_handler() [out-of-band stage] clockevents_handle_event(real_dev) handle_oob_event(proxy_dev) ...(in-band tick emulation)... tick_notify_proxy() ... proxy_irq_handler(proxy_dev) [in-band stage] clockevents_handle_event(proxy_dev) inband_event_handler(proxy_dev)  Последнее изменение: Wed, 27 Jun 2018 17:15:23 +0200 ",
    "description": "",
    "tags": null,
    "title": "Tick devices",
    "uri": "/ru/dovetail/porting/timer/"
  },
  {
    "content": "Your autonomous core most likely needs a fast access to the current clock source from the out-of-band context, for reading precise timestamps which are in sync with the kernel’s idea of time. The best way to achieve this is by enabling the fast clock_gettime(3) helper in the vDSO support for the target CPU architecture. At least, you may want user-space tasks controlled by the core to have access to the POSIX-defined CLOCK_MONOTONIC and CLOCK_REALTIME clocks from the out-of-band context, using a vDSO call, with no execution and response time penalty involved in invoking an [in-band syscall] (/ru/dovetail/altsched/#inband-switch).\nReading timestamps via the vDSO in a nutshell Basically, A vDSO-based clock_gettime(3) implementation wants to read from non-privileged CPU mode the same monotonic hardware counter which is currently used by the kernel for timekeeping, before converting the count to nanoseconds. In other words, this implementation should mirror the kernel clocksource behavior. However, it should do so relying exclusively on resources which may be accessed from user mode, since the vDSO code segment containing this helper is merely a kernel-defined extension of the application code running in non-privileged mode. In order to perform such conversion, the vDSO code also needs additional timekeeping information maintained by the kernel, which it usually gets from a small data segment the kernel maps into every application as part of the vDSO support (see the various update_vsyscall() implementations for more on this).\nThere are two common ways of reading the hardware counter used for timekeeping by the kernel from a non-privileged environement:\n  either by reading some non-privileged on-chip register (e.g. powerpc’s timebase register, or x86’s TSC register).\n  or by reading a memory-mapped counter, from a memory mapping the calling application has access to.\n  The ARM situation For several CPU architectures Linux supports, reading the CLOCK_MONOTONIC and CLOCK_REALTIME clocks is already possible via vDSO calls, including from tasks running out-of-band without incurring any execution stage switch.\nFor ARM, the situation is clumsy: the mainline kernel implementation supports reading timestamps directly from the vDSO only for the so-called architected timer the armv8 specification requires from compliant CPUs. With CPUs following an earlier specification, a truckload of different hardware chips may be used for that purpose instead, which the vDSO implementation does not provide any support for. Sometimes, the architected timer is present but not usable for timekeeping duties because of firmware issues. In these cases, a plain in-band system call would be issued whenever the vDSO-based clock_gettime(3) is called from an application, which would be a showstopper for keeping the response time short and bounded.\nThe generic vDSO and USER_MMIO clock sources If your target kernel supports the generic vDSO implementation (CONFIG_HAVE_GENERIC_VDSO), Dovetail already provides the core support for accessing MMIO-based clock sources from the vDSO, which is essentially part of the application context:\n  firstly, it extends the clocksource_mmio semantics with the MMIO-accessed clock sources mapped to user space aka struct clocksource_user_mmio, implemented in drivers/clocksource/mmio.c, which we call USER_MMIO for short in this document. In essence, a USER_MMIO clock source is a MMIO-based clock source which any application may map into its own address space, so that it can read the hardware counter directly. Specifically, the MMIO space covering the hardware counter register(s) is mapped into the caller’s address space.\n  Secondly, Dovetail extends the generic vDSO implementation in lib/vdso.c and kernel/time/vsyscall.c in order to make USER_MMIO clock sources visible to applications, in addition to the architected timer if present.\n  Finally, Dovetail converts some MMIO clock sources to USER_MMIO clock sources, such as the OMAP2 general-purpose timer, the ARM global timer counter and the DesignWare APB timer. More conversions will come over time, as Dovetail is ported to a broader range of hardware.\n  ЗаметкаThe mapping operation happens once in a process lifetime, during the very first call to clock_gettime(3) issued by the application. Since this involves running in-band code for updating the caller’s address space, this particular call gives absolutely no response time guarantee. So it’s good practice to force an initial dummy call to clock_gettime(3) from the library code which initializes the interface between applications and your autonomous core (i.e. some user-space library which implements the out-of-band system calls wrappers to send requests to this core). For EVL, this is done in libevl.\n Making a MMIO clock source accessible from the vDSO If you need to convert an existing MMIO clock source to a user-mappable one visible from the generic vDSO, you can follow this three-step process:\n  in the Kconfig stanza enabling the clock source, select [CONFIG_]GENERIC_CLOCKSOURCE_VDSO in the dependency list to compile the required support in the generic vDSO code, which in turn selects the USER_MMIO support it depends on.\n  in the clock source implementation, convert the struct clocksource descriptor to a struct clocksource_user_mmio descriptor. The original struct clocksource object is now available as a member of the struct clocksource_user_mmio descriptor, so you may have to move the original initializers accordingly. You also need to fix up the clock source’s .read() handler, changing it to one of the helpers clocksource_user_mmio knows about. Do not use any other helper outside of the following set, or you would receive -EINVAL from clocksource_user_mmio_init():\n  clocksource_mmio_readl_up(), for reading a 32bit count-up register (i.e. reg_higher is NULL in clocksource_mmio_regs as described below).\n  clocksource_mmio_readl_down(), for reading a 32bit count-down register.\n  clocksource_mmio_readw_up(), for reading a 16bit count-up register.\n  clocksource_mmio_readw_down(), for reading a 16bit count-down register.\n  clocksource_dual_mmio_readl_up(), for reading a count-up counter composed of two 32bit registers (i.e. both reg_lower and reg_higher must be valid in clocksource_mmio_regs as described below).\n  clocksource_dual_mmio_readl_down(), for reading a count-down counter composed of two 32bit registers.\n    ВниманиеOnly continuous clock sources can be converted to clocksource_user_mmio, otherwise the registration fails with -EINVAL in clocksource_user_mmio_init(). Therefore, only clock sources originally bearing the CLOCK_SOURCE_IS_CONTINUOUS flag can be converted.\n   eventually, substitute the call to clocksource_register_hz() by a call to clocksource_user_mmio_init() instead. This function takes the following arguments:\n  the USER_MMIO descriptor address\n  the address of a clocksource_mmio_regs structure which defines the method and parameters for reading the hardware counter. Such counter can be represented by up to two 32bit MMIO registers, making a 64bit value. A lower precision is acceptable too, the vDSO code deals with wrapping as needed. However, the higher the precision, the better the accuracy for applications. The clocksource_mmio_regs structure should be filled with the following information:\n - _reg\\_lower_ is the **virtual** address of the counter's low 32bit register. This address was most likely obtained from `ioremap()` in the original clock source driver code; it cannot be NULL.    bits_lower is a bitmask defining the significant bits to read from the low register, starting from the low order bit. For instance, if the first 31 bits only are significant, 0x7fffffff should be passed.\n  reg_higher is the virtual address of the counter’s high register. This address can be NULL if the hardware counter is only 32bit wide or less, in which case bits_higher is ignored too.\n  bits_higher is a bitmask defining the significant bits to read from the high register.\n  revmap is a reverse mapping helper, for resolving the physical address of the low and high registers mentioned above, based on the virtual address passed in reg_lower and reg_higher. If revmap is NULL, clocksource_user_mmio_init() tries to figure this out by resolving the address of the containing memory frame via a call to find_vma(), which is usually fine. If this resolution should be done in a different way, you should specify your own handler in revmap, which receives the virtual address to resolve, and should return the corresponding physical address, or zero upon failure.\n    the hardware clock rate that was originally passed to clocksource_register_hz().\n    Example: converting the OMAP2 GP-timer The Beaglebone Black is an AM335X processor equipped with a Cortex A8 CPU, therefore no ARM architected timer is available. Instead, Linux runs one of the available general purpose timers on this platform for timekeeping purpose. The clock source driver for such devices is implemented in arch/arm/mach-omap2/timer.c. The patch below illustrates the changes Dovetail introduces to convert this clock source to a user-mappable one, which the ARM vDSO implementation can use.\n--- a/arch/arm/mach-omap2/Kconfig +++ b/arch/arm/mach-omap2/Kconfig @@ -96,6 +96,7 @@ config ARCH_OMAP2PLUS select ARCH_HAS_HOLES_MEMORYMODEL select ARCH_OMAP select CLKSRC_MMIO +\tselect GENERIC_CLOCKSOURCE_VDSO select GENERIC_IRQ_CHIP select GPIOLIB select MACH_OMAP_GENERIC diff --git a/arch/arm/mach-omap2/timer.c b/arch/arm/mach-omap2/timer.c index 69c3a6d94933..bc7d177759c3 100644 --- a/arch/arm/mach-omap2/timer.c +++ b/arch/arm/mach-omap2/timer.c @@ -413,17 +413,14 @@ static bool use_gptimer_clksrc __initdata; /* * clocksource */ -static u64 clocksource_read_cycles(struct clocksource *cs) -{ -\treturn (u64)__omap_dm_timer_read_counter(\u0026clksrc, -\tOMAP_TIMER_NONPOSTED); -} -static struct clocksource clocksource_gpt = { -\t.rating\t= 300, -\t.read\t= clocksource_read_cycles, -\t.mask\t= CLOCKSOURCE_MASK(32), -\t.flags\t= CLOCK_SOURCE_IS_CONTINUOUS, +static struct clocksource_user_mmio clocksource_gpt = { +\t.mmio.clksrc = { +\t.rating\t= 300, +\t.read\t= clocksource_mmio_readl_up, +\t.mask\t= CLOCKSOURCE_MASK(32), +\t.flags\t= CLOCK_SOURCE_IS_CONTINUOUS, +\t}, }; static u64 notrace dmtimer_read_sched_clock(void) @@ -505,21 +502,22 @@ static void __init omap2_gptimer_clocksource_init(int gptimer_id, const char *fck_source, const char *property) { +\tstruct clocksource_mmio_regs mmr; int res; clksrc.id = gptimer_id; clksrc.errata = omap_dm_timer_get_errata(); res = omap_dm_timer_init_one(\u0026clksrc, fck_source, property, -\t\u0026clocksource_gpt.name, +\t\u0026clocksource_gpt.mmio.clksrc.name, OMAP_TIMER_NONPOSTED); if (soc_is_am43xx()) { -\tclocksource_gpt.suspend = omap2_gptimer_clksrc_suspend; -\tclocksource_gpt.resume = omap2_gptimer_clksrc_resume; +\tclocksource_gpt.mmio.clksrc.suspend = omap2_gptimer_clksrc_suspend; +\tclocksource_gpt.mmio.clksrc.resume = omap2_gptimer_clksrc_resume; clocksource_gpt_hwmod = -\tomap_hwmod_lookup(clocksource_gpt.name); +\tomap_hwmod_lookup(clocksource_gpt.mmio.clksrc.name); } BUG_ON(res); @@ -529,12 +527,18 @@ static void __init omap2_gptimer_clocksource_init(int gptimer_id, OMAP_TIMER_NONPOSTED); sched_clock_register(dmtimer_read_sched_clock, 32, clksrc.rate); -\tif (clocksource_register_hz(\u0026clocksource_gpt, clksrc.rate)) +\tmmr.reg_lower = clksrc.func_base + (OMAP_TIMER_COUNTER_REG \u0026 0xff); +\tmmr.bits_lower = 32; +\tmmr.reg_upper = 0; +\tmmr.bits_upper = 0; +\tmmr.revmap = NULL; + +\tif (clocksource_user_mmio_init(\u0026clocksource_gpt, \u0026mmr, clksrc.rate)) pr_err(\"Could not register clocksource %s\\n\", -\tclocksource_gpt.name); +\tclocksource_gpt.mmio.clksrc.name); else pr_info(\"OMAP clocksource: %s at %lu Hz\\n\", -\tclocksource_gpt.name, clksrc.rate); +\tclocksource_gpt.mmio.clksrc.name, clksrc.rate); }  СоветIn some rare cases, converting the available clocksource(s) so that we can read them directly from the vDSO might not be an option, typically because reading them would require supervisor privileges in the CPU, which the vDSO context excludes by definition. For these almost desperate situations, there is still the option for your companion core to intercept system calls to clock_gettime(3) from the out-of-band handler, handling them directly from that spot. This would be slower compared to a direct readout from the vDSO, but the core would manage to get timestamps for CLOCK_MONOTONIC and CLOCK_REALTIME clocks at least without involving the in-band stage. EVL solves a limitation with clock sources on legacy x86 hardware this way.\n  Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Reading clock sources",
    "uri": "/ru/dovetail/porting/clocksource/"
  },
  {
    "content": " Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Syscall path",
    "uri": "/ru/dovetail/porting/syscall/"
  },
  {
    "content": "Unless you are lucky enough to have an ICE for debugging hard issues involving out-of-band contexts, you might have to resort to basic printk-style debugging over a serial line. Although the printk() machinery can be used from out-of-band context when Dovetail is enabled, the output is deferred until the in-band stage gets back in control, which means that:\n  you can’t reliably trace out-of-band code on the spot, deferred output issued from an out-of-band context, or from a section of code running with interrupts disabled in the CPU may appear after subsequent in-band messages under some circumstances, due to a buffering effect.\n  if the debug traces are sent at high pace (e.g. from an out-of-band IRQ handler every few hundreds of microseconds), the machine is likely to come to a stall due to the massive output the heavy printk() machinery would have to handle, leading to an apparent lockup.\n  The only sane option for printk-like debugging in demanding out-of-band context is using the raw_printk() routine for issuing raw debug messages to a serial console, so that you may get some sensible feedback for understanding what is going on with the execution flow. This feature should be enabled by turning on CONFIG_RAW_PRINTK, otherwise all output sent to raw_printk() is discarded.\nBecause a stock serial console driver won’t be usable from out-of-band context, enabling raw printk support requires adapting the serial console driver your platform is using, by adding a raw write handler to the console description. Just like the write() handler, the write_raw() output handler receives a console pointer, the character string to output and its length as parameters. This handler should send the characters to the UART as quickly as possible, with little to no preparation.\nAll output formatted by the generic raw_printk() routine is passed to the raw write handler of the current serial console driver if present. Calls to the raw output handler are serialized in raw_printk() by holding a hard spinlock, which means that interrupts are disabled in the CPU when running the handler.\nA raw write handler is normally derived from the regular write handler for the same serial console device, skipping any in-band locking construct, only waiting for the bare minimum time for the output to drain in the UART since we want to keep interrupt latency low.\nВниманиеYou cannot expect mixed output sent via printk() then raw_printk() to appear in the same sequence as their respective calls: normal printk() output may be deferred for an undefined amount of time until some console driver sends it to the terminal device, which may involve a task rescheduling. On the other hand, raw_printk() immediately writes the output to the hardware device, bypassing any buffering from printk(). So the output from a sequence of printk() followed by raw_printk() may appear in the opposite order on the terminal device. The converse never happen though.\n  Adding RAW_PRINTK support to the AMBA PL011 serial driver\n --- a/drivers/tty/serial/amba-pl011.c +++ b/drivers/tty/serial/amba-pl011.c @@ -2206,6 +2206,40 @@ static void pl011_console_putchar(struct uart_port *port, int ch) pl011_write(ch, uap, REG_DR); } +#ifdef CONFIG_RAW_PRINTK + +/* + * The uart clk stays on all along in the current implementation, + * despite what pl011_console_write() suggests, so for the time being, + * just emit the characters assuming the chip is clocked. If the clock + * ends up being turned off after writing, we may need to clk_enable() + * it at console setup, relying on the non-zero enable_count for + * keeping pl011_console_write() from disabling it. + */ +static void +pl011_console_write_raw(struct console *co, const char *s, unsigned int count) +{ +\tstruct uart_amba_port *uap = amba_ports[co-\u003eindex]; +\tunsigned int old_cr, new_cr, status; + +\told_cr = readw(uap-\u003eport.membase + UART011_CR); +\tnew_cr = old_cr \u0026 ~UART011_CR_CTSEN; +\tnew_cr |= UART01x_CR_UARTEN | UART011_CR_TXE; +\twritew(new_cr, uap-\u003eport.membase + UART011_CR); + +\twhile (count-- \u003e 0) { +\tif (*s == '\\n') +\tpl011_console_putchar(\u0026uap-\u003eport, '\\r'); +\tpl011_console_putchar(\u0026uap-\u003eport, *s++); +\t} +\tdo +\tstatus = readw(uap-\u003eport.membase + UART01x_FR); +\twhile (status \u0026 UART01x_FR_BUSY); +\twritew(old_cr, uap-\u003eport.membase + UART011_CR); +} + +#endif /* !CONFIG_RAW_PRINTK */ + static void pl011_console_write(struct console *co, const char *s, unsigned int count) { @@ -2406,6 +2440,9 @@ static struct console amba_console = { .device\t= uart_console_device, .setup\t= pl011_console_setup, .match\t= pl011_console_match, +#ifdef CONFIG_RAW_PRINTK +\t.write_raw\t= pl011_console_write_raw, +#endif .flags\t= CON_PRINTBUFFER | CON_ANYTIME, .index\t= -1, .data\t= \u0026amba_reg, ARM-specific raw console driver The vanilla ARM kernel port already provides an UART-based raw output routine called printascii() when CONFIG_DEBUG_LL is enabled, provided the right debug UART channel is defined too (CONFIG_DEBUG_UART_xx).\nWhen CONFIG_RAW_PRINTK and CONFIG_DEBUG_LL are both defined in the kernel configuration, the ARM implementation of Dovetail automatically registers a special console device for emitting debug output (see arch/arm/kernel/raw_printk.c), which redirects calls to its raw write handler by raw_printk() to printascii(). In other words, if CONFIG_DEBUG_LL already provides you with a functional debug output channel, you don’t need the active serial console driver to implement a raw write handler for enabling raw_printk(), the raw console device should handle raw_printk() requests just fine.\nВниманиеEnabling CONFIG_DEBUG_LL with a wrong UART debug channel is a common cause of lockup at boot. You do want to make sure the proper CONFIG_DEBUG_UART_xx symbol matching your hardware is selected along with CONFIG_DEBUG_LL.\n  Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Raw printk support",
    "uri": "/ru/dovetail/porting/rawprintk/"
  },
  {
    "content": "printk() support printk() may be called by out-of-band code safely, without encurring extra latency. The output is conveyed like NMI-originated output, which involves some delay until the in-band code resumes, and the console driver(s) can handle it.\nTracing Tracepoints can be traversed by out-of-band code safely. Dynamic tracing is available to a kernel running the pipelined interrupt model too.\n Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Misc",
    "uri": "/ru/dovetail/porting/misc/"
  },
  {
    "content": "You can obtain the current API revision of libevl either at compilation time using the value of the __EVL__ macro defined in the \u003cevl/evl.h\u003e main header file, or dynamically by calling evl_get_version().\nrev. 18 (libevl r26) Introduces the socket interface:\n  oob_recvmsg() to receive a message in out-of-band mode.\n  oob_sendmsg() to send a message in out-of-band mode.\n  The regular socket(2) call as extended by ABI 26 is capable of creating oob-capable sockets when receiving the SOCK_OOB type flag, so there is no EVL-specific call for this operation.\nrev. 17 (libevl r17) Enables HM support for threads. Since ABI 23, the core is able to channel T_WOSS, T_WOLI and T_WOSX error notifications (SIGDEBUG_xxx) through the thread observable component if present. Introduce the T_HMSIG and T_HMOBS mode bits for configuring the HM notification source(s) of a thread with evl_set_thread_mode().\nSIGDEBUG_xxx codes are renamed to EVL_HMDIAG_xxx diag codes, so that we have a single nomenclature for these errors regardless of whether threads are notified via SIGDEBUG or their observable component.\nrev. 16 (libevl r17) Introduces the API changes for supporting the new Observable element:\n  adds evl_subscribe() and evl_unsubscribe() to the thread API.\n  adds the evl_create_observable(), evl_update_observable() and evl_read_observable() services for the new Observable API.\n  allows to pass an opaque data to evl_add_pollfd() and evl_mode_pollfd(), which is returned into the struct evl_poll_event descriptor.\n  rev. 15 (libevl r16) Adds evl_set_thread_mode() and evl_clear_thread_mode().\nrev. 14 Adds evl_unblock_thread() and evl_demote_thread().\nrev. 13 Adds evl_yield().\nrev. 12 (libevl r15) Element visibility is introduced, as a result:\n  Most element classes provides a new long-form evl_create_*() call, in order to receive creation flags. Currently, the visibility attribute of elements is the only flag supported (see EVL_CLONE_PRIVATE, EVL_CLONE_PUBLIC). The additional creation calls are evl_create_event(), evl_create_flags(), evl_create_mutex(), evl_create_proxy(), evl_create_sem() and evl_create_xbuf(). Likewise, the new evl_attach_thread() and evl_detach_thread() calls receive attachment and detachment flags for threads. evl_attach_self() is now equivalent to attaching a private thread by default, unless the thread name says otherwise. evl_detach_self() is unchanged.\n  All evl_new_*() calls become shorthands to their respective evl_create_*() counterparts, picking reasonable default creation parameters for the new element, including private visibility (unless overriden by the leading slash rule explained in this document).\n  All long-form evl_new_*_any() calls have been removed from the API. Applications should use the corresponding evl_create_*() call instead.\n  evl_new_proxy() creates a proxy element with no write granularity by default, which caused this this parameter to be dropped from the call signature. Applications should use evl_create_proxy() to specify a non-default granularity.\n  evl_new_xbuf() creates a cross-buffer element with identically sized input and output buffers by default, which caused one of the two size parameters to be dropped from the call signature. Applications should use evl_create_xbuf() to specify distinct sizes for input and output.\n  All former long-form static initializers EVL_*_ANY_INITIALIZER() have been renamed EVL_*_INITIALIZER(), dropping the former short-form (if any). For instance, EVL_MUTEX_ANY_INITIALIZER() has been renamed EVL_MUTEX_INITIALIZER(), with an additional parameter for mentioning both the lock type and visibility attribute.\n  Selecting the lock type of a mutex is now done using the evl_create_mutex() call, ORing either EVL_MUTEX_NORMAL or EVL_MUTEX_RECURSIVE into the creation flags. This method replaces the type argument to the former evl_new_mutex_any() call.\n   rev. 11 (libevl r14) For naming consistency, evl_sched_control() was renamed evl_control_sched().\n Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "API revisions",
    "uri": "/ru/core/user-api/api-revs/"
  },
  {
    "content": "Generic Fundamentally preemption-safe contexts Over a few contexts, we may traverse code using unprotected, preemption-sensitive accessors such as percpu() without disabling preemption specifically, because either one condition is true;\n  if preempt_count() bears either of the PIPELINE_MASK or STAGE_MASK bits, which turns preemption off, therefore CPU migration cannot happen (debug_smp_processor_id() and preempt checks in percpu accessors would detect such context properly too).\n  if we are running over the context of the in-band stage’s event log syncer (sync_current_stage()) playing a deferred interrupt, in which case the virtual interrupt disable bit is set, so no CPU migration may occur either.\n  For instance, the following contexts qualify:\n  clockevents_handle_event(), which should either be called from the oob stage - therefore STAGE_MASK is set - when the [proxy tick device is active] (/ru/dovetail/porting/timer/#proxy-tick-logic) on the CPU, and/or from the in-band stage playing a timer interrupt event from the corresponding device.\n  any IRQ flow handler from kernel/irq/chip.c. When called from generic_pipeline_irq() for pushing an external event to the pipeline, on_pipeline_entry() is true, which indicates that PIPELINE_MASK is set. When called for playing a deferred interrupt on the in-band stage, the virtual interrupt disable bit is set.\n  Checking for out-of-band interrupt property The IRQF_OOB action flag should not be used for testing whether an interrupt is out-of-band, because out-of-band handling may be turned on/off dynamically on an IRQ descriptor using irq_switch_oob(), which would not translate to IRQF_OOB being set/cleared for the attached action handlers.\nirq_is_oob() is the right way to check for out-of-band handling.\nstop_machine() hard disables interrupts The stop_machine() service guarantees that all online CPUs are spinning non-preemptible in a known code location before a subset of them may safely run a stop-context function. This service is typically useful for live patching the kernel code, or changing global memory mappings, so that no activity could run in parallel until the system has returned to a stable state after all stop-context operations have completed.\nWhen interrupt pipelining is enabled, Dovetail provides the same guarantee by restoring hard interrupt disabling where virtualizing the interrupt disable flag would defeat it.\nAs those lines are written, all stop_machine() use cases must also exclude any oob stage activity (e.g. ftrace live patching the kernel code for installing tracepoints), or happen before any such activity can ever take place (e.g. KPTI boot mappings). Dovetail makes a basic assumption that stop_machine() could not get in the way of latency-sensitive processes, simply because the latter could not keep running safely until a call to the former has completed anyway.\nHowever, one should keep an eye on stop_machine() usage upstream, identifying new callers which might cause unwanted latency spots under specific circumstances (maybe even abusing the interface).\nVirtual interrupt disable state breakage When some WARN_ON() triggers due to a wrong interrupt disable state (e.g. entering the softirqs/bh code with IRQs unexpectedly [virtually] disabled), this may be due to the CPU and virtual interrupt states being out-of-sync when traversing the epilogue code after a syscall, IRQ or trap has been handled during the latest kernel entry.\nTypically, do_work_pending() or do_notify_resume() should make sure to reconcile both states in the work loop, and also to restore the virtual state they received on entry before returning to their caller.\nСоветThe routines just mentioned always enter from their assembly call site with interrupts hard disabled in the CPU. However, they may be entered with the virtual interrupt state enabled or disabled, depending on the kind of event which led to them eventually. Typically, a system call epilogue would always enter with the virtual state enabled, but a fault might also occur when the virtual state is disabled though. The epilogue routine called for finalizing some IRQ handling must enter with the virtual state enabled, since the latter is a pre-requisite for running such code.\n Losing the timer tick The symptom of a common issue in a Dovetail port is losing the timer interrupt when the autonomous core takes control over the tick device, causing the in-band kernel to stall. After some time spent hanging, the in-band kernel may eventually complain about a RCU stall situation with a message like INFO: rcu_preempt detected stalls on CPUs/tasks followed by stack dump(s). In other cases, the machine may simply lock up due to an interrupt storm.\nThis is typical of timer interrupt events not flowing down normally to the in-band kernel anymore because something went wrong as soon as the proxy tick device replaced the regular device for serving in-band timing requests. When this happens, you should check the following code spots for bugs:\n  the timer acknowledge code is wrong once called from the oob stage, which is going to be the case as soon as an autonomous core installs the proxy tick device for interposing on the timer. Being wrong here means performing actions which are not legit from such a context.\n  the irqchip driver managing the interrupt event for the timer tick is wrong somehow, causing such interrupt to stay masked or stuck for some reason whenever it is switched to out-of-band mode. You need to double-check the implementation of the chip handlers, considering the effects and requirements of interrupt pipelining.\n  power management (CONFIG_CPUIDLE) gets in the way, often due to the infamous C3STOP misfeature turning off the original timer hardware controlled by the proxy device. A detailed explanation is given in Documentation/irq_pipeline.rst when discussing the few changes to the scheduler core for supporting the Dovetail interface. If this is acceptable from a power saving perspective, having the autonomous core prevent the in-band kernel from entering a deeper C-state is enough to fix the issue, by overriding the irq_cpuidle_control() routine as follows:\n  bool irq_cpuidle_control(struct cpuidle_device *dev, struct cpuidle_state *state) { /* * Deny entering sleep state if this entails stopping the * timer (i.e. C3STOP misfeature). */ if (state \u0026\u0026 (state-\u003eflags \u0026 CPUIDLE_FLAG_TIMER_STOP)) return false; return true; }  СоветPrintk-debugging such timer issue requires enabling raw printk() support, you won’t get away with tracing the kernel behavior using the plain printk() routine for this, because most of the output would remain stuck into a buffer, never reaching the console driver before the board hangs eventually.\n Hard interrupt masking in clock chip handlers The only valid way of sharing a clock tick device between the in-band and out-of-band stages is to access it through the tick proxy. For this reason, we don’t need to enforce hard interrupt masking in clock chip handlers to make them pipeline-safe, because once proxying is active for a tick device, hardware interrupts are off across calls to its handlers when applicable. As a result, either all accesses to the clock chip handlers are proxied and proper masking is already in place, or there is no proxy, which means the in-band kernel is still controlling the device, in which case there is no way we might conflict with out-of-band accesses.\nObviously, this fact does not preclude why we would still want to serialize CPUs when accessing shared data there, in which case hard locking should be in place to ensure this.\nMake no assumption in virtualizing arch_local_irq_restore() Do not make any assumption with respect to the current interrupt state when arch_local_irq_restore() is called, specifically don’t expect the inband stage to be stalled on entry. Some archs use constructs like follows, which breaks such assumption:\nlocal_save_flags(flags); local_irq_enable(); ... local_irq_restore(flags); In that case, we do want the stall bit to be restored unconditionally from flags. A correct implementation would be:\nstatic inline notrace void arch_local_irq_restore(unsigned long flags) { inband_irq_restore(arch_irqs_disabled_flags(flags)); barrier(); } RCU and out-of-band context The out-of-band context is semantically equivalent to the NMI context, therefore the current CPU cannot be in an extended quiescent state RCU-wise if running oob. CAUTION: the converse assertion is NOT true (i.e. a CPU running code on the in-band stage may be idle RCU-wise).\nCommon services which are safe in out-of-band context Dovetail guarantees that the following services are safe to call from the out-of-band stage:\n  irq_work() may be called from the out-of-band stage to schedule a (synthetic) interrupt in the in-band stage.\n  __raise_softirq_irqoff() may be called from the out-of-band stage to schedule a softirq. For instance, the EVL network stack uses this to kick the NET_TX_SOFTIRQ event in the in-band stage.\n  printk() may be called from any context, including out-of-band interrupt handlers. Messages are queued, then passed to the output device(s) only when the in-band stage resumes though.\n  ARM Context assumption with outer L2 cache There is no reason for the outer cache to be invalidated/flushed/cleaned from an out-of-band context, all cache maintenance operations must happen from in-band code. Therefore, we neither need nor want to convert the spinlock serializing access to the cache maintenance operations for L2 to a hard lock.\nЗаметкаThis above assumption is unfortunately only partially right, because at some point in the future we may want to run DMA transfers from the out-of-band context, which could entail cache maintenance operations.\n Conversion to hard lock may cause latency to skyrocket on some i.MX6 hardware, equipped with PL22x cache units, or PL31x with errata 588369 or 727915 for particular hardware revisions, as each background operation would be awaited for completion with hard irqs disabled, in order to work around some silicon bug.\n Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Developer's Notes",
    "uri": "/ru/dovetail/porting/devnotes/"
  },
  {
    "content": "For specific use cases requiring reliable, ultra-low response times, we want to enable hosted autonomous software cores to control common Linux tasks based on their own scheduler infrastructure, fully decoupled from the host’s scheduler, with absolute priority over all other kernel activities.\nThis being said, Dovetail also promotes the idea that a dual kernel system should keep the functional overlap between the main kernel and the autonomous core minimal. To this end, a task from such core should be merely seen as a regular Linux task with additional scheduling capabilities guaranteeing very low and bounded response times. To support such idea, Dovetail enables kthreads and regular user tasks to run alternatively in the out-of-band execution context introduced by the interrupt pipeline (aka out-of-band stage), or the common in-band kernel context for GPOS operations (aka in-band stage). These new capabilities are built on the interrupt pipeline machinery.\nAs a result, autonomous core applications in user-space benefit from the common Linux programming model - including virtual memory protection -, and still have access to the main kernel services when carrying out non time-critical work.\nDovetail provides mechanisms to autonomous cores for supporting this as follows:\n  services for moving Linux tasks back and forth between the in-band and out-of-band stages in an orderly and safe way, properly synchronizing the operation between the two schedulers involved.\n  notifications about in-band events the autonomous core may want to know about for maintaining its own version of the current task’s state.\n  notifications about events such as faults, syscalls and other exceptions issued from the out-of-band execution stage.\n  integrated support for performing context switches between out-of-band tasks, including memory context and FPU management.\n  Theory of operations   a Linux task running in user-space, or a kernel thread, need to initialize the alternate scheduling feature for themselves with a call to dovetail_init_altsched(). For instance, the EVL core does so as part of the attachment process of a thread to the autonomous core when evl_attach_self() is called by the application.\n  once the alternate scheduling feature is initialized, the task should enable it by a call to dovetail_start_altsched(). From this point, that task:\n  can switch between the in-band and out-of-band execution stages freely.\n  can emit out-of-band system calls the autonomous core can handle.\n  is tracked by Dovetail’s notification system, so that in-band and out-of-band events which may involve such task are dispatched to the core.\n  can be involved in Dovetail-based context switches triggered by the autonomous core, independently from the scheduling operations carried out by the main kernel.\nConversely, dovetail_stop_altsched() disables the notifications for a task, which is likely to be detached from the autonomous core later on.\n    at any point in time, any Linux task is either controlled by the main kernel or by the autonomous core, scheduling-wise. There cannot be any overlap (for obvious reasons):\n  a task which is currently scheduled by the autonomous core runs or sleeps on the out-of-band stage. At the same time, such task is deemed to be sleeping in TASK_INTERRUPTIBLE state for the main kernel.\n  conversely, a task which is controlled by the main kernel runs or sleeps on the in-band stage. It must be considered as blocked on the other side. For instance, the EVL core defines the T_INBAND blocking condition for representing such state.\n    since the out-of-band stage receives interrupts first, regardless of any interrupt masking which may be in effect for the ongoing in-band work, the autonomous core can run high-priority interrupt handlers then reschedule with no delay.\n  tasks may switch back and forth between the in-band and out-of-band stages at will. However, this comes at a cost:\n  the process of switching stages is heavyweight, it includes a double scheduling operation at least (i.e. one to suspend on the exited stage, one to resume from the opposite stage).\n  only the out-of-band stage guarantees bounded response times to external and internal events. Therefore, a task which leaves the out-of-band stage for resuming in-band looses such guarantee, until it has fully switched back to out-of-band context at some point later.\n    at any point in time, the autonomous core keeps the CPU busy until no more task it knows about is runnable on that CPU, on the out-of-band stage. When the out-of-band activity quiesces, the core is expected to relinquish the CPU to the in-band stage by scheduling in the context which was originally preempted. This is commonly done by having a task placeholder with the lowest possible priority represent the main kernel and its in-band context linked to each per-CPU run queue maintained by the autonomous core. For instance, the EVL core assigns such a placeholder task to its SCHED_IDLE policy, which get picked when other policies have no runnable task on the CPU.\n  once the in-band context resumes, interrupt events which have no out-of-band handlers are delivered to the regular in-band IRQ handlers installed by the main kernel, if the virtual masking state allows it.\n    void dovetail_init_altsched(struct dovetail_altsched_context *p)  This call initializes the alternate scheduling context for the current task; this should be done once, before the task calls dovetail_start_altsched().\npThe alternate scheduling context is kept in a per-task structure of type dovetail_altsched_context which should be maintained by the autonomous core. This can be done as part of the per-task context management feature Dovetail introduces.\n\n  void dovetail_start_altsched(void)  This call tells the kernel that the current task may request alternate scheduling operations any time from now on, such as switching out-of-band or back in-band. It also activates the event notifier for the task, which allows it to emit out-of-band system calls to the core.\n  void dovetail_stop_altsched(void)  This call disables the event notifier for the current task, which must be done before dismantling the alternate scheduling support for that task in the autonomous core.\n What you really need to know at this point There is a not-so-subtle but somewhat confusing distinction between running a Linux task out-of-band, and running whatever code from the out-of-band execution stage.\n  In the first case, the task is not controlled by the main kernel scheduler anymore, but runs under the supervision of the autonomous core. This is obtained by performing an out-of-band switch for a task.\n  In the other case, the current underlying context (task or whatever else) may or may not be controlled by the main kernel, but the interrupt pipeline machinery has switched the CPU to the out-of-band execution mode, which means that only interrupts bearing the IRQF_OOB flag are delivered. Typically, run_oob_call() is a service provided by the interrupt pipeline which executes a function call over this context, without requiring the calling task to be scheduled by the autonomous core.\n  You will also find references to pseudo-routines called core_schedule(), core_suspend_task() or core_resume_task() in various places. Don’t look for them into the code, they don’t actually exist: those routines are mere placeholders for the corresponding services you would provide in your autonomous core. For instance, the EVL core implements them as evl_schedule(), evl_suspend_thread() and evl_release_thread().\nYou may want to keep this in mind when going through the rest of this document.\nSwitching between execution stages Out-of-band switch  Switching out-of-band is the operation by which a Linux task moves under the control of the alternate scheduler the autonomous core adds to the main kernel. From that point, the scheduling decisions are made by this core regarding that task. There are two reasons a Linux task may switch from in-band to out-of-band execution:\n  either such transition is explicitly requested via a system call of the autonomous core, such as EVL’s evl_switch_oob().\n  or the autonomous core has forced such transition, in response to a user request, such as issuing a system call which can only be handled from the out-of-band stage. The EVL core ensures this.\n  Using Dovetail, a task which is executing on the in-band stage can switch out-of-band following this sequence of actions:\n  this task calls dovetail_leave_inband() from the in-band stage it runs on (blue), which prepares for the transition, puts the caller to sleep (TASK_INTERRUPTIBLE) then reschedules immediately. At this point, the migrating task is in flight to the out-of-band stage (light red). Meanwhile, schedule() resumes the next in-band task which should run on the current CPU.\n  as the next in-band task context resumes, the scheduling tail code checks for any task pending transition to out-of-band stage, before the CPU is fully relinquished to the resuming in-band task. This check is performed by the inband_switch_tail() call present in the main scheduler. Such call has two purposes:\n  detect when a task is in flight to the out-of-band stage, so that we can notify the autonomous core for finalizing the migration process.\n  detect when a task is resuming on the out-of-band stage, which happens when the autonomous core switches context back to the current task, once the migration process is complete.\n    When switching out-of-band, case #1 is met, which triggers a call to the resume_oob_task() handler the companion core should implement for completing the transition. This would typically mean: unblock the migrating task from the standpoint of its own scheduler, then reschedule. In the following flowchart, core_resume_task() and core_schedule() stand for these two operations, with each dotted link representing a context switch:\ngraph LR; S(\"start transition\") -- A style S fill:#99ccff; A[\"dovetail_leave_inband()\"] -- B[\"schedule()\"] style A fill:#99ccff; style B fill:#99ccff; B -.- C[\"inband_switch_tail()\"] C -- D{task in flight?} D --|Yes| E[\"resume_oob_task()\"] style E fill:#99ccff; D --|No| F{out-of-band?} E -- G[\"core_resume_task()\"] style G fill:#99ccff; G -- H[\"core_schedule()\"] style H fill:#ff950e; F --|Yes| I(transition complete) style I fill:#ff950e; F --|No| J(regular switch tail) style J fill:#99ccff; H -.- C  At the end of this process, we should have observed a double context switch, with the migrating task offloaded to the out-of-band scheduler:\nСоветevl_switch_oob() implements the switch to out-of-band context in the EVL core, with support from evl_release_thread() and evl_schedule() for resuming and rescheduling threads respectively.\n In-band switch Switching in-band is the operation by which a Linux task moves under the control of the main scheduler, coming from the out-of-band execution stage. From that point, the scheduling decisions are made by the main kernel regarding that task, and it may be subject to preemption by in-band interrupts as well. In other words, a task switching in-band looses all guarantees regarding bounded response time; however, it regains access to the entire set of GPOS services in the same move.\nThere are several reasons a Linux task which was running out-of-band so far may have to switch in-band:\n  it has requested it explicitly using a system call provided by the autonomous core such as EVL’s evl_switch_inband().\n  the autonomous core has forced such transition for a task running in user-space:\n  in response to some request this task did, such as issuing a regular system call which can only be handled from the in-band stage. Typically, this behavior is enforced by the EVL core.\n  because the task received a synchronous fault or exception, such as a memory access violation, FPU exception and so on. A demotion is required, because handling such events directly from the out-of-band stage would require a fair amount of code duplication, and most likely raise all sorts of funky conflicts between the out-of-band handlers and several in-band sub-systems. Besides, there is not much point in expecting real-time guarantees from a code that basically fixes up a situation caused by a dysfunctioning application in the first place.\n  a signal is pending for the task. Because the main kernel logic may require signals to be acknowledged by the recipient, we have to transition through the in-band stage to make sure the pending signal(s) will be delivered asap.\n    Using Dovetail, a task which executes on the out-of-band stage moves in-band following this sequence of actions:\n  the execution of an in-band handler is scheduled from the context of the migrating task on the out-of-band stage. Once it runs, this handler should call wake_up_process() to unblock that task from the standpoint of the main kernel scheduler, since it is sleeping in TASK_INTERRUPTIBLE state there. Typically, the irq_work mechanism can be used for this, because:\n  as extended by the interrupt pipeline support, this interface can be used from the out-of-band stage.\n  the handler is guaranteed to run on the CPU from which the request was issued. Because the in-band work will wait until the out-of-band activity quiesces on that CPU, this in turn ensures all other operations we have to carry out from the out-of-band stage for preparing the migration are done before the task is woken up eventually.\n    the autonomous core blocks/suspends the migrating task, rescheduling immediately afterwards. For instance, the EVL core adds the T_INBAND block bit to the task’s state for this purpose.\n  at some point later, the out-of-band context is exited by the current CPU when no more out-of-band work is left, causing the in-band kernel code to resume execution at the latest preemption point. The handler scheduled at step #1 eventually runs, waking up the migrating task from the standpoint of the main kernel. The TASK_RUNNING state is set for the task.\n  the migrating task resumes from the tail scheduling code of the alternate scheduler, where it suspended in step #2. Noticing the migration, the core calls dovetail_resume_inband() eventually, for finalizing the transition of the incoming task to the in-band stage.\n  In the following flowchart, core_suspend_task() and core_schedule() stand for the operations described at step #2, with each dotted link representing a context switch. The out-of-band idle state represents the CPU transitioning from out-of-band (light red) to in-band (blue) execution stage, as the core has no more out-of-band task to schedule:\ngraph LR; S(\"start transition\") -- A style S fill:#ff950e; A[\"irq_work(wakeup_req)\"] -- B[\"core_suspend_task()\"] style A fill:#ff950e; style B fill:#ff950e; B -- C[\"core_schedule()\"] style C fill:#ff950e; C -.- Y((OOB idle)) Y -.- D[\"wake_up_process()\"] style D fill:#99ccff; D -- E[\"schedule()\"] style E fill:#99ccff; E -.- X[\"out-of-band switch tail\"] style X fill:#99ccff; X -- G[\"dovetail_resume_inband()\"] style G fill:#99ccff; G -- I(\"transition complete\") style I fill:#99ccff;  At the end of this process, the task has transitioned from a running state to a blocked state in the autonomous core, and conversely from TASK_INTERRUPTIBLE to TASK_RUNNING in the main scheduler.\nСоветevl_switch_inband() switches the caller to in-band context in the EVL core, with support from evl_suspend_thread() and evl_schedule() for suspending and rescheduling threads respectively.\n Switching tasks out-of-band Dovetail allows an autonomous core embedded into the main kernel to schedule a set of Linux tasks out-of-band compared to the regular in-band kernel work, so that:\n  the worse-case response time of such tasks only depends on the performance of a software core which has a limited complexity.\n  the main kernel’s logic does not have to cope with stringent bounded response time requirements, which otherwise tends to lower the throughput while making the design, implementation and maintenance significantly more complex.\n  The flowchart below represents the typical context switch sequence an autonomous core should implement, from the context of the outgoing PREV task to the incoming NEXT task:\ngraph LR; S(PREV) -- A[\"core_schedule()\"] A -- B{in-band IRQ stage?} B --|Yes| D[\"jump out-of-band\"] D -- A B --|No| C[\"pick NEXT\"] style C fill:#ff950e; C -- P{PREV == NEXT?} style P fill:#ff950e; P --|Yes| Q(no change) style Q fill:#ff950e; P --|No| H[\"dovetail_context_switch()\"] style H fill:#ff950e; H -.- I(NEXT)  Those steps are:\n  core_schedule() is called over the PREV context for picking the NEXT task to schedule, by priority order. If PREV still has the highest priority among all runnable tasks on the current CPU, the sequence stops there. CAVEAT: the core must make sure to perform context switches from the out-of-band execution stage, otherwise weird things may happen down the road. run_oob_call() is a routine the interrupt pipeline provides which may help there. See the implementation of evl_schedule() in the EVL core for a typical usage.\n  dovetail_context_switch() is called, switching the memory context as/if required, and the CPU register file to NEXT’s, saving PREV’s in the same move. If NEXT is not the low priority placeholder task but PREV is, we will be preempting the in-band kernel: in this case, we must tell the kernel about such preemption by passing leave_inband=true to dovetail_context_switch().\n  NEXT resumes from its latest switching point, which may be:\n  the switch tail code in core_schedule(), if NEXT was running out-of-band prior to sleeping, in which case dovetail_context_switch() returns false.\n  the switch tail code of schedule() if NEXT is completing an in-band switch, in which case dovetail_context_switch() returns true.\n      bool dovetail_context_switch(struct dovetail_altsched_context *prev, struct dovetail_altsched_context *next, bool leave_inband)  prevThe alternate scheduling context block of the outgoing task.\n\nnextThe alternate scheduling context block of the incoming task.\n\nleave_inbandA boolean indicating whether we are leaving the in-band tasking mode, which happens when prev is the autonomous core’s low priority placeholder task standing for the in-band kernel context as a whole.\n\nThis routine performs an out-of-band context switch. It must be called with hard IRQs off. The arch-specific arch_dovetail_context_resume() handler is called by the resuming task before leaving dovetail_context_switch(). This weak handler should be overriden by a Dovetail port which requires arch-specific tweaks for completing the reactivation of next. For instance, the arm64 port performs the fpsimd management from this handler.\ndovetail_context_switch() returns a boolean value telling the caller whether the current task just returned from a transition from out-of-band to in-band context.\n  int dovetail_leave_inband(void)  dovetail_leave_inband() should be called by your companion core in order to perform the out-of-band switch for the current task.\nOn success, zero is returned, and the calling context is running on the out-of-band stage. Otherwise, -ERESTARTSYS is returned if a signal was pending at the time of the call, in which case the transition could not take place.\n The usage is illustrated by the implementation of evl_switch_oob() in the EVL core.\n   void dovetail_resume_inband(void)  dovetail_resume_inband() should be called by your companion core as part of the in-band switch process for the current task, after the current task has resumed on the in-band execution stage from the out-of-band suspension call in step 2 of the in-band switch process. This mandatory call finalizes the transition to this stage, by reconciling the current task state with the internal state of the in-band scheduler.\n The usage is illustrated by the implementation of evl_switch_inband() in the EVL core.\n   __weak void resume_oob_task(void)  This handler should be implemented by your companion core, in order to complete step 2 of the out-of-band switch process. Basically, this handler should lift the blocking condition added to the task at step 2 of the in-band switch process which denotes in-band execution, such as T_INBAND for the EVL core.\nresume_oob_task is called with interrupts disabled in the CPU, out-of-band stage is stalled.\n An implementation of resume_oob_task() is present in the EVL core.\n The event notifier Once dovetail_start_altsched() has been called for a regular in-band task, it may receive events of interest with respect to running under the supervision of an autonomous core. Those events are delivered by invoking handlers which should by implemented by this core.\nOut-of-band exception handling If a processor exception is raised while the CPU is busy running a task on the out-of-band stage (e.g. due to some invalid memory access, bad instruction, FPU or alignment error etc.), the task has to leave such context before it may run the in-band fault handler. Dovetail notifies the core about incoming exceptions early from the low-level fault trampolines, but only when some out-of-band code was running when the exception was taken. The core may then fix up the current context, such as switching to the in-band execution stage.\nСоветEnabling debuggers to trace tasks running on the out-of-band stage involves dealing with debug traps ptrace() may poke into the debuggee’s code for breakpointing.\n The notification of entering a trap is delivered to the handle_oob_trap_entry() handler the core should override for receiving those events (__weak binding). handle_oob_trap_entry() is passed the exception code as defined in arch/*/include/asm/dovetail.h, and a pointer to the register frame of the faulting context (struct pt_regs).\nBefore the in-band trap handler eventually exits, it invokes handle_oob_trap_exit() which the core should override if it needs to perform any fixup before the trap context is left (__weak binding). handle_oob_trap_exit() is passed the same arguments than handle_oob_trap_entry().\n  __weak void handle_oob_trap_entry(unsigned int trapnr, struct pt_regs *regs)  This handler is called when a CPU trap is received by a Dovetail-enabled task while running on the out-of-band execution stage. In such an event, the caller must be switched to the in-band stage, in order to safely perform the normal trap handling operations on return. In other words, Dovetail invokes this handler to ask your companion core to switch back to a safe in-band context, before the in-band kernel can actually handle such trap.\nObviously, the caller would lose the benefit of running on the out-of-band stage, inducing latency in the process, but since it has taken a CPU trap, it looks like things did not go as expected already anyway. So the best option in this case is to switch the caller to in-band mode, leaving the actual trap handling and any related fixup to the in-band code once the transition is done.\ntrapnrThe trap code number. Such code depends on the CPU architecture:\n the documented Intel trap numbers are used for x86 (#GP, #DE, #OF etc.) other architectures may use a Dovetail-specific enumeration defined in arch/*/include/asm/dovetail.h.  \nregsThe register file at the time of the trap.\n\nInterrupts are disabled in the CPU when this handler is called.\n An implementation of handle_oob_trap_entry() is present in the EVL core.\n  __weak void handle_oob_trap_exit(unsigned int trapnr, struct pt_regs *regs)  This handler is called when the in-band trap handler is about to unwind a CPU trap context for a Dovetail-enabled task.\nhandle_oob_trap_exit is paired with handle_oob_trap_exit, and receives the same arguments.\ntrapnrThe trap code number. Such code depends on the CPU architecture:\n the documented Intel trap numbers are used for x86 (#GP, #DE, #OF etc.) other architectures may use a Dovetail-specific enumeration defined in arch/*/include/asm/dovetail.h.  \nregsThe register file at the time of the trap.\n\nInterrupts are disabled in the CPU when this handler is called.\n An implementation of handle_oob_trap_exit() is present in the EVL core.\n System calls The autonomous core is likely to introduce its own set of system calls application tasks may invoke. From the standpoint of the in-band kernel, this is a foreign set of calls, which can be distinguished unambiguously from regular ones. If a task attached to the core issues any system call, regardless of which of the kernel or the core should handle it, the latter must be given the opportunity to:\n  handle the request directly, possibly switching the caller to out-of-band context first if required.\n  pass the request downward to the in-band system call path on the in-band stage, possibly switching the caller to in-band context if needed.\n  Dovetail intercepts system calls early in the kernel entry code, delivering them to one of these handlers the core should override:\n  the call is delivered to the handle_oob_syscall() handler if the system call number is not in the valid range for the in-band kernel - i.e. it has to belong to the core instead -, and the caller is currently running on the out-of-band stage. This is the fast path, when a task running out-of-band is requesting a service the core provides.\n  otherwise the slow path is taken, in which handle_pipelined_syscall() is probed for handling the request from the appropriate execution stage. In this case, Dovetail performs the following actions:\n  graph LR; S(\"switch to out-of-band IRQ stage\") -- A style S fill:#ff950e; A[\"handle_pipelined_syscall()\"] -- B{returned zero?} B --|Yes| C{on in-band IRQ stage?} B --|No| R{on in-band IRQ stage?} R --|Yes| U(branch to in-band user mode exit) R --|No| V(done) C --|Yes| Q(branch to in-band syscall handler) style Q fill:#99ccff; C --|No| P[switch to in-band IRQ stage] style P fill:#ff950e; P -- A  In the flowchart above, handle_pipelined_syscall() should return zero if it wants Dovetail to propagate an unhandled system call down the pipeline at each of the two possible steps, non-zero if the request was handled. Branching to the in-band user mode exit code ensures any pending (in-band) signal is delivered to the current task and rescheduling opportunities are taken when (in-band) kernel preemption is enabled.\nWhat makes a system call number out-of-range for the in-band kernel is architecture-dependent. All architectures currently use the most-significant bit in the syscall number as a differentiator (i.e. regular if MSB cleared, foreign if LSB set). See how __EVL_SYSCALL_BIT is used for this purpose in the libevl implementation for arm64, x86 and ARM respectively.\nInterrupts are always enabled in the CPU when any of these handlers is called.\nЗаметкаThe core may need to switch the calling task to the converse execution stage (i.e. in-band \u003c-\u003e out-of-band) either from the handle_oob_syscall() or handle_pipelined_syscall() handlers, this is fine. Dovetail would notice and continue accordingly to the current stage on return of these handlers.\n   __weak void handle_oob_syscall(struct pt_regs *regs)  This handler should implement the fast path for handling out-of-band system calls. It is called by Dovetail when a system call is detected on entry of the common syscall path implemented by the in-band kernel, which should be handled directly by the companion core instead. Both of the following conditions must be met for this route to be taken:\n  the caller is a user task running on the out-of-band execution stage.\n  the system call number is not in the range of the regular in-band system call numbers.\n  A system call meeting those conditions denotes a request issued by an application to a companion core which it can handle directly from its native execution stage (i.e. out-of-band). This handler should be defined in the code of your companion core implementation for receiving such system calls. Dovetail defines a dummy weak implementation of it, which the implementation of the core would supersede if defined (__weak binding).\nregsThe register file at the time of the system call, which contains the call arguments passed by the application.\n\nThe handler should write the error code of the request to the in-memory storage of the proper CPU register in regs, as defined by the ABI convention for the CPU architecture.\nhandle_oob_syscall is called with interrupts enabled in the CPU, out-of-band stage is unstalled. Whether the in-band stage accepts interrupts is undefined.\n The EVL core exhibits a typical implementation of such a handler.\n   __weak void handle_pipelined_syscall(struct irq_stage *stage, struct pt_regs *regs)  This handler is called by Dovetail when a system call is detected on entry of the common syscall path implemented by the in-band kernel, if the requirements for delivering it to the companion core via the fast route implemented by handle_oob_syscall are not met, and any of the following condition is true:\n  the caller is a user task for which alternate scheduling was enabled.\n  the system call number is not in the range of the regular in-band system call numbers, which means that a companion core might be able to handle it (if not eventually, such a request would cause the caller to receive -ENOSYS).\n  stageThe descriptor address of the calling stage, either oob_stage or inband_stage.\n\nregsThe register file at the time of the system call.\n\nThe handler should write the error code of the request to the in-memory storage of the proper CPU register in regs, as defined by the ABI convention for the CPU architecture. In addition, handle_pipelined_syscall should return an integer status, which specifies the action Dovetail should take on return:\n  zero tells Dovetail to pass the system call to the regular in-band handler next. This status makes sense if the companion core did not handle the request, but did switch the calling context to in-band mode. This typically happens when the companion core wants to automatically demote the execution stage of the caller when it detects a regular in-band system call issued over the wrong (i.e. out-of-band) context, in which case it may switch it in-band automatically for preserving the system integrity, before asking Dovetail to forward the request to the right (in-band) handler.\n  a strictly positive status tells Dovetail to branch to the system call exit path immediately. This status makes sense if the companion core did handle the request, leaving the caller on the out-of-band execution stage.\n  a negative status tells Dovetail to branch to the regular system call epilogue, without passing the system call to the regular in-band handler though. This status makes sense if the companion core already handled the request, switching to in-band mode in the process. In such an event, the in-band kernel still wants to check for pending signals and rescheduling opportunity, which is the purpose of the epilogue code.\n  handle_pipelined_syscall is called with interrupts enabled in the CPU, out-of-band stage is unstalled. If current, the in-band stage accepts interrupts, otherwise whether the in-band stage is stalled or not is undefined.\n An implementation of handle_pipelined_syscall() is present in the EVL core.\n In-band events The last set of notifications involves pure in-band events which the autonomous core may need to know about, as they may affect its own task management. Except for INBAND_TASK_EXIT and INBAND_PROCESS_CLEANUP which are called for any exiting user-space task, other notifications are only issued for tasks for which dovetailing is enabled (see dovetail_start_altsched()).\nThe notification is delivered to the handle_inband_event() handler. The execution context of this handler is always in-band. The out-of-band and in-band stages are both unstalled at the time of the call. The notification handler receives the event type code, and a single pointer argument which depends on the event type. The following events are defined (see include/linux/dovetail.h):\n  INBAND_TASK_SIGNAL(struct task_struct *target)\nsent when @target is about to receive a signal. The core may decide to schedule a transition of the recipient to the in-band stage in order to have it handle that signal asap, which is required for keeping the kernel sane. This notification is always sent from the context of the issuer.\n  INBAND_TASK_MIGRATION(struct dovetail_migration_data *p)\nsent when p-\u003etask is about to move to CPU p-\u003edest_cpu.\n  INBAND_TASK_EXIT(struct task_struct *current)\nsent from do_exit() before the current task has dropped the files and mappings it owns.\n  INBAND_PROCESS_CLEANUP(struct mm_struct *mm)\nsent before mm is entirely dropped, before the mappings are exited. Per-process resources which might still be maintained by the core could be released there, as all tasks sharing this memory context have exited. Unlike other events, this one is triggered for every user-space task which is being dismantled by the kernel, regardless of whether some of its threads were known from your autonomous core.\n  INBAND_TASK_RETUSER(void)\nsent as a result of arming the return-to-user request via a call to dovetail_request_ucall(). In other words, Dovetail fires this event because you asked for it by calling the latter service. The INBAND_TASK_RETUSER event handler in your companion core is allowed to switch the caller to the out-of-band stage before returning, ensuring the application code resumes in that context.\n  INBAND_TASK_PTSTOP(void)\nsent when the current in-band task is about to sleep in ptrace_stop(), which is the place a task is supposed to wait until a debugger allows it to continue. A user-space task which receives SIGTRAP as a result of hitting a breakpoint, or SIGSTOP from the ptrace(2) machinery parks itself by calling ptrace_stop(), until resumed by a ptrace(2) request.\n  INBAND_TASK_PTCONT(void)\nsent when the current in-band task is waking up from ptrace_stop() after the debugger allowed it to continue. The task may return to user-space afterwards, or go handling some pending signals.\n  INBAND_TASK_PTSTEP(struct task_struct *target)\nsent by the ptrace(2) implementation when it is about to (single-)step the @target task. Like PTSTOP and PTCONT, PTSTEP can be used to synchronize your companion core with the ptrace(2) logic. For instance, EVL uses these events to provide support for synchronous breakpoints when debugging applications over gdb.\n    __weak void handle_inband_event(enum inband_event_type event, void *data)  The handler which should be defined in the code of your companion core implementation for receiving in-band event notifications. Dovetail defines a dummy weak implementation of it, which your implementation would supersede if defined.\neventThe event code (INBAND_TASK_*) as defined above.\n\ndataAn opaque pointer to some data further qualifying the event, which actual type depends on the event code.\n\nThe in-band stage is always current and accepts interrupts on entry to this call.\n An implementation of handle_inband_event() is present in the EVL core.\n Alternate task context Your autonomous core will need to keep its own set of per-task data, starting with the alternate scheduling context block. To help in maintaining such information on a per-task, per-architecture basis, Dovetail adds the oob_state member of type struct oob_thread_state to the (stack-based) thread information block (aka struct thread_info) each kernel architecture port defines.\nYou may want to fill that structure reserved to out-of-band support with any information your core may need for maintaining a task context. By having this information defined in a file accessible from the architecture-specific code by including \u003cdovetail/thread_info.h\u003e, the core-specific structure is automatically added to struct thread_info as required. For instance:\n arch/arm/include/dovetail/thread_info.h\n struct oob_thread_state { /* Define your core-specific per-task data here. */ }; The core may then retrieve the address of the structure by calling dovetail_current_state(). For instance, this is the definition the EVL core has for the oob_thread_state structure, storing a backpointer to its own task control block, along with a core-specific preemption count for fast stack-based access:\nstruct evl_thread; struct oob_thread_state { struct evl_thread *thread; int preempt_count; }; Which gives the following chain:\ngraph LR; T(\"struct thread_info\") -- |includes| t t(\"struct oob_thread_state\") -.- |refers to| c c(\"struct evl_thread\")  Note that we should not add all of the out-of-band state data directly into the oob_thread_state structure, because the latter is present in every task_struct descriptor, although only a few tasks may actually need it. Hence the backpointer to the evl_thread structure which only consumes a few bytes, so leaving it unused (NULL) for all other - in-band only - tasks is no big deal.\n  struct oob_thread_state *dovetail_current_state(void)  This call retrieves the address of the out-of-band data structure for the current task, which is always a valid pointer. The content of this structure is zeroed when the task is created, and stays so until your autonomous core initializes it.\nExtended memory context Your autonomous core may also need to keep its own set of per-process data. To help in maintaining such information on a per-mm basis, Dovetail adds the oob_state member of type struct oob_mm_state to the generic struct mm_struct descriptor . Because kernel threads can only borrow memory contexts temporarily but do not actually own any, this Dovetail extension is only available to EVL threads running in user-space, not to threads created by evl_run_kthread().\nYou may want to fill that structure reserved to out-of-band support with any information your core may need for maintaining a per-mm context. By having this information defined in a file accessible from the architecture-specific code by including \\\u003cdovetail/mm_info.h\\\u003e, the core-specific structure is automatically added to struct mm_struct as required. For instance:\n arch/arm/include/dovetail/mm_info.h\n struct oob_mm_state { /* Define your core-specific per-mm data here. */ }; The core may then retrieve the address of the structure for the current task by calling dovetail_mm_state(). The EVL core does not define any extended memory context yet, but it could be used to maintain a per-process file descriptor table for instance.\nСоветCatching the INBAND_PROCESS_CLEANUP event would allow you to drop any resources attached to the extended memory context information, since it is called when this data is no longer in use by any thread of the exiting process.\n   struct oob_mm_state *dovetail_mm_state(void)  This call retrieves the address of the out-of-band data structure within the mm descriptor of the current user-space task. The content of this structure is zeroed by the in-band kernel when it creates the memory context, and stays so until your autonomous core initializes it. If called from a kernel thread, NULL is returned instead.\n Definition of dovetail_mm_state()\n static inline struct oob_mm_state *dovetail_mm_state(void) { if (current-\u003eflags \u0026 PF_KTHREAD) return NULL; return \u0026current-\u003emm-\u003eoob_state; } Intercepting return to in-band user mode In some specific cases, the implementation of your companion core may need some thread running in-band to call back before it resumes execution of the application code in user mode. For instance, you might want to force that thread to switch back to the out-of-band stage before it leaves the kernel and resumes user mode execution. For this to happen, you would need that thread to jump back to the core at that point, which would do the right thing from there. dovetail_request_ucall allows that.\n  void dovetail_request_ucall(struct task_struct *target)  targetThe task which should call back at the first opportunity. target should have enabled the alternate scheduling support by a previous call to dovetail_start_altsched(). If not, dovetail_request_ucall has no effect.\n\nPend a request for target to fire the INBAND_TASK_RETUSER event at the first opportunity, which happens when the task is about to resume execution in user mode from the in-band stage.\nIf the INBAND_TASK_RETUSER event handler in the companion core sends any signal to the current task, it will be delivered before the application code resumes. The event handler may pend a request to be called back again using dovetail_request_ucall().\n Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC content/dovetail/altsched.md e6f2cbe ",
    "description": "",
    "tags": null,
    "title": "Alternate scheduling",
    "uri": "/ru/dovetail/altsched/"
  },
  {
    "content": "Turn on debug options in the kernel configuration! During the development phase, do yourself a favour: turn on CONFIG_DEBUG_IRQ_PIPELINE and CONFIG_DEBUG_DOVETAIL.\nThe first one will catch many nasty issues, such as calling unsafe in-band code from out-of-band context. The second one checks the integrity of the alternate scheduling support, detecting issues in the architecture port.\nThe runtime overhead induced by enabling these options is marginal. Just don’t port Dovetail or implement out-of-band client code without them enabled in your target kernel, seriously.\nSerialize stages when accessing shared data If some writable data is shared between in-band and out-of-band code, you have to guard against out-of-band code preempting or racing with the in-band code which accesses the same data. This is required to prevent dirty reads and dirty writes:\n  one the same CPU, by disabling interrupts in the CPU.\n  from different CPUs, by using hard or hybrid spinlocks.\n  Check that the pipeline torture tests pass Before any consideration is made to implement out-of-band code on a platform, check that interrupt pipelining is sane there, by enabling CONFIG_IRQ_PIPELINE_TORTURE_TEST in the configuration. As its name suggests, this option enables test code which exercizes the interrupt pipeline core, and related features such as the proxy tick device.\nСоветSince the torture tests need to enable the out-of-band stage for their own purpose, you may have to disable any Dovetail-based autonomous core in the kernel configuration for running those tests, like switching off CONFIG_EVL if the EVL core is enabled.\n When those tests pass, the following output should appear in the kernel log:\nStarting IRQ pipeline tests... IRQ pipeline: high-priority torture stage added. irq_pipeline-torture: CPU0 initiates stop_machine() irq_pipeline-torture: CPU3 responds to stop_machine() irq_pipeline-torture: CPU1 responds to stop_machine() irq_pipeline-torture: CPU2 responds to stop_machine() irq_pipeline-torture: CPU1: proxy tick registered (62.50MHz) irq_pipeline-torture: CPU2: proxy tick registered (62.50MHz) irq_pipeline-torture: CPU3: proxy tick registered (62.50MHz) irq_pipeline-torture: CPU0: proxy tick registered (62.50MHz) irq_pipeline-torture: CPU0: irq_work handled irq_pipeline-torture: CPU0: in-band-\u003ein-band irq_work trigger works irq_pipeline-torture: CPU0: stage escalation request works irq_pipeline-torture: CPU0: irq_work handled irq_pipeline-torture: CPU0: oob-\u003ein-band irq_work trigger works IRQ pipeline: torture stage removed. IRQ pipeline tests OK. Otherwise, if you observe any issue when running any of those tests, then the IRQ pipeline definitely needs fixing.\nKnow how to differentiate safe from unsafe in-band code Not all in-band kernel code is safe to be called from out-of-band context, actually most of it is unsafe for doing so.\nA code is deemed safe in this respect when you are 101% sure that it never does, directly or indirectly, any of the following:\n  attempts to reschedule in-band wise, meaning that schedule() would end up being called. The rule of thumb is that any section of code traversing the might_sleep() check cannot be called from out-of-band context.\n  takes a spinlock from any regular type like raw_spinlock_t or spinlock_t. The former would affect the virtual interrupt disable flag which is invalid outside of the in-band context, the latter might reschedule if CONFIG_PREEMPT is enabled.\n  СоветIn the early days of dual kernel support in Linux, some people would mistakenly invoke the do_gettimeofday() routine from an out-of-band context in order to get a wallclock timestamp for their real-time code. Doing so would create a deadlock situation if some in-band code running do_gettimeofday() is preempted by the out-of-band code re-entering the same routine on the same CPU. The out-of-band code would then wait spinning indefinitely for the in-band context to leave the routine - which won’t happen by design - leading to a lockup. Nowadays, enabling CONFIG_DEBUG_IRQ_PIPELINE would be enough to detect such mistake early enough to preserve your mental health.\n Careful with disabling interrupts in the CPU When pipelining is enabled, use hard interrupt protection with caution, especially from in-band code. Not only this might send latency figures over the top, but this might even cause random lockups would a rescheduling happen while interrupts are hard disabled.\nDealing with spinlocks Converting regular kernel spinlocks (e.g. spinlock_t, raw_spin_lock_t) to hard spinlocks should involve a careful review of every section covered by such lock. Any such section would then inherit the following requirements:\n  no unsafe in-band kernel service should be called within the section.\n  the section covered by the lock should be short enough to keep interrupt latency low.\n  Enable RAW_PRINTK support for printk-like debugging Unless you are lucky enough to have an ICE for debugging hard issues involving out-of-band contexts, you might have to resort to basic printk-style debugging over a serial line. Although the printk() machinery can be used from out-of-band context when Dovetail is enabled, you should rather use the raw_printk() interface for this.\n Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "Rules Of Thumb",
    "uri": "/ru/dovetail/rulesofthumb/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Xenomai 4",
    "uri": "/ru/"
  },
  {
    "content": "  #abimap { width: 35%; margin-left: auto; margin-right: auto; } #abimap th { text-align: center; } #abimap td { text-align: center; } #abimap tr:nth-child(even) { background-color: #f2f2f2; }   Revision Purpose libevl release   27 Handle prctl()-based syscall form. This enables Valgrind for EVL applications, while keeping backward compatibility for the legacy call form. r28   26 Add socket interface. r21   25 Add latmus request for measuring in-band response time to synthetic interrupt latency. r21   24 Add proxy read side. r19   23 Add the Observable element, and thread observability. r17   22 Add EVL_THRIOC_UNBLOCK, EVL_THRIOC_DEMOTE and EVL_THRIOC_YIELD, update EVL_THRIOC_*_MODE operations. r16   21 Introduce element visibility attribute r15   20 Add support for compat mode (32-bit exec over 64-bit kernel) r12   19 Make y2038 safe r11   18 Plan for supporting a range of ABI revisions -   17 Replace SIGEVL_ACTION_HOME with RETUSER event -   16 Add synchronous breakpoint support -   15 Notify stax-related oob exclusion via SIGDEBUG_STAGE_LOCKED -   14 Add stax test helpers to 'hectic' driver -   13 Add stage exclusion lock mechanism -   12 Add support for recursive gate monitor lock -   11 Read count of timer expiries as a 64bit value -   10 Track count of remote thread wakeups -   9 Complete information returned by EVL_THRIOC_GET_STATE -   8 Add query for CPU state -   7 Drop time remainder return from EVL_CLKIOC_SLEEP -   6 Enable fixed-size writes to proxy -   5 Ensure latmus sends the ultimate bulk of results -   4 Split per-thread debug mode flags -   3 Add count of referrers to poll object shared state -   2 Drop obsolete T_MOVED from thread status bits -   1 Add protocol specifier to monitor element -   0 Initial revision -     Последнее изменение: Mon, 01 Jan 0001 00:00:00 UTC ",
    "description": "",
    "tags": null,
    "title": "ABI revisions",
    "uri": "/ru/core/abi-revs/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/ru/categories/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/ru/tags/"
  }
]
